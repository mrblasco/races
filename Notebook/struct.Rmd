# Identification

## The model

For each room $(l=1, ..., L)$, we observe the count of participants $y_l$ among the $n_l$ registered competitors who have been assigned to some treatment $w_l\in\{0, 1, 2\}$. The count of participating competitors is the realization of a random variable $Y_l$ that depends on a vector of room characteristics $z_l=(x_l, w_l)$ that include the assignment $w_l$ and additional room characteristics $x_l$. We assume that $Y_l$ is distributed as a binomial variable with parameters $n_l$ and $p(x_l, \theta)$ where $p(x, \theta)$ describes the probability for a competitor to participate in a room described by the vector $x$. The model is then the following:

\begin{equation}
	\label{model}
	Y_{l} \sim \text{Binomial}(N_l, p(x_l, \theta)) \quad \text{for each } l=1,...,L.
\end{equation}

In binomial linear models, the probability function $p(x, \theta)$ is related to a linear prediction through a one-to-one transformation called the link function (see xxxx). Following our theoretical approach, participation is one for those with abilities above a given value:

$$
	Y_{l} = 1 \iff  \text{ability}  \geq m_{l}
$$

where $m_l$ is the marginal type for the room.  Given $F_l(\cdot)$ the distribution of individual abilities in each room, the probability of entry is:

\begin{equation} 
	p(m_l, \theta) = 1 - F_l(m_l)
\end{equation}

The marginal type is determined by the zero profit condition:

\begin{equation}
	\label{zero profit}
	R_l(m_l) = m_l^\alpha y_0^\beta t_0^\gamma
\end{equation}

where the $R_l(m_l)$ is the expected revenue of the marginal type in a given room and it must be equal to the  costs evaluated at the deadline $t_0$ and the target $y_0$. 

If we assume one single prize $q=1$ (and we normalize the cost of meeting the deadline to one $t_0^\gamma=1$)  the zero profit condition becomes: 

\begin{equation}
	F_l(m_l)^{n_l-1} = m_l^\alpha y_0^\beta 
\end{equation}

By raising to the power $1/(n_l-1)$ both sides of the equation, we obtain a structural relationship between the probability of entry (eq. XXX) and the marginal type, the number of registered competitors, and the cost function.

$$
	p(x_l, \theta) = 1 - \left[m_l^\alpha y_0^\beta \right]^{1/(n_l-1)} \qquad \text{with } \alpha\leq-1, \beta\geq1.
$$


<!--
We further assume the distribution is known up to the parameters $\theta$ that we want to estimate from the data: 
$$
	F_l(x) = F(x;\theta_l). 
$$
-->

As a result, one could use a "method of moments" to estimate the parameters $\theta=(\alpha, \beta, m_l)$ ($y_0$ and $n_l$ are observed). 

## Estimation

First, we rewrite the probability $p(x, \theta)$ in the following way:

$$
	p(x_l, \theta) = 1 - \theta_1 x_l^{\theta_2}
$$

where we denote the (observed) target by $x_l$ and the vector of parameters by $\theta=(\theta_1, \theta_2)$ where $\theta_1 \equiv m_l^{\alpha/(n_l-1)}$ and $\theta_2 \equiv \beta/(n_l-1)$.  Assume that $n_l=n$ for each $l=1,...,L$. 


Let $Y_1, Y_2, ..., Y_L$ be the count of participants generated by our binomial-distribution model. 
The likelihood is:

$$
	\mathcal{L} = \prod_{l=1}^L \binom{n_l}{y_l} p(x_l, \theta)^{y_l} [1-p(x_l, \theta)]^{n_l - y_l}
$$

The log-likelihood is:

$$
		ll = \sum_{l=1}^L  y_l\log(p(x_l, \theta)) + (n_l - y_l) \log(1-p(x_l, \theta))
$$

where we omit the binomial coefficient as it is a constant that does not affect estimation.

Using our model into the aboev eqution:

$$
	ll = \sum_{l=1}^L  y_l\log(1 - \theta_1 x_l^{\theta_2})
		 + (n - y_l) [\log(\theta_1) +  \theta_2\log(x_l)]
$$

Let the summation be denoted by  $\sum y_l = \bar y$. We can rewrite the ll expression: 

$$
	ll = (L n - \bar y)\log(\theta_1) +  \sum_{l=1}^L  y_l\log(1 - \theta_1 x_l^{\theta_2})
		 +  (n - y_l) \theta_2\log(x_l)
$$


First order conditions are:

$$
	\frac{\partial 	ll}{\partial \theta_1} 
	=  \frac{(L n - \bar y)}{\theta_1}  
		- \sum_{l=1}^L y_l \frac{x_l^{\theta_2}}{1 - \theta_1 x_l^{\theta_2}} = 0
	$$

$$
	\frac{\partial 	ll}{\partial \theta_2} 
	=  \sum_{l=1}^L \left[- y_l \frac{x_l^{\theta_2}\theta_1 \log(x_l)}{1 - \theta_1 x_l^{\theta_2}} 
		+ (n - y_l) \log(x_l) \right]= 0. 
$$

Try with simulations


```{r}
n <- 10
nsize <- 199
theta1 <- 0.75
theta2 <- 0.05
params <- c(theta1, theta2)
x <- runif(nsize)
p <- 1 -  theta1 * x ^theta2
# hist(prob, 100)

loglik <- function(theta, data) {
	theta1 <- theta[1]
	theta2 <- theta[2]
	x <- data$x
	y <- data$y
	n <- data$n
	p <- 1 - theta1 * x ^theta2
	out <- dbinom(y, n, p, log=T)
	return(-sum(out))
}

# Simulate data
y <- rbinom(nsize, size=n, prob=p)

# Example:
# loglik(c(0.5, 0.9), data.frame(y, x, n))

# MLE estimation
dat <- data.frame(y, x, n) 
theta.start <- runif(2)
mle.est <- optim(theta.start, loglik, data=dat)
thetahat <- mle.est$par

# Replications
out <- replicate(1e3, {
	y <- rbinom(nsize, size=n, prob=p)
	dat <- data.frame(y, x, n) 
	mle.est <- optim(theta.start, loglik, data=dat)$par
})
boxplot(t(out))
abline(h=params, col=2)

truth <- function(x) 1 - params[1] * x^params[2]

# Now I can "predictions" on what would happen with different 'x' distribution
prediction <- function(x) 1 - thetahat[1] * x ^thetahat[2]

# Compare with logistic regression
N <- rep(n, nsize)
logistic <- glm(cbind(y, N) ~ x, family=binomial(logit))
prediction.logistic <- function(x) {
	yhat <- coef(logistic)[1] + coef(logistic)[2] * x
	exp(yhat) / (1+exp(yhat))
}

# FIGURE
curve(truth, xlab='Room covariate', ylim=c(0, 1))
curve(prediction, add=TRUE, lty=2)
curve(prediction.logistic(x), add=TRUE, lty=3)
legend("topright", c("Truth", "Estimated", "Logistic"), lty=1:3)
```

