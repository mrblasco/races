# Model estimation

*  We observe $Y_1, ..., Y_L$  participants for $(l=1, ..., L)$ rooms
* Each room has $n_l$ registered competitors and room characteristcs $X_l$ (deadline, target, etc.).
* We assume that $Y_l$ is distributed as a binomial variable with parameters $n_l$ and $p(x_l, \theta)$ where $p(x, \theta)$ describes the probability for a competitor to participate in a room described by the vector $x$: 

\begin{equation}
	\label{model}
	Y_{l} \sim \text{Binomial}(N_l, p(x_l, \theta)) \quad \text{for each } l=1,...,L.
\end{equation}

*  In binomial linear models, the probability function $p(x, \theta)$ is related to a linear prediction through a one-to-one transformation called the link function (see XXXX). [Let see if our model is linear]

* Competitors have a latent ability variable $a^*$ drawn from a common distribution $F_l(\cdot)$ (distributions may differ across rooms). 

* At equilibrium, $Y_{l} = 1 \iff  a^* \geq a_{0l}$ where $a_{0l}$ is the marginal type for the room in room $l$. 

* Zero profit condition (revenues = costs) is

$$
	R(a; F_l) = C(a, y_{0,l}, t_{0,l})
$$

* Given $F_l(\cdot)$ the distribution of individual abilities in each room, the probability of entry is:

\begin{equation} 
	p(x_l, \theta) = 1 - F_l(a_{0l})
\end{equation}


First, we rewrite the probability $p(x, \theta)$ in the following way:

$$
	p(x_l, \theta) = 1 - \theta_1 x_l^{\theta_2}
$$

where we denote the (observed) target by $x_l$ and the vector of parameters by $\theta=(\theta_1, \theta_2)$ where $\theta_1 \equiv m_l^{\alpha/(n_l-1)}$ and $\theta_2 \equiv \beta/(n_l-1)$.  Assume that $n_l=n$ for each $l=1,...,L$. 


Let $Y_1, Y_2, ..., Y_L$ be the count of participants generated by our binomial-distribution model. 
The likelihood is:

$$
	\mathcal{L} = \prod_{l=1}^L \binom{n_l}{y_l} p(x_l, \theta)^{y_l} [1-p(x_l, \theta)]^{n_l - y_l}
$$

The log-likelihood is:

$$
		ll = \sum_{l=1}^L  y_l\log(p(x_l, \theta)) + (n_l - y_l) \log(1-p(x_l, \theta))
$$

where we omit the binomial coefficient as it is a constant that does not affect estimation.

Using our model into the aboev eqution:

$$
	ll = \sum_{l=1}^L  y_l\log(1 - \theta_1 x_l^{\theta_2})
		 + (n - y_l) [\log(\theta_1) +  \theta_2\log(x_l)]
$$

Let the summation be denoted by  $\sum y_l = \bar y$. We can rewrite the ll expression: 

$$
	ll = (L n - \bar y)\log(\theta_1) +  \sum_{l=1}^L  y_l\log(1 - \theta_1 x_l^{\theta_2})
		 +  (n - y_l) \theta_2\log(x_l)
$$


First order conditions are:

$$
	\frac{\partial 	ll}{\partial \theta_1} 
	=  \frac{(L n - \bar y)}{\theta_1}  
		- \sum_{l=1}^L y_l \frac{x_l^{\theta_2}}{1 - \theta_1 x_l^{\theta_2}} = 0
	$$

$$
	\frac{\partial 	ll}{\partial \theta_2} 
	=  \sum_{l=1}^L \left[- y_l \frac{x_l^{\theta_2}\theta_1 \log(x_l)}{1 - \theta_1 x_l^{\theta_2}} 
		+ (n - y_l) \log(x_l) \right]= 0. 
$$

Try with simulations


```{r}
n <- 10
nsize <- 199
theta1 <- 0.75
theta2 <- 0.05
params <- c(theta1, theta2)
x <- runif(nsize)
p <- 1 -  theta1 * x ^theta2
# hist(prob, 100)

loglik <- function(theta, data) {
	theta1 <- theta[1]
	theta2 <- theta[2]
	x <- data$x
	y <- data$y
	n <- data$n
	p <- 1 - theta1 * x ^theta2
	out <- dbinom(y, n, p, log=T)
	return(-sum(out))
}

# Simulate data
y <- rbinom(nsize, size=n, prob=p)

# Example:
# loglik(c(0.5, 0.9), data.frame(y, x, n))

# MLE estimation
dat <- data.frame(y, x, n) 
theta.start <- runif(2)
mle.est <- optim(theta.start, loglik, data=dat)
thetahat <- mle.est$par

# Replications
out <- replicate(1e3, {
	y <- rbinom(nsize, size=n, prob=p)
	dat <- data.frame(y, x, n) 
	mle.est <- optim(theta.start, loglik, data=dat)$par
})
boxplot(t(out))
abline(h=params, col=2)

truth <- function(x) 1 - params[1] * x^params[2]

# Now I can "predictions" on what would happen with different 'x' distribution
prediction <- function(x) 1 - thetahat[1] * x ^thetahat[2]

# Compare with logistic regression
N <- rep(n, nsize)
logistic <- glm(cbind(y, N) ~ x, family=binomial(logit))
prediction.logistic <- function(x) {
	yhat <- coef(logistic)[1] + coef(logistic)[2] * x
	exp(yhat) / (1+exp(yhat))
}

# FIGURE
curve(truth, xlab='Room covariate', ylim=c(0, 1))
curve(prediction, add=TRUE, lty=2)
curve(prediction.logistic(x), add=TRUE, lty=3)
legend("topright", c("Truth", "Estimated", "Logistic"), lty=1:3)
```

