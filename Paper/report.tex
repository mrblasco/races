\documentclass[10pt, titlepage]{article}
\usepackage{beamerarticle}
\usepackage[utf8]{inputenc}
\usepackage{amssymb,amsmath}

% Page settings
\usepackage[letterpaper, margin=1in]{geometry}
\usepackage{times} % palatino, lmodern
\usepackage{setspace}
\onehalfspacing
%\doublespacing  % \singlespacing 

% Appendix
\usepackage{appendix}

% Figure caption on top
% See: https://github.com/axelsommerfeldt/caption/blob/master/doc/caption-eng.pdf
\usepackage[bf,textfont=sc,position=above]{caption}

% Line numbers
%\usepackage{lineno}
%\linenumbers

% Links
\usepackage{hyperref}
\hypersetup{%
  colorlinks=false,% hyperlinks will be black
  linkbordercolor=red,% hyperlink borders will be red
  pdfborderstyle={/S/U/W 1}% border style will be underline of width 1pt
}

% Tables
\usepackage{array,booktabs,longtable,rotating}

% Position tables {here, top, bottom, page}
\makeatletter
\def\fps@table{htbp}
\makeatother

%% ... at the end of paper
\usepackage{float,endfloat}

% Graphics
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother



\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}


%\usepackage{xcolor}
%\usepackage{framed}
%\colorlet{shadecolor}{orange!15}
\usepackage{array}
\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}p{#1}}

%\usepackage{floatrow}
%\floatsetup[table]{capposition=top}
%\floatsetup[figure]{capposition=top}

% Math environments
\newtheorem{proposition}{Proposition}
\newtheorem{define}{Definition}


% Thresholds, limits, bounds, etc.
\newcommand\deadline{\bar{t}}
\newcommand\target{\underline{y}}

% Competitions
\newcommand\race{\text{race}}
\newcommand\tournament{\text{tour}}

% Cost functions
\newcommand\ctime{c_{\tau}}
\newcommand\cscore{c_{y}}
\newcommand\cability{c_{a}}
\newcommand\costs{\cability(a_i)\cscore(y_i)\ctime(t_i)}

% Distribution of types
%\newcommand\ability{a_i}
\newcommand\marginaltype{\hat{a}}
\newcommand\mtype{\hat{a}}
\newcommand\lotype{\underline{a}}
\newcommand\hitype{\bar{a}}



% Derivatives
\newcommand\dystar{\frac{\partial y^*(x,\target)}{\partial\target}dF_{N:N}(x)}

\title{Races or Tournaments? Theory and Evidence from the Field\thanks{Blasco: Harvard Institute for Quantitative Social Science, Harvard
University, 1737 Cambridge Street, Cambridge, MA 02138 (email:
\href{mailto:ablasco@fas.harvard.edu}{\nolinkurl{ablasco@fas.harvard.edu}}).}}
\providecommand{\subtitle}[1]{}
\subtitle{{[}PRELIMINARY AND INCOMPLETE{]}}
\author{Andrea Blasco \and Kevin J. Boudreau \and Karim R. Lakhani \and Michael Menietti}
\date{Last updated: 10 August, 2017}

\begin{document}
\maketitle
\begin{abstract}
We examine the performance of two different choices of contest design:
the race (where the winner is the first to achieve a minimum quality)
and the tournament (where the winner is the one with the highest quality
in a given period). After characterizing the optimal design, we report
results of a field experiment conducted to compare the performance of
three alternatives motivated by theory: the race, the tournament, and
the tournament with a minimum quality requirement. Outcomes in a race
are of comparable quality, supplied faster, and with lower participation
rates. Based on these findings, we show the optimal design under several
counterfactual situations.

\smallskip\noindent 
JEL Classification: M15; M52; O31.

\smallskip\noindent 
Keywords: races; tournaments; contest theory; crowdsourcing; innovation.
\end{abstract}


\clearpage

\section{Introduction}\label{introduction}

Many economic situations---in the public sector, business, sports, and
academia---are decided by either a tournament---a competition to be
best---or a race---a competition to be first. Both types of competition
have become increasingly widespread in large part due to their
effectiveness as sources of incentives. As shown in recent studies,
government-sponsored competitions have had a tremendous impact on
economic growth leading to improvements in agriculture (xxx), navigation
(xxx), and the aviation industry (xxxx). In business, patent laws have
resulted in races that have in many cases encouraged
innovation;\footnote{Although this view contendxxx and empirical
  literature on the topic is not conclusive.} and internal contests have
been proven successful in motivating workers inside organizations (xxxx)
and promoting internal innovation (xxxx). In recent years, we have also
witnessed an explosion of online competitions where organizations have
been sponsoring competitions like tournaments and races to make use of
large online communities of freelance workers (xxxx).

While an extensive literature has focused on the effectiveness of both
races and tournaments, it is unclear when to go for one or the other.
Contest designer seem to choose one or the other depending on their
preferences, the nature of the problem, etc. But there is no theoretical
guidance in choosing how to compete. {[}How strategic players
interact{]}. Two issues: preferences and efficiency when playrers are
strategic {[}How the choice affects entry of strategic players and, in
turn, how entry affects final outcomes{]}. For example, deadlines can be
used but xxx. This also impact efficiency as more entry involves more
duplication costs.

In this paper we do xx and yy.

shown that government sponsored contests have use prize-based contests
to procure public services, motivate workers, accelerate technical
innovation, and promote scientific progress. These contests can be
internal---among workers--- or external---inviting a large crowd of
xxxx. The use of contests raises many questions on how these people
should be choosing how to compete. {[}In public-sector agencies,
managers use both types of competition to procure public services,
accelerate technical innovation, and promote scientific progress. In
business, executives arrange various forms of internal competition to
motivate workers. In the last decade or so, we have also witnessed an
explosion of online contest where many different organizations including
firms and philanthropic organizations have been sponsoring competitions
like tournaments and races to xxxx large online communities of freelance
workers.{]}

As people are increasingly facing the question of choosing how to
compete, xxxx.

Understanding the costs and benefits of these different forms of
competition continues to be a formidable question for economists and
practitioners. This xxx is subtle to analyze.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Preferences for time - It is not clear which is the most effective
\item
  Efficiency\\
  - Avoid duplication costs - Mechanisms to force optimal entry in the
  sense of Mcafee xxxx.
\end{enumerate}

One may claim that it depends on contest sponsors's preferences towards
two desirable, but often incompatible, goals: \(i\)) maximizing revenues
through raising competitors' performances while \(ii\)) lowering the
time it takes to complete a given job. It is, however, unclear in
general which of the type types of competition is more effective. A
tournament may stimulate competition that increases performance and, by
having a relatively short deadline, it may also accelerate production as
well. At the same time, when the deadline is set too short, it may deter
entry thereby lowering the intensity of competition, which in turn may
move down performance. In a similar way, races do not necessarily
improve faster \ldots{} If the target is too high and xxxx.

Alternatively, the choice between tournaments and races can be seen as
the response to ``efficiency'' concerns of the contest sponsors. Under
this view, races and tournaments may lead to the same expected outcomes
in terms of time or effort but lead to different duplication costs by
regulating ``entry'' into the contest {[}as discussed by Fullerton
Mcafee, xxxx{]}. Hence, the ``time preferences'' story and the
``efficiency'' are two possible explanation for using a race or a
contest.

In this article, we investigate the choice between races and tournaments
both theoretically, and empirically in the field. We proceed in two
ways. First, we develop a contest model that encompasses both the race
and the tournament in a single framework. Exploring the duality of the
model, we compare equilibrium behaviors under both competitive formats
and characterize the optimal choice for the contest designer. Then, we
design and execute an experiment to test the implications of the theory
in the field, and xxxx providing policy recommendations.

Our theoritical approach extends the contest model introduced by
Moldovanu and Sela (2001) to a situation in which xxx decide both time
and quality. Thus, contests have an all-pay structure by which
participants pay an immediate cost for an uncertain future reward. The
decision of timing and quality is made under the uncertainty of the
costs of the rivals. The contest designer wants to maximize reveues and
has preferences for both time and quality. Following the analysis of the
model, we show that the optimal design depends on the number of
participants and the concavity of their cost function. We also show that
XXX, YYY, and ZZZZ.

To fix ideas, imagine a government willing to design an innovation
contest aimed at finding solutions to a problem of public health, such
as antibiotic resistance.\footnote{This example is taken\ldots{}} To
minimize the risk that the threat of xxxx will materialize before a
solution is found, one may choose a tournament competition format with a
tight deadline for participants to provide their solutions. The problem
is to find the right duration. When the duration of the competition is
too short, incentives maybe insufficents for competitors to exert enough
effort resulting in inadequate solutions. Alternatively, the government
can set up a race competition with a prize being awarded to the first
competitor who achieves, or goes beyond, a minimum quality threshold.
Here the problem of accelarating the timing of innovation should not be
a big issue but competitors may work inefficiently, as they have no
incentives to exceed the minimum threshold. Fixed the prize structure,
both approaches have specific advantages and limitations. However, xxxx.

The context of the field experiment was an online programming
competition run on Topcoder at the end of 2016. In a typical programming
competition, participants compete writing source code that solves a
given problem for winning a monetary prize. We worked together with
researchers from the United States National Health Institute (NIH) and
the Scripps Research Institute (SCRIPPS) to select a challenging problem
for the contest. The selected problem was based on an algorithm called
BANNER built by NIH (Leaman, Gonzalez, and others 2008) that uses expert
labeling to annotate abstracts from a prominent life sciences and
biomedical search engine, PubMed, so disease characteristics can be more
easily identified. The goal of the programming competition was to
improve upon the current NIH's system by using a combination of expert
and non-expert labeling, as described by Good et al. (2014). The
competition was hosted online on the platform Topcoder.com (about 1M
registered users in 2016). Top submissions were awarded monetary prizes
ranging between \$100 to \$5000 for a total prize pool of more than
\$40,000.

Our intervention consisted in sorting at random participants into
independent virtual rooms of 10 or 15 people. These virtual rooms were
then randomly assigned to one of three different competitive settings: a
race, a tournament, and a tournament with a reserve score, which is the
lowest acceptable score by the platform for a submission to be awarded a
prize.

We find that xxxxx {[}participation in the tournament is xxx compared to
the race the reserve.{]}

We also find that xxxx {[}submission are quicker in a race, whereas are
equally distributed at the end of the competition in the the tournament
and in the tournament with quality requirement.{]}

Another interesting finding is that xxxxx {[}No evidence trade-off
between a race and a tournament in terms of higher scores vs faster
submissions. We do find that scores are higher in the tournament but we
do not find a strong trade-off in the sense that race had comparable
good quality solutions than the tournament.{]}

\section{Literature}\label{literature}

This paper is related to the contest theory literature Dixit (1987) Baye
and Hoppe (2003), Parreiras and Rubinchik (2010), Moldovanu and Sela
(2001), Moldovanu and Sela (2006), Siegel (2009), Siegel (2014). It also
relates to the literature on innovation contests Taylor (1995), Che and
Gale (2003). And the personnel economics approach to contests Lazear and
Rosen (1981), Green and Stokey (1983), Mary, Viscusi, and Zeckhauser
(1984).

Empirically, Dechenaux, Kovenock, and Sheremeta (2014) provide a
comprehensive summary of the experimental literature on contests and
tourments. Large body of empirical works have focused on sports contests
Szymanski (2003). More recently, inside firms (xxx) and online contest
(xxxx).

This paper is also related to the econometrics of auctions Paarsch
(1992), Laffont, Ossard, and Vuong (1995), Donald and Paarsch (1996) and
more recently Athey, Levin, and Seira (2011), Athey and Haile (2002),
and Athey and Haile (2007).

An extensive literature has discussed the reasons why contests are
sometimes preferred to other forms of incentives (e.g., individual
contracts). Typically, contests reduce monitoring costs {[}xxx{]},
incentivize production with common risks {[}xxx{]}, and deal with
indivisible rewards {[}xxxx{]}, among others. While there is not much
debate on why contests should be used, the issue of how to effectively
design and deploy a contest still attracts much research.

Several aspects of contest design have been investigated, including the
optimal prize structure {[}XXX, xxxx, xxxx{]}, number of competitors
{[}XXX, XXX{]}, and imposing restrictions to competition such as minimum
effort requirements {[}XXX, XXX{]}. Also, a great deal of theoretical
models of races and tournaments have been developed and applied to a
wide range of economic situations including patent races {[}xxx{]}, arms
races {[}xxx{]}, sports {[}xxx{]}, the mechanism of promotions inside
firms {[}xxxx{]}, sales tournaments {[}xxxx{]}, etc.

Harris and Vickers (1987), Grossman and Shapiro (1987) investigate the
dynamics issues patent races where the interest is how firms compete for
a patent. Bimpikis, Ehsani, and Mostagir (2014) looks at the problem of
how to design an information structure that is optimal when the contest
is a race and innovation is uncertain (encouragement and competition
effect). In the laboratory, Zizzo (2002) finds poor support to
predictions of dynamic xxxx. In general we do not know much about the
dynamic aspect of contests.

The duality. As pointed out by Baye and Hoppe (2003), many of these
models of tournament and race competitions are specific cases of a more
general ``contest games.'' And sometimes it is possible to design one or
the other in a way to exploit a ``duality.'' In other words, in theory,
a competition can be designed as a tournament to do xxx or as a race to
do xxx. While theoretically very useful, how to exploit this duality in
practice remains largely unknown. Lack of data. As before, xxxx. The
main challenge is self-selection. The answer to this optimal design
question relates to the cost function of agents with respect to ``time''
and to ``effort.'' It is hard to say which solution is better. However,
it is easier to tell whether you should have one prize or multiple
prizes.

\newcommand\competition{c}
\newcommand\ability{a}
\newcommand\performance{y}
\newcommand\timing{t}
\newcommand\Timing{T}
\newcommand\Performance{Y}

\section{The model}\label{the-model}

We now generalize the contest game introduced by Moldovanu and Sela
(2001) to a situation where players simultaneously decide \(i)\) how
fast to perform a given task and \(ii)\) the quality of their
performance (e.g., the time to solve a problem and effectiveness of the
solution). Then we explore the problem of revenue maximization faced by
a contest designer with preferences for both time and quality.

\subsection{Basic setup}\label{basic-setup}

Consider a contest game where \(i=1,..., n\) contestants are competing
for \(p\) prizes of value \(v_1\geq v_2\geq ...\geq v_p\geq0\); and
imagine the total value distributed to winners is normalized to one,
\(\sum_{k=1}^p v_k =1\). To win a prize, each contestant will act
independently over the course of a given period of time \(\deadline>0\)
working to develop solutions to a given problem. The time it takes to
develop their solutions and the effectiveness or quality of the
delivered solutions are denoted by two nonnegative variables
\(\timing_i\) and \(\performance_i\) respectively. Solving the problem
can be thus defined as developing a solution with an effectiveness of at
least \(\target\geq0\).

Each contestant \(i\) has a cost of problem solving \(c_i\) given by \[
    c_i = \ability_i^\alpha C(\performance_i, \timing_i)
\] where \(C(\cdot)\) denotes a cost function of the inputs and \(a_i\)
is a (strictly positive) ability parameter changing across contestants
and \(\alpha<0\).

The cost function \(C(\cdot)\) is decreasing in the time to develop
solutions. It is increasing in the quality of the developed sultions. It
is \emph{multiplicative} (the higher the quality the more time it takes
to develop the solution at the same cost). And it is assumed to have the
following functional form:
\(C(\performance, \timing) = \performance^\alpha \timing^\beta,\) where
the parameters \(\alpha>1\) and \(\beta<0\) denote the \emph{elasticity}
of the cost function with respect to its inputs.

The ability parameter reflects heterogeneity in skills, time
constraints, and other elements affecting their ability to solve the
problem. Each ability parameter \(a_i\) is drawn at random from a common
distribution function \(F_A(\cdot)\) with (absolutely continuous)
density \(f_A(\cdot)\) on a bounded interval. Players observe privately
the realization of their own ability (their \emph{type}) before making
their choices.

The contest designer determines how contestants compete for prizes. We
consider three types of competition: the race, tournament, and
tournament with reserve.

\begin{itemize}
\item
  In a race, the first contestant that solves the problem (i.e.,
  achieves a performance equal or higher than \(\target\)) wins the
  first prize. The second contestant to solve the problem wins the
  second prize, and so on until either all the prizes are awarded or
  there are no more eligible contestants (i.e., those who have solved
  the problem in the given time).
\item
  In a tournament, the contestant achieving the highest performance
  within the given deadline \(\deadline\) wins the first prize. The
  contestant with the second highest performance wins the second prize,
  and so on until all prizes are awarded.
\item
  Finally, the tournament with reserve is a slightly more general form
  of tournament competition where, to win a prize, competitors must
  achieve a performance equal or higher than the target \(\target\).
  Prizes are thus awarded based on performance until either all prizes
  are allocated or there no more eligible contestants (i.e., those with
  a performance equal or higher than \(\target\)).
\end{itemize}

In all three types of competition, the payoff of a contestant \(i\) with
ability \(a_i\) who has developed a solution of quality \(y_i\) that was
delivered in time \(t_i\), is either
\(v_k - \ability_i^\alpha C(y_i, t_i)\) if \(i\) wins the k-th prize, or
\(-\ability_i^\alpha C(y_i, t_i)\) if \(i\) does not win a prize.

The contest designer is risk neutral and aims to maximize its revenues
by increasing the winner's performance, while keeping low the time of
delivery (i.e., max the speed of production). The actions of the contest
winner are random variables denoted by \((Y^w, T^w)\). The contest
designer's expected payoff (net of total prizes distributed to winners)
is:

\[
    \pi_{cd} = E[Y^w - \tau T^w]
\]

where \(E\) denotes the expectation with respect to the distribution of
players' actions in equilibrium; and \(\tau\) denotes the intensity of
the contest designer's (negative) preferences towards solutions
delivered later in time.

\subsection{Equilibrium}\label{equilibrium}

We now solve the model for the unique symmetric Bayesian Nash
equilibrium of players.

\subsubsection{Races}\label{races}

The first key observation is that, in a race, any solution of quality
below the target \(\target\) gives a zero probability of winning prizes;
and any solution of quality above the target gives a constant
probability of winning. It is thus a dominant strategy for every
contestant to choose a quality either of zero (i.e., the lowest
possible) or equal to the target \(\target\).

As choosing a quality of zero gives a null payoff for any \(t\), we
avoid multiplicity of equilibrium by using the convention that when
\(y_i=0\) then \(t_i = \deadline\).

To ensure monotonicity, we use the convention that any player who sets
performance equal to zero also sets the time higher than or equal to the
deadline (e.g., to infinity). By doing so, the probability of winning
the \(k\)-th prize in a race reduces to the probability the timing is
lower than the xxx but higher than the xxx order statistic.

Assuming there exists a monotonic function \(t^*(\cdot)\) such that the
optimal timing \(t_i^*\) for a player with ability \(a_i\) is
\(t^*_i = t^*(a_i)\). The optimal timing \(t_i^*\) is given by first
order conditions \[
    \frac{d g(\phi)}{d \phi} \dot\phi = a^\alpha y^\beta \delta t^{\delta-1}
\]

Using the definition: \[
    \frac{d \phi}{d y} =  g(\phi) h(t)
\] where \[
    g(\phi) = \phi^\alpha \left[\sum \dot p^k (\phi) v_k \right]^{-1}
\] and \[
    h(y) =  \target^\beta  \delta t^{\delta -1}
\] with initial conditions \(\phi(\deadline)=0\).

Here, the key observation is that, for a given level of quality, any
time that is strictly below the deadline does not affect the probability
of winning but is costly in terms of effort (working faster is costlier)
and any time that is strictly above the deadline gives a negative
payoff. Thus, choosing \(t_i=\deadline\) is a (weakly) dominant strategy
for each player. Then the first order condition with respect to quality
is:

\[
    \sum_{k=1}^{q} \hat p^{\prime}_{k}(y_i) v_k = \cability(a_i) \cscore^\prime(y_i) \ctime(\deadline).
\]

where \(\hat p = p(\cdot, \deadline).\) Then it can be show that xxxx.

\begin{align}
0 = & \alpha f_{(1:N-1)}(\phi) \phi^{\prime} 
    + (1-\alpha)\phi^{\prime}\{[1 - F_{(1:N-1)}(\phi)]f_{(1:N-2)}(\phi) + \nonumber\\
    & + f_{(1:N-1)}(\phi) F_{(1:N-2)}(\phi)\} 
    - c_{a}(a) c_{y}(\target) \ctime^{\prime}(t_i)
\end{align}

subject to the boundary condition \(\phi(0) = \lotype\) (i.e., the
lowest-ability competitor's optimal output quality is zero).

As shown by Moldovanu and Sela (2001), the solution is

\begin{equation} \label{ystar}
y^*(a_i) = 
    \cscore^{-1}
    \left[\cscore(\target) 
    + \frac{1}{\ctime(\deadline)}
    \left(\alpha \int_{a_i}^{\hitype} A(z) dz
      + (1-\alpha) \int_{a_i}^{\hitype} B(z)  dz
    \right)
    \right]
\end{equation}

where

\begin{equation}
  A(x) = \frac{1}{c_{a}(x)} f_{(n-1:n-1)}(x)
\end{equation}

and

\begin{equation}
  B(x) = \frac{1}{c_{a}(x)} \left\{
      \left[1- F_{(n-1:n-1)}(x)\right]f_{(n-1:n-2)}(x)
      + f_{(n-1:n-1)}(x) F_{(n-1:n-2)}(x)
    \right\}.
\end{equation}

Monotonicity of the equilibrium output quality implies that, for every
\(i=1, ..., n\), the equilibrium expected payoff from the contest
\(\pi_i^*\) depends on the rank of the player's ability relative to the
others. As a result, the equilibrium expected payoff net of costs is

\begin{equation} 
    R(a_i) = \alpha F_{n:n}(a_i) + (1-\alpha)[1-F_{n:n}(a_i)] F_{n-1:n-1}(a_i).
\end{equation}

\% payoffs

\subsubsection{Equilibrium in a race}\label{equilibrium-in-a-race}

In a similar way, one can derive the equilibrium strategy in a race.
Again the key observation is that any quality below the target gives a
zero probability of winning and any quality above the target gives a
constant probability of winning. Thus, player \(i\)'s choice of optimal
quality \(y^*\) is either zero (with \(t_i=\deadline\) by convention) or
\(y^*=\target\).

Then, the equilibrium xxx for player \(i\) is

\begin{equation} \label{tstar}
t^*(a_i) = 
    \ctime^{-1}
    \left[\ctime(\deadline) 
    + \frac{1}{\cscore(\target)}
    \left(\alpha \int_{a_i}^{\hitype} A^\prime(z) dz
      + (1-\alpha) \int_{a_i}^{\hitype} B^\prime(z)  dz
    \right)
    \right]
\end{equation}

where

\begin{equation}
  A(x) = \frac{1}{c_{a}(x)} f_{(n-1:n-1)}(x)
\end{equation}

and

\begin{equation}
    B(x) = \frac{1}{c_{a}(x)} \left\{
            \left[1- F_{(n-1:n-1)}(x)\right]f_{(n-1:n-2)}(x)
            + f_{(n-1:n-1)}(x) F_{(n-1:n-2)}(x)
    \right\}.
\end{equation}

An important property of XX is that \(y^*(a_i)\) has its upper bound in
XX and lower bound in XX. Again payoffs are xxxx. Hence, by setting to
zero and solving for the ability, gives the marginal ability
\({\underline a}\) as

\begin{equation}
  {\underline a}= h(n, V, F_A, C, d).
\end{equation}

\subsubsection{Tournament vs races}\label{tournament-vs-races}

By comparing equilibrium xxx and xxx, we find that the race and the
tournament do not (ex-post) dominate one another with respect to output
quality. Whereas the race always dominates the tournament with respect
to completion time. {[}This is only when the deadline is the same.
Otherwise, there's always xxxx.{]} This result is stated below.

\begin{proposition}
There always exist an interval of abilities where the output quality is higher in the race than in the tournament. By contrast, every player takes less completion time in the race than in the tournament.
\end{proposition}

\begin{proof}
Marginal type has utility zero in a race but the same type has a strictly positive utility in the tournament. Since probability of winning is not different in the race or the tournament (the bid is a monotonic transformation of the individual ability or, in other words, rankings are virtually the same), expected payoffs in equilibrium differ only in the cost functions. Hence, to be an equilibrium, the player in the tournament should bid less than the player in the race to earn a strictly positive expected payoff. 
\end{proof}

Let's make an example.

\begin{verbatim}
p <- plnorm   # pdf individual abilities 
r <- rlnorm   # Simulate individual abilities
cy <- function(x) x^2 # Cost function performance
ct <- function(x) 2*exp(1-x)  # Cost function timing 
\end{verbatim}

FIGURE 1. Equilibrium bids in a race and a tournament.

Implications. The above proposition applies only if the target is higher
in a race than in a tournament. But what if the two competitions had the
same target ? In that case, tournaments and races have the same marginal
type. Therefore, the performance of players in the tournament with
reserve are always non-lower than those in the race. This does not imply
that it is optimal to set the target. On the contrary, we will show that
it is optimal to set an optimal target in a tournament that is below the
optimal target in a race. Next section.

\subsection{The contest designer's
problem}\label{the-contest-designers-problem}

Let us now focus on the contest designer's problem. Imagine the contest
designer can choose the competition format to be either the race or the
tournament. Imagine all other aspects of design are given. The prize
structure \(\alpha\) has been already chosen. There is a deadline
\(\deadline\), which is the same in both competition formats. {[}The
quality requirement \(\target_c\) in the tournament is smalle than that
in the race \(\target_\race > \target_\tournament\)){]} We will relax
this assumption later to consider a more general setting where these
variables are also part of the contest designer's problem.

The contest designer has an objective function that is increasing in the
expected quality of the winning solution and decreasing in the
corresponding time to completion. Here, to do not complicate exposition,
we assume that the contest designer cares about the winning submission
only: second placed efforts are not considered. {[}If the principal
values the diversity of the solutions \ldots{} but we assume it does
not.{]}

XXX EQUATION XXXX

The optimal choice involves a comparison of the expected profits between
the race and the tournament. Given xxxx, we can show that there will be
a threshold on the cost of completion time \(\hat\tau\) above which the
race is a better choice than the tournament, and vice versa.

\begin{proposition}
There's a tau above which ... 
\end{proposition}

Proof. In a tournament, the objective function is

\begin{align}
R_\tournament & = \Pr(t_{(1:n)}\leq \deadline) \left\{\int y^*(x \mid t_{(1:n)}\leq \deadline) dF_{n:n}(x) - \tau \deadline - 1 \right\}  \nonumber\\
  & = \int_{\mtype}^{\hitype} y^*(x) dF_{n:n}(x) - \tau \deadline - 1. 
\end{align}

That is, the contest designer's objective function is the sum of the
expected output quality for a given deadline, minus the cost \(\tau\) of
having the winner working on the task until completion (i.e., until the
deadline), and the cost of the prize pool (recall the prize pool is
normalized to one).

{[}Implicitly, you're assuming that the prize is always large enough to
ensure positive effort.{]} {[}Second prize too is stochastic!!!!{]}

In a race, the objective function is

\begin{align}
R_\race & =  
  \Pr(a_{(N)}\geq \mtype) \left\{\target - \alpha -
  \Pr(a_{(N-1)}\geq \mtype) (1-\alpha) \right\}
  - \tau \int_{\mtype}^{\infty} t^*(x) dF_{N:N}(x) \nonumber\\
  & = [1-F_{N:N}(\mtype)] \left\{\target - \alpha -
  [1-F_{N-1:N}(\mtype)] (1 - \alpha) \right\}
  - \tau \int_{\mtype}^{\infty} t^*(x) dF_{N:N}(x).
\end{align}

Note. \(t^*(x) \leq \deadline\) for all \(x\)'s. Thus, a lower bound for
the above objective function can be computed:

\begin{align}
\underline {R_\race} & = 
  [1-F_{N:N}(\mtype)] \left\{\target - \alpha -
  [1-F_{N-1:N}(\mtype)] (1 - \alpha) - \tau \deadline\right\}
\end{align}

An even simpler lower bound is rewriting the above expression as if
\(\alpha=1\) (note if the real alpha was set 1 then also mtype would
change and therefore setting alpha hits a lower bound only when mtype
does xxxx when alpha is 1).

Note. \(y^*(x)\) is lower than \(\target\) for all \(a < \mtype\). Thus,
a lower bound of the tournament's expression is

\begin{align}
\overline {R_\tournament} & = 
  [1-F_{N:N}(\mtype)] \target + \int_{\mtype}^\infty y^*(x) dF_{N:N}(x) 
  - \tau \deadline - 1. 
\end{align}

\begin{align}
  \underline {R_\race} \geq & \overline {R_\tournament} \nonumber\\
  [1-F_{N:N}(\mtype)] (\target - 1 - \tau \deadline) \geq &
  [1-F_{N:N}(\mtype)] \target + \int_{\mtype}^\infty y^*(x) dF_{N:N}(x) 
  - \tau \deadline - 1 \nonumber\\
  - [1-F_{N:N}(\mtype)] (\tau\deadline + 1) \geq &
  \int_{\mtype}^\infty y^*(x) dF_{N:N}(x) 
  - (\tau \deadline + 1) \nonumber\\
  F_{N:N}(\mtype) (\tau \deadline + 1) \geq &
  \int_{\mtype}^\infty y^*(x) dF_{N:N}(x) \nonumber\\
  \tau \geq & 
    \left[
      \frac{\int_{\mtype}^\infty y^*(x) dF_{N:N}(x)}{F_{N:N}(\mtype)} -1 
    \right] \frac{1}{\deadline}
\end{align}

End proof.

When the cost of time \(\tau\) is sufficiently high, the race is
preferred. Interestingly, the threshold is a function of the deadline to
complete the job, as xxx. It also depends on the shape of xxxx.

\subsubsection{Optimal minimum-entry}\label{optimal-minimum-entry}

Now we turn to discuss the contest designer's choice of an optimal
minimum requirement \(\target\). So far, we have assumed that
\(\target_\race>\target_\tournament\). Now, we show that the assumption
that xxxx is indeed an optimal choice of the contest designer. This is
summarized in the next proposition.

\begin{proposition}
Suppose the contest designer can choose the target that max profits in both the race and the tournament. Then, the optimal $\target$ in tournament is generally lower than that in a race.
\end{proposition}

To prove that it is indeed the case. We proceed in two steps. First, we
assume that the contest designer does not care about minimizing the
timing of the innovation by imposing \(\tau = 0\). For simplicity,
assume that \(\alpha=1\) (winner-takes-all). In a race, this means that
the optimal target will be a value that makes equal the costs in terms
of less participation versus the gains in terms of higher values of the
winning solutions. Formally, the contest designer's problem in a race is

\begin{align}
  \text{maximize } & R^\race = [1-F_{N:N}(\mtype)] (\target_\race - 1).
\end{align}

Note that \(\mtype\) depends on the target. This is clearly concave in
\(\target_\race\). Thus, the first order condition is also sufficient.

\begin{align}\label{foc race}
  \text{FOC } & \Rightarrow -F^\prime_{N:N}(\mtype) \mtype^\prime (\target_\race - 1) + [1-F_{N:N}(\mtype)] = 0.
\end{align}

In a tournament, \ldots{}

\begin{align}
  \text{maximize } & R^\race = \int_{\mtype}^\infty y^*(x, \target) d F_{N:N}(x) - [1-F_{N:N}(\mtype)]. 
\end{align}

Convexity is not sure. If not, then the optimal target is zero. Which is
lower than the optimal target in a race.

Instead. If the objective function is (strictly) concave then there's an
internal solution.

\begin{align} \label{foc tournament}
  \text{FOC } \Rightarrow & 
    \frac{d\int_{\mtype}^\infty y^*(x, \target) d F_{N:N}(x)) }{d \target}
      + F^\prime_{N:N}(\mtype) \mtype^\prime =0 \nonumber\\ 
    & \text{(by using Leibniz rule)}\nonumber\\
  \Rightarrow & - y^*(\mtype, \target) \mtype^\prime F^\prime_{N:N}(\mtype) 
      + \int_{\mtype}^\infty \dystar - F^\prime_{N:N}(\mtype) \mtype^\prime = 0\nonumber\\
  \Rightarrow & -\target \mtype^\prime F^\prime_{N:N}(\mtype) 
      + \int_{\mtype}^\infty \dystar - F^\prime_{N:N}(\mtype) \mtype^\prime = 0.
\end{align}

Using \eqref{foc race} with \eqref{foc tournament}, the optimal target
is the same in the race and the tournament only if

\begin{align} 
  \int_{\mtype}^\infty \dystar = [1- F_{N:N}(\mtype)].
\end{align}

\[
  \frac{\partial y^*(x, \target)}{\partial \target} = 
    \frac{c_y^\prime(\target)}{c_y^\prime(y^*(x, \target))}. 
\]

Then.

\begin{itemize}
\item
  If \(c_y(\cdot)\) is linear, we have that the ratio is one for all
  \(x\).
\item
  If \(c_y(\cdot)\) is convex, then we have that it is less than one. If
\item
  If \(c_y(\cdot)\) is concave, then we have that it is higher than one.
\end{itemize}

As a result, if linear or convex the first order condition is lower than
that in the race. Since the obj. function is concave (second order is
decreasing), the target should be lower in a tournament than in a race
to satisfy the first order condition. (a lower target increases the
focs.).

Conjecture. If \(\tau>0\), the \(\target\) in the race is higher.

\subsection{Structural econometric
model}\label{structural-econometric-model}

Readings:

\begin{itemize}
\item
  \href{http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.199.680\&rep=rep1\&type=pdf}{The
  winner's curse, reserve prices, and endogenous entry: Empirical
  insights from eBay auctions}
\item
  \href{https://www.econstor.eu/bitstream/10419/79263/1/516364804.pdf}{Entry
  and competition effects in first-price auctions: theory and evidence
  from procurement auctions}
\item
  \href{http://www.vita.mcafee.cc/PDF/AuctionswithEntry.pdf}{Auctions
  with entry}
\end{itemize}

General two-step strategy:

\begin{itemize}
\item
  First step. Identify the marginal type from the data and the
  distribution of types.
\item
  Second step. Using the estimated distribution of types.
\end{itemize}

Basic idea. Equilibrium condition gives:

\begin{equation}
  y_i^* = y^*(a_i; F_{\mathcal{A}}). 
\end{equation}

with \(y^*(\cdot)\) being an invertible function with \(\phi\) denoting
the inverse.

Hence the distribution of bids is

\begin{equation}
F_{Y}(y) = \Pr(y_i^* \leq y) 
        = \Pr(y^*(a_i) \leq y) 
        = \Pr(a_i \leq \phi(y)) 
        = F_\mathcal{A}( \phi(y)).
\end{equation}

Identification of the model. suggest

\section{Experimental design}\label{experimental-design}

Over the past few years, an extensive literature has focused on
naturally occurring data to study competition in contests. The use of
naturally occurring data for comparing races and tournaments, however,
is problematic. As our simple theoretical model indicates, there are
sensible differences in competitors' payoffs between a race and a
tournament competition. Likewise contest designers may design contests
with competition styles that best suit their preferences for time and
quality. These elements will create selection problems that are hard to
control for and may bias the analysis.

Instead of using naturally occurring data, we test our theory by
designing and executing a field experiment. In doing so, one need an
environment where the same type of contest can be replicated under
different competition styles, while keeping fixed all other relevant
features. It is also crucial to avoid the selection problem by
registering competitors for the contest before they learn about the
competition style. Finally, one should be able to record accurately
competitors' actions and to obtain performance measures like the timing
and quality of the submissions made during the contest.

Such an environment was provided by the online platform for programming
competitions \emph{Topcoder} who agreed to provide i) access to its
large member base of competitors (over 1 million registered users in
2016) and ii) access to its platform's tools (leaderboards, payment
system, scoring methods) for managing online contests that we used for
the execution of our experimental design.

Three key factors made this platform ideal for our experiment.

First, platform members have generally a great deal of experience and
knowledge of programming competitions. The platform offers periodic
programming competitions in which members usually participate. These
competitions can be of two types: Single Round Matches (SRMs) --- a two
hour competition where contestants are presented with three problems and
get points based on the time elapsed between when a problem was open
until a working solution was submitted; and Marathon Matches (MMs) --- a
two-to-four week competition where contestasts are presented with a
difficult algorithmic problem and top solutions that satisfy given
requirements are awarded cash prizes. We thus expected platform members
to understand and be familiar with the kind of strategic interactions
that naturally arise in a race or a tournament, and be comfortable with
a situation in which they are competing for cash prizes.

Second, the platform provides us with different measures of competitors'
individual ability based on their past performance. These include two
independent ``skill ratings'' (one for each competition type) that are
computed and provide a metric of their ability as contestants in each
competition type (SRMs and MMs). These metrics are publicly available
for each member on the platform. The fact that these measures are
publicly available is particularly important because our theoretical
approach presumes competitors will form common beliefs about the overall
distribution of skills in the contest, which is particularly reasonable
when members have constant access to information about the overall
distribution of ratings on the platform.

Third, the platform collects rich data analytics on contest
participation (timing and number of submissions) and computes an
objective metric of the quality of a solution in the form of a
submission score. This is habitually done for each programming
competition. Contestants are free to choose when and how many
submissions to make. Solutions are recorded on the platform and
automatically scored. In general, the correctness of a solution is not a
simple pass/fail, but solutions may be scored on many different criteria
including how close the return values match a theoretical ``correct''
answer like a prediction error or how fast solutions run. These scoring
metrics serve to form a provisional leaderboard that updates in real
time during the submission phase. After the submission phase is over, a
final leaderboard is then shown that serves to name the winners of the
contest. Provisional and final leaderboards will generally differ as
final scores are computed with additional tests like estimating
prediction errors on unseen datasets. As the platform provides us with
all this information, it gives us the capability of measuring the extent
and quality of participation for each competitor with high accuracy.

The timing of the experiment was as follows.

As a first step, a four day registration period for a new competition
was announced to the entire community. The announcement was inviting
platform members to register and participate in a new (Marathon Match)
programming competition for solving a hard algorithmic problem (which we
will describe later). As an incentive for participation, a total prize
pool of more than \$40,000 was offered to top submissions in the form of
cash prizes. The announcement was sent via email to all newsletter
subscribers and was publicized in a post on the platform's blog.

The online registration process was not overly burdensome. It simply
involved giving an informed consent to participation in the research and
responding to a short initial survey to collect participant
demographics. As experienced members are more likely to behave according
to model predictions, we limited participation to those members.
Platform members who had never registered for a MMs competition before
the experiment were unable to register. Newly signed-up members were
thus excluded from our sample.

A total of 299 participants registered for this experiment. We then
sorted at random all participants into 24 groups or ``competition
rooms'' --- the largest number of concomitant virtual rooms allowed by
the platform at that time. Each competition room can be viewed as an
independent contest where we offered cash prizes of \$1,000 and \$100 to
the first and second placed competitor. In addition, each room provided
competitors with access to a private leaderboard (updated about every 48
hours), a web forum to ask questions about the computational problem,
and a submission system through which they could submit their codes. The
whole submission period lasted 8 days.

Rooms differed in size, as half of the rooms had 10 competitors and the
rest had 15 competitors. From each block of rooms with equal room size,
we split rooms at random into 3 competition styles: i) tournament, ii)
race, and iii) tournament with a reservation quality (hereinafter simply
``reserve''), that we describe below. Hence, our experimental design is
summarized by a 3x2 matrix, as shown in Table \ref{experimental design}.

\begin{table}
\centering
\caption{Experimental Design}
\label{experimental design}
\begin{tabular}{rrrr}
  \\[-1.8ex]\hline\hline\\[-1.8ex]
 & Large & Small & Sum \\ 
  \hline\\[-1.86ex]
Race & 60 & 39 & 99 \\ 
  Tournament & 60 & 40 & 100 \\ 
  Reserve & 60 & 40 & 100 \\ 
  Sum & 180 & 119 & 299 \\ 
   \hline\\[-1.8ex]
\end{tabular}
\end{table}

In a Tournament, the solution with the highest score in the room was
awarded a first-place room prize of \$1000 and the solution with the
second highest score in the room was awarded a second-place room prize
of \$100. In addition, a ``grand'' prize of \$6,000 was awarded to the
solution with the highest score from all rooms that were assigned to the
Tournament condition. If a competitor had made more than one submission,
only the the last solution in terms of time was scored.

In a Race, the first solution to achieve a score equal or higher than a
given target (we will discuss later criteria used to pick the target)
was awarded a first-place room prize of \$1000 and the solution with the
second highest score in the room was awarded a second-place room prize
of \$100. In addition, a ``grand'' prize of \$6,000 was awarded to the
first solution to meet the target across all rooms that were assigned to
the Race condition. {[}Scores were computed at discrete intervals
\ldots{}about every 48 hours.{]}

Finally, in a Reserve, {[}xxxxx{]}.

The competition style was announced at the start of the submission phase
via email and in one section of the description of the computational
problem as well. Payments were administered by the platform. To collect
self-reported measures of effort, limited-edition T-shirts were offered
as incentive for responding to a final survey. This final survey took
place a few days after the end of the submission period (but before the
final ranking was shown and winners fully identified).

{[}Importantlty the contest was a non-rated event.{]}

\subsection{The algorithmic problem and fixing a
target.}\label{the-algorithmic-problem-and-fixing-a-target.}

To select a challenging algorithmic problem for the experiment, we
worked together with researchers from the United States National Health
Institute (NIH) and the Scripps Research Institute (SCRIPPS). The
selected problem was based on an algorithm called BANNER (Leaman,
Gonzalez, and others 2008) that was built by researchers at the NIH. The
algorithm uses domain-expert manual labeling to train a Natural Language
Entity Recognition (NLER)'s model that performs automatic annotation of
abstracts from a large corpus of biomedical research papers (e.g.,
PubMed). As automatic annotations help disease characteristics to be
more easily identified, improving the {[}xxx{]} was very important for
x, y, and z. Though adding annotations can be very costly, because
{[}xxxx{]}.

The specific goal of the programming competition was to improve upon the
current NIH's BANNER by using a combination of domain-expert and
non-expert (e.g., Amazon Mechanical Turk's workers) manual labeling
(e.g., Good et al. 2014). {[}xxxx{]}

\begin{itemize}
\item
  The threshold in the ``race'' and in the ``reserve'' was chosen
  following two main criteria. First, we run a pre-trail experiment that
  involved 4 coders solving the same problem in isolation for 5 days.
  This helped us forming basic predictions about xxxx. Second, we
  surveyed the NIH researchers who developed Banner asking for three
  percentage improvements they considered ``useful,'' ``desirable,'' and
  ``unlikely.''
\item
  As a measure of the ability to make correct predictions of domain
  expert annotations we used the ``F-score'' defined as the harmonic
  mean of precision and recall:
  \(F = 2 * (precision * recall) / (precision + recall)\)). This score
  was computed on 300 abstracts (100 of which were not disculosed to
  avoid overfitting) with about xxxx entities to correctly identify from
  a dictionary with xxxx labels.
\item
  The baseline F-score achieved by NIH researchers was 0.793. We set up
  a hard-to-reach F-score target for the race competition which was
  0.818 (about a 3 percent increase of the baseline). The winner
  achieved a score of 0.844. This represents a 5.11 percentage points
  increase compared to the baseline which can be regarded as a very
  remarkable improvement (more than 6 percent).
\end{itemize}

\subsection{Data}\label{data}

\begin{table}
\centering
\caption{Descriptive statistics}
\label{summary}
\begin{tabular}{@{}lrrrrrrr}
  \\[-1.8ex]\hline\hline\\[-1.8ex]
 & Mean & Median & St.Dev. & Min & Max & Obs. & F-statistic \\ 
  \hline\\[-1.86ex]
 \multicolumn{1}{@{}l}{\emph{Platform data:}}\\
 \\[-1.86ex]~rating & 13.2 & 13 & 4 & 6 & 30 & 210 & 0.003 \\ 
   \\[-1.86ex]~ratingsrm & 1320.1 & 1238 & 560 & 312 & 2958 & 238 & 0.114 \\ 
   \\[-1.86ex]~nreg & 17.6 & 9 & 23 & 1 & 161 & 299 & 0.631 \\ 
   \\[-1.86ex]~nsub & 7.2 & 2 & 12 & 0 & 91 & 299 & 0.410 \\ 
   \\[-1.86ex]~nregsrm & 45.7 & 20 & 64 & 0 & 365 & 299 & 0.149 \\ 
   \\[-1.86ex]~nsubsrm & 40.6 & 19 & 58 & 0 & 338 & 299 & 0.187 \\ 
   \\[-1.86ex]~year & 5.1 & 5 & 4 & 0 & 14 & 299 & 0.617 \\ 
   \\[-1.86ex]~paidyr & 64.9 & 6 & 150 & 0 & 1069 & 139 & 0.104 \\ 
   \\[-1.86ex]~nwins & 3.0 & 1 & 5 & 1 & 27 & 28 & 1.908 \\ 
   \\[-1.86ex]\hline\multicolumn{1}{@{}l}{\emph{Survey data:}}\\
 \\[-1.86ex]~ntop5 & 4.0 & 2 & 8 & 1 & 56 & 70 & 0.565 \\ 
   \\[-1.86ex]~ntop10 & 5.0 & 2 & 9 & 1 & 64 & 94 & 1.464 \\ 
   \\[-1.86ex]~male & 1.0 & 1 & 0 & 0 & 1 & 276 & 0.981 \\ 
   \\[-1.86ex]~timezone & 2.1 & 2 & 5 & -8 & 10 & 277 & 1.340 \\ 
   \\[-1.86ex]~postgrad & 0.4 & 0 & 0 & 0 & 1 & 278 & 1.289 \\ 
   \\[-1.86ex]~below30 & 0.7 & 1 & 0 & 0 & 1 & 278 & 0.680 \\ 
   \\[-1.86ex]~risk & 6.4 & 7 & 2 & 1 & 10 & 279 & 0.141 \\ 
   \\[-1.86ex]~hours & 31.3 & 24 & 25 & 0 & 192 & 277 & 0.388 \\ 
   \\[-1.86ex]~hours12 & 7.7 & 6 & 7 & 0 & 48 & 277 & 0.664 \\ 
   \\[-1.86ex]~hours34 & 7.0 & 5 & 6 & 0 & 48 & 277 & 0.592 \\ 
   \\[-1.86ex]~hours56 & 7.5 & 5 & 7 & 0 & 48 & 278 & 0.329 \\ 
   \\[-1.86ex]~hours78 & 9.1 & 8 & 8 & 0 & 48 & 277 & 0.538 \\ 
   \hline\\[-1.8ex]
\end{tabular}
\begin{minipage}{\textwidth}
\footnotesize\emph{Notes:}{ Platform data: `year` denotes the years as platform member; `nreg` and `nregsrm` are the counts of registrations to past MMs and SRMs competitions, respectively; `nsub` and `nsubsrm` are the counts of submissions to past MMs and SRMs competitions, respectively; `paidyr` is prize money per year (in thousand of dollars) won in past competitions; `nwins`, `ntop5`, `ntop10` denote placements in past MMs competitions; Registration survey: `risk` is a measure of risk aversion; `hours` anticipated hours of work on solving the problem of the contest; `male` indicates the gender; `timezone` refers to competitor's residence during the contest; `postgrad` is an indicator for post-graduate educational degree (MAs or PhDs); and `below30` indicates age below 30 years old.
}\end{minipage}
\end{table}

We collected extensive platform data for each registered participant.
These include the full history of their registrations in past contests,
as well as the outcomes of these contests like the number of solutions
submitted, placements in the final rankings, and total cash prizes
earned. As reported in Table \ref{summary}, our data show that those who
registered in the experiment were very experienced competitors. They had
been platform members for an average of 5 years, had registered in an
average of 18 multi-week long contests (MMs), in which they had been
earning a median of 6 hundred dollars per year in cash prizes. As our
data shows, on average, they had submitted solutions in only 7 of these
contests (29 percent of their registrations), suggesting that even
highly experienced platform members frequently drop out of competitions.
Our sample had also registered in an average of 46 speed-based contests
(SRMs) with a relatively higher submission rate (84 percent), presumably
because these contests take only a few hours to complete and have
relatively simpler problems that require less effort to solve.

\begin{figure}
\centering
\caption{Distribution of skill ratings}
\label{skill ratings}
\includegraphics{Figures/ratingplot-1.pdf}
\end{figure}

A key variable to measure was competitors' ability in solving
algorithmic problems (like the named entity recognition task used for
the experiment) and their speed in programming working solutions. A
sensible measure of problem-solving ability was the individual skill
rating that is computed on multi-week long contests (MMs). Similarly, a
sensible measure for programming speed was the skill rating computed on
speed based competitions (SRMs).\footnote{The skill rating is an
  elo-type measure of a competitor's relative ability compared to
  others. This measure is represented by a number that increases or
  decreases at the end of a competition depending on the difference
  between a hypothetical expected rank (based on the pre-contest values
  of the skill rating of the opponents) and the actual rank achieved by
  a competitor at the end of a competition. When the actual rank is
  higher than the expected rank, the skill rating increases whereas it
  decreases otherwise. Skill ratings are computed independently for each
  type of competition run on the platform (MMs and SRMS).}

As shown in Figure \ref{skill ratings}, both skill ratings (those
measuring problem solving ability and speed) have slightly asymmetric
probability distributions due to the presence of a few competitors with
very high skill ratings relative to the rest. The right-hand panel of
Figure \ref{skill ratings} shows that, while the skill ratings computed
on MMs is positively correlated with the skill rating computed on
speed-based competitions SRMs, there is considerable variation in
skills.\footnote{The fraction of explained variance or R squared by
  simple linear regression was 0.212.} So, a relatively good competitor
in speed-based contests (SRMs), could turn out to be a relatively bad
competitor when faced with hard-to-solve algorithmic problems in
multi-week long contests (MMs) --- and vice versa. However, variation
seems to reduce for the very top competitors, (those who have high
scores in at least one of the two types of competitions), suggesting
that being fast and being able to solve hard problems are highly
correlated for top competitors.

The online registration survey provided additional data on basic
demographics like gender, age, geographic origins, education, and most
preferred programming language. It also provided measures of attitudes
towards risk as our initial survey asked registrants to indicate their
``willingness to take risks in general'' on a 11-point scale (from 0
``Unwilling'' to 10 ``Completely willing'').\footnote{The validity and
  economic relevance of this way to measure risk preferences has been
  shown by Dohmen et al. (2011).} Our survey also collected measures of
time availability. Registrants were asked to make a forecast of how many
hours they anticipated to be able to work on the problem during the
submission phase of the challenge.\footnote{The exact question was:
  ``Looking ahead a week, how many hours do you forecast to be able to
  work on the solution of the problem?''. Participants had to pick an
  integer between 0 and 48 hours for every 2 days of the submission
  phase (a total of 4 choices).}

Demographics reflected the overall distribution of characteristics of
active platform members. As shown in Table \ref{summary}, the gender
composition was highly unbalanced towards male (95 percent) and young
(below 30 years old) (66 percent) competitors. In terms of risk
aversion, the median response was higher than in previous studies
(xxxx), indicating competitors had perhaps higher propensity to take
risk than the norm. Our data also show that participants anticipated a
median of 24 hours of work to solve the algorithmic problem during the
contest (anticipating slightly more hours of work in the first and last
2 days of the contest).

To test whether randomization into treatment groups was successful, we
conducted a series of F-tests to check the statistical significance of
mean differences in competitors' characteristics between treatments. As
shown in Table \ref{summary}, these tests returned very small F
statistics for each variable indicating that our randomization was
successful.

\section{Results}\label{results}

At the end of the eight day submission period of the contest, we
collected individual data like the number, timing, and score of each
code submission made by competitors. As competitors behaviors are
generally correlated within each room, we then aggregated these
individual data into room responses to examine differences between
competition styles at the room level.\footnote{While such a dependence
  does not generally affect estimatation of mean differences, it might
  bias inference accuracy. As actions across rooms can be treated as
  independent, the inferential problem is thus less severe when the
  analysis is at the room level.}

\subsection{Entry}\label{entry}

Our analysis of the effects of different competition styles begins by
looking at competitors' entry. The entry variable is the fraction of
competitors in a room who made at least one submission during the eight
day submission period of the contest. Our theoretical model predicts
that, all else being equal, competitors' entry will be higher in a
tournament than in a race competition, because of the lack of minimum
performance requirements. For the same reason, it also predicts higher
entry rates in regular tournaments compared to tournaments with reserve.
Consistent with these predictions, Figure \ref{room entrants} shows a
higher median percentage of entrants in tournaments compared to both
races and tournaments with reserve. It also shows that the difference in
entry between tournaments and races, though it is consistently positive
across room sizes, seemed more marked in small rooms compared to large
rooms, suggesting sobering effects associated with size.

\begin{figure}
\caption{Percentage of room entrants by competition and room size}
\label{room entrants}
\includegraphics{Figures/entrybox-1.pdf}
\end{figure}

On average, 4.1 competitors (33.8 percent) assigned to tournaments were
submitting solutions compared to 3.2 competitors (26.1 percent) assigned
to races and 3.4 (26.2 percent) assigned to tournaments with reserve. To
test if the observed positive difference in entry between tournaments
and races was statistically greater than zero, we used a multiple linear
regression model. We regressed the fraction of entrants in each room
against treatment dummies and room size dummies. To control for random
differences in competitors' baseline characteristics across rooms, we
added controls for differences in room abilities and
demographics.\footnote{We consider two sets of room controls: a
  ``partial set'' and a ``full set.'' The partial set includes control
  variables with a relatively higher chance of being different across
  treatment groups. Those are variables with a relatively high (i.e.,
  equal or larger than one) F statistic as reported on Table
  \ref{summary}. The full set includes all the available controls with
  the exception of a few variables. To avoid multicollinearity issues,
  the full set does not include: the anticipated hours of work in each
  day (\texttt{hours12-78}) that are replaced by the sum
  (\texttt{hours}); and the number of top five placements that was
  highly correlated with the number of wins. Other minor issues. Since
  our data has only a few winners with many wins, instead of using the
  mean number of wins of the room, we took an indicator for whether a
  winner was in the room or not.}

\begin{table}
\centering
\caption{Estimates of the effects of competition and room size on entry}\label{ols entry}
\begin{tabular}{@{}lcccccc}
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
 & \multicolumn{6}{c}{\textit{Dependent variable:}} \\ 
\cline{2-7} 
\\[-1.8ex] & \multicolumn{3}{c}{Entry/n} & \multicolumn{3}{c}{log(Entry/n)} \\ 
\\[-1.8ex] & (1) & (2) & (3) & (4) & (5) & (6)\\ 
\hline \\[-1.8ex] 
 Tournament & 0.076 & 0.108$^{*}$ & 0.181$^{***}$ & 0.303 & 0.396$^{*}$ & 0.678$^{***}$ \\ 
  & (0.055) & (0.061) & (0.038) & (0.207) & (0.225) & (0.112) \\ 
  & & & & & & \\ 
 Tournament w/reserve & 0.001 & 0.080 & 0.058 & 0.024 & 0.352 & 0.422 \\ 
  & (0.055) & (0.090) & (0.086) & (0.207) & (0.331) & (0.252) \\ 
  & & & & & & \\ 
 Room size (small) & $-$0.004 & $-$0.013 & $-$0.012 & $-$0.124 & $-$0.167 & $-$0.156 \\ 
  & (0.045) & (0.053) & (0.041) & (0.169) & (0.197) & (0.119) \\ 
  & & & & & & \\ 
 Constant & 0.263$^{***}$ & 0.413 & $-$0.187 & $-$1.370$^{***}$ & $-$0.066 & $-$3.410 \\ 
  & (0.045) & (0.518) & (0.845) & (0.169) & (1.910) & (2.480) \\ 
  & & & & & & \\ 
\hline \\[-1.8ex] 
Room controls & no controls & partial & full & no controls & partial & full \\ 
Observations & 24 & 24 & 24 & 24 & 24 & 24 \\ 
R$^{2}$ & 0.113 & 0.272 & 0.952 & 0.137 & 0.323 & 0.972 \\ 
\hline 
\hline \\[-1.8ex] 
\end{tabular} 
\begin{minipage}{1.000000\textwidth}
\footnotesize\emph{Note:} The table reports regression estimates of the effects of different competition styles on entry computed using three sets of room controls: ``no controls", ``partial", ``full." Standard errors are reported in parenthesis. ***,**, * indicate statistical significance at 1, 5, and 10 percent level.
\end{minipage}
\end{table}

As shown in Table \ref{ols entry}, the estimated difference in entry
between tournaments and races is of 0.08. This corresponds to a
difference of 1 more entrant every 13 = 1/0.076 competitors or, using
the specification with a log-transformed proportion (column 4), an
increase by a factor of 1.35 = exp(0.303) in entry relative to races.
Without any adjustment for random differences in the baseline
characteristics of the rooms, the effect on entry is significantly
greater than zero with a one-sided test at 10 percent level (t=1.391;
one-sided p=0.09). Statistical significance increases when the effect is
computed using the ``partial'' and ``full'' set of controls, thus
adjusting for random differences at the room level like skills, past
experience, and other demographics. Taken together, these results show
evidence supporting a positive effect of tournaments on entry relative
to races. In addition, our estimates show no systematic difference in
entry rates between races and tournaments with reserve (as predicted by
theory); and fail to detect any statistically significant difference
between large and small rooms.

Because tournaments yielded higher entry rates relative to races, it is
now useful to discuss possible drivers of entry by looking at the
sorting patterns across competition styles. Our theory suggests that,
all else being equal, the positive gap in entry of tournaments compared
to races should be driven by low-ability competitors sorting into
tournaments at higher rates. Though theory prescinds from practical
definitions of ability, two main types of abilities seemd the most
relevent in our context. One involves proficiency in solving algorithmic
problems like having adequate problem-solving skills and a good
knowledge of programming. The second pertains to having as much time
availability as required to solve the given problem. Our analysis thus
focused on two types of data: platform data on skills and experience;
and the hours of work as anticipated by competitors at registration.

\begin{table}
\centering
\caption{Sorting patterns}
\label{sorting}
\begin{tabular}{@{}lcccc}
  \\[-1.8ex]\hline\hline\\[-1.8ex]
Variable & \multicolumn{1}{L{2cm}}{Difference entrants vs non-entrants} & \multicolumn{1}{L{2cm}}{Difference entrants in races vs tournaments} & \multicolumn{1}{L{2cm}}{Difference entrants in races vs tournaments w/reserve} & Obs. \\ 
  \hline\\[-1.86ex]
log(rating) &  0.14*** &  0.07 &  $-$0.13 & 210 \\ 
   & (0.04) & (0.11) & (0.11) &  \\ 
  log(ratingsrm) &  0.07 &  0.08 &  $-$0.18 & 238 \\ 
   & (0.07) & (0.16) & (0.16) &  \\ 
  log(nreg) &  0.77*** &  0.50 &   0.86** & 299 \\ 
   & (0.16) & (0.4) & (0.41) &  \\ 
  log(nsub+1) &  0.81*** &  0.32 &   0.56 & 299 \\ 
   & (0.15) & (0.36) & (0.37) &  \\ 
  log(nregsrm+1) &  0.36 &  0.40 &  $-$0.29 & 299 \\ 
   & (0.23) & (0.55) & (0.57) &  \\ 
  log(nsubsrm+1) &  0.39* &  0.44 &  $-$0.26 & 299 \\ 
   & (0.23) & (0.55) & (0.57) &  \\ 
  log(year+1) &  0.19** &  0.28 &   0.11 & 299 \\ 
   & (0.09) & (0.22) & (0.22) &  \\ 
  log(paidyr) &  0.68 & $-$1.34 &  $-$1.02 & 139 \\ 
   & (0.46) & (1.12) & (1.15) &  \\ 
  log(nwins) &  0.21 & $-$0.70 &  $-$1.07 & 28 \\ 
   & (0.35) & (0.9) & (0.89) &  \\ 
  log(ntop5) &  0.23 &  0.05 &  $-$0.50 & 70 \\ 
   & (0.23) & (0.58) & (0.55) &  \\ 
  log(ntop10) &  0.33 & $-$0.20 &  $-$0.66 & 94 \\ 
   & (0.22) & (0.57) & (0.55) &  \\ 
  male &  0.03 & $-$0.11 &  $-$0.06 & 276 \\ 
   & (0.03) & (0.07) & (0.07) &  \\ 
  timezone &  0.26 &  1.34 &   0.29 & 277 \\ 
   & (0.68) & (1.66) & (1.7) &  \\ 
  postgrad &  0.00 &  0.11 &   0.07 & 278 \\ 
   & (0.06) & (0.16) & (0.16) &  \\ 
  below30 & $-$0.16*** &  0.02 &  $-$0.13 & 278 \\ 
   & (0.06) & (0.15) & (0.16) &  \\ 
  log(risk) & $-$0.13** &  0.31** &   0.11 & 279 \\ 
   & (0.06) & (0.14) & (0.15) &  \\ 
  hours &  7.31** & $-$2.22 & $-$17.18** & 277 \\ 
   & (3.35) & (8.15) & (8.31) &  \\ 
  hours12 &  1.97** & $-$1.64 &  $-$4.91** & 277 \\ 
   & (0.9) & (2.2) & (2.24) &  \\ 
  hours34 &  1.77** & $-$0.42 &  $-$4.06* & 277 \\ 
   & (0.84) & (2.05) & (2.09) &  \\ 
  hours56 &  1.68* & $-$0.04 &  $-$3.33 & 278 \\ 
   & (0.9) & (2.19) & (2.23) &  \\ 
  hours78 &  1.81* &  0.14 &  $-$4.63* & 277 \\ 
   & (0.99) & (2.41) & (2.46) &  \\ 
   \hline\\[-1.8ex]
\end{tabular}
\begin{minipage}{\textwidth}
\footnotesize\emph{Notes:}{ The table reports conditional mean differences of various competitors' characteristics conditional on entry and the randomly assigned competition style. Standard errors are reported in parenthesis. ***,**, * indicate statistical significance for t-test at 1, 5, and 10 percent level.
}\end{minipage}
\end{table}

As shown in Table \ref{sorting}, entrants were more skilled than
non-entrants. They had, on average, 15 percent higher (p=0.001) skill
ratings (computed on MMs), registered to xxx more past competitions,
earned 1.97 times more (p=0.141) money in cash prizes per year relative
to non-entrants. Regarding time availability, the other dimension of
interest, the analysis of our registration survey data showed that
entrants --- before knowing the randomly assigned competition style ---
anticipated working many more hours relative than non-entrants (an
average of 7.3 more total hours of work on programming/solving the
algorithmic problem). Since skill ratings and anticipated hours of work
are uncorrelated in our dataset, an interpretation of these results is
that our measures were indeed capturing two different dimensions of
ability (problem solving skills and time availability) and that these
factors were significant determinants of entry.

While our results show stark differences between entrants and
non-entrants, we failed to detect significant differences in entrants'
characteristics between races and tournaments. As reported in Table
\ref{sorting}, all our estimates of the mean differences between races
and tournaments are insignificant (with the only exception of one
variable). Thus, we find no evidence of skill-based sorting in terms of
both our measures of ability (time and skill ratings). By contrast, we
find significant differences in entrants' anticipated hours of work
between races and tournaments with reserve. Since entry rates are not
different between these two treatment groups, one may conclude that some
sort of substitution effect must have occurred. As these effects are not
explicitly tackled by our model, we will not be focusing on the
determinants of this substitution effect.

\subsection{Scores}\label{scores}

The results on entry suggest that tournaments had a positive impact on
competitors' participation relative to races and tournaments with
reserve. The analysis of sorting showed further that the increment of
entrants was driven by xxxx rather than differences in experience and
proficiency in solving problems, raising the question of whether xxxx
will generate relatively higher performance levels. Our model predicts
that, all else being equal, tournaments will yield higher performance
levels relative to races. It also predicts that tournaments with reserve
will dominate both regular tournaments and races in terms of
performance. To investigate this issue, we now focus on differences in
performance as measured by the highest score attained by a competitor's
last submission in each room.

Figure XXX shows xxxx. The platform keeps record of provisional and
system scores for each submission. Provisional and system scores are
computed in the same way but on different datasets: provisional scores
are computed on a ``testing'' dataset and then published on the
leaderboard; system scores, by contrast, are computed on a ``system
testing'' dataset and kept secret until the end of the contest. As final
scores matter for awarding cash prizes, keeping these scores secret
prevents the so-called ``overfitting'' problem (i.e., a form of model
misspecification).\footnote{When competitors have made more than one
  submission, the final score is generally computed on the competitor's
  last submission.}

\begin{figure}
\centering
\includegraphics{Figures/finalbox-1.pdf}
\caption{Highest room scores by competition style and room
size\label{scores}}
\end{figure}

While final scores give a very powerful indication of performance, one
problem with measuring individual performance using final scores is that
any failure in the submitted code might return very low values, as shown
in Figure \ref{scores over time}, that are not wholly indicative of
performance. As outliers will in general bias the room means, we
explored two corrective methods (1) we trimmed last scores before
computing the room means; and (2) we replaced all final scores that were
below the baseline score with the baseline. The first method gives a
winsorized mean, which is an unbiased estimate of the average
{[}XXXX{]}. The second approach equals censoring by assuming that all
entrants had a performance at least equal to the final score that one
would obtain by submitting the BANNER's algorithm without making any
useful change.

\begin{table}
\centering
\caption{Estimates of the Effect of Competition Style on Performance}\label{ols entry}
\begin{tabular}{@{}lcccccc}
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
 & \multicolumn{6}{c}{\textit{Dependent variable:}} \\ 
\cline{2-7} 
\\[-1.8ex] & \multicolumn{3}{c}{Highest score} & \multicolumn{3}{c}{log(Highest score)} \\ 
\\[-1.8ex] & (1) & (2) & (3) & (4) & (5) & (6)\\ 
\hline \\[-1.8ex] 
 Tournament & 0.14 & 0.36 & 0.46 & 0.001 & 0.004 & 0.005 \\ 
  & (0.74) & (0.80) & (0.69) & (0.01) & (0.01) & (0.01) \\ 
  & & & & & & \\ 
 Tournament w/reserve & $-$0.01 & 0.05 & 0.37 & $-$0.0003 & 0.0004 & 0.004 \\ 
  & (0.74) & (1.18) & (1.56) & (0.01) & (0.01) & (0.02) \\ 
  & & & & & & \\ 
 Room size (small) & $-$0.57 & $-$0.69 & $-$0.41 & $-$0.01 & $-$0.01 & $-$0.004 \\ 
  & (0.60) & (0.70) & (0.74) & (0.01) & (0.01) & (0.01) \\ 
  & & & & & & \\ 
 Constant & 100.00$^{***}$ & 100.00$^{***}$ & 103.00$^{***}$ & 4.61$^{***}$ & 4.60$^{***}$ & 4.64$^{***}$ \\ 
  & (0.60) & (6.79) & (15.30) & (0.01) & (0.07) & (0.15) \\ 
  & & & & & & \\ 
\hline \\[-1.8ex] 
Room controls & no controls & partial & full & no controls & partial & full \\ 
Observations & 24 & 24 & 24 & 24 & 24 & 24 \\ 
R$^{2}$ & 0.05 & 0.25 & 0.91 & 0.05 & 0.25 & 0.91 \\ 
\hline 
\hline \\[-1.8ex] 
\end{tabular} 
\begin{minipage}{1.000000\textwidth}
\footnotesize\emph{Note:} The table reports regression estimates of the effects of different competition and room size on the highest score in a room computed using three sets of room controls: ``no controls", ``partial", ``fulla." Standard errors are reported in parenthesis. ***,**, * indicate statistical significance at 1, 5, and 10 percent level.
\end{minipage}
\end{table}

Results. we do not find evidence supporting our hypothesis that mean
room scores were higher in the Tournament with Reserve compared to the
other treatments. The reader may find this finding not entirely
surprising given the lack skill-based selection across groups that we
documented earlier.

\begin{itemize}
\item
  We find a significant difference in variance, suggesting that variance
  was higher in the reserve.
\item
  We replace 3 values that were 1.5 times below the inter-quantile range
  of the distribution with the baseline value.
\end{itemize}

\subsection{Speed}\label{speed}

Finally, we examined differences in submission speed measured by the
mean time of the first submission in each room. As shown in the right
panel of Figure \ref{room outcomes}, the median room time-to-submit in
the Race was about 20 and 40 hours shorter than the Tournament and the
Tournament with Reserve, respectively. The variance, however, was also
larger with values ranging from below 60 to 190 hours, whereas by
comparison the other groups' distributions were both above the 120
hours. To test to see if the difference in means between speed in the
Race and the other groups was greater than zero, we used a xxxx which
gives a (one-sided) p-value of xxxxx. {[}Bootstrap{]} Thus, our data
support the hypothesis that competitors' speed in a Race was higher than
in the other groups. Taken together with the absence of skill-based
selection and score differences, this result implies that competitors in
the Race competition have exerted greater efforts (i.e., by lowering
execution time while keeping performance at a comparable level) relative
to the other groups.

\begin{figure}
\caption{Average time of first submission by competition and room size}
\label{room entrants}
\includegraphics{Figures/firstsubbox-1.pdf}
\end{figure}

\begin{figure}
\caption{Average score of first submissions by competition and room size}
\label{room entrants}
\includegraphics{Figures/speedbox-1.pdf}
\end{figure}

\begin{table}
\centering
\caption{Estimates of the Effect of Competition Style on Production Speed}\label{ols entry}
\begin{tabular}{@{}lcccccc}
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
 & \multicolumn{6}{c}{\textit{Dependent variable:}} \\ 
\cline{2-7} 
\\[-1.8ex] & \multicolumn{3}{c}{Speed} & \multicolumn{3}{c}{log(Speed)} \\ 
\\[-1.8ex] & (1) & (2) & (3) & (4) & (5) & (6)\\ 
\hline \\[-1.8ex] 
 Tournament & $-$1.10$^{*}$ & $-$1.09 & $-$1.57 & $-$0.80$^{*}$ & $-$0.87$^{*}$ & $-$1.14 \\ 
  & (0.61) & (0.65) & (0.74) & (0.43) & (0.46) & (0.57) \\ 
  & & & & & & \\ 
 Tournament w/reserve & $-$1.22$^{*}$ & $-$0.76 & $-$1.99 & $-$1.11$^{**}$ & $-$0.45 & $-$0.92 \\ 
  & (0.61) & (0.96) & (1.66) & (0.43) & (0.68) & (1.28) \\ 
  & & & & & & \\ 
 Room size (small) & 0.61 & 0.50 & $-$0.38 & 0.12 & 0.19 & $-$0.07 \\ 
  & (0.50) & (0.57) & (0.79) & (0.35) & (0.40) & (0.61) \\ 
  & & & & & & \\ 
 Constant & 1.18$^{**}$ & 4.55 & 17.70 & $-$0.37 & 2.79 & 9.91 \\ 
  & (0.50) & (5.50) & (16.40) & (0.35) & (3.89) & (12.70) \\ 
  & & & & & & \\ 
\hline \\[-1.8ex] 
Room controls & no controls & partial & full & no controls & partial & full \\ 
Observations & 24 & 24 & 24 & 24 & 24 & 24 \\ 
R$^{2}$ & 0.24 & 0.43 & 0.88 & 0.27 & 0.44 & 0.85 \\ 
\hline 
\hline \\[-1.8ex] 
\end{tabular} 
\begin{minipage}{1.000000\textwidth}
\footnotesize\emph{Note:} The table reports regression estimates of the effects of different competition and room size on the highest score in a room computed using three sets of room controls: ``no controls", ``partial", ``fulla." Standard errors are reported in parenthesis. ***,**, * indicate statistical significance at 1, 5, and 10 percent level.
\end{minipage}
\end{table}

To summarize the results obtained so far, we have found evidence
supporting a higher participation in the Tournament. This higher
participation, however, does not seem to be driven by low-skilled
competitors. We have also found that competitors in the race made
submissions faster without sacrificing performance, which suggests that
they have paid higher costs from effort associated with the accelerated
speed compared to other competitors. Finally, we reject the hypothesis
that the Tournament with Reserve yields higher performance levels.
{[}One explanation is that competitors were fishing to hit the threshold
and then stopped exerting effort{]}.

\begin{verbatim}
##  setting  value                       
##  version  R version 3.3.2 (2016-10-31)
##  system   x86_64, darwin13.4.0        
##  ui       X11                         
##  language (EN)                        
##  collate  en_US.UTF-8                 
##  tz       America/New_York            
##  date     2017-08-10                  
## 
##  package   * version date       source        
##  backports   1.0.5   2017-01-18 CRAN (R 3.3.2)
##  base      * 3.3.2   2016-10-31 local         
##  boot      * 1.3-18  2016-02-23 CRAN (R 3.3.0)
##  contest   * 0.1     2017-07-09 local (@0.1)  
##  datasets  * 3.3.2   2016-10-31 local         
##  devtools    1.13.2  2017-06-02 CRAN (R 3.3.2)
##  digest      0.6.12  2017-01-27 CRAN (R 3.3.2)
##  evaluate    0.10    2016-10-11 CRAN (R 3.3.0)
##  graphics  * 3.3.2   2016-10-31 local         
##  grDevices * 3.3.2   2016-10-31 local         
##  grid        3.3.2   2016-10-31 local         
##  highr       0.6     2016-05-09 CRAN (R 3.3.0)
##  htmltools   0.3.5   2016-03-21 CRAN (R 3.3.0)
##  knitr     * 1.15.1  2016-11-22 CRAN (R 3.3.2)
##  lattice     0.20-34 2016-09-06 CRAN (R 3.3.2)
##  magrittr  * 1.5     2014-11-22 CRAN (R 3.3.0)
##  Matrix      1.2-7.1 2016-09-01 CRAN (R 3.3.2)
##  memoise     1.0.0   2016-01-29 CRAN (R 3.3.0)
##  methods     3.3.2   2016-10-31 local         
##  races     * 0.3     2017-07-27 local (@0.3)  
##  Rcpp        0.12.9  2017-01-14 CRAN (R 3.3.2)
##  rmarkdown   1.3     2016-12-21 CRAN (R 3.3.2)
##  rprojroot   1.2     2017-01-16 CRAN (R 3.3.2)
##  splines     3.3.2   2016-10-31 local         
##  stargazer * 5.2     2015-07-14 CRAN (R 3.3.0)
##  stats     * 3.3.2   2016-10-31 local         
##  stringi     1.1.2   2016-10-01 CRAN (R 3.3.0)
##  stringr     1.2.0   2017-02-18 CRAN (R 3.3.2)
##  survival  * 2.40-1  2016-10-30 CRAN (R 3.3.0)
##  tools       3.3.2   2016-10-31 local         
##  utils     * 3.3.2   2016-10-31 local         
##  withr       1.0.2   2016-06-20 CRAN (R 3.3.0)
##  xtable    * 1.8-2   2016-02-05 CRAN (R 3.3.0)
##  yaml        2.1.14  2016-11-12 CRAN (R 3.3.2)
\end{verbatim}

\section*{References}\label{references}
\addcontentsline{toc}{section}{References}

\hypertarget{refs}{}
\hypertarget{ref-athey2002identification}{}
Athey, Susan, and Philip A Haile. 2002. ``Identification of Standard
Auction Models.'' \emph{Econometrica} 70 (6). Wiley Online Library:
2107--40.

\hypertarget{ref-athey2007nonparametric}{}
---------. 2007. ``Nonparametric Approaches to Auctions.''
\emph{Handbook of Econometrics} 6. Elsevier: 3847--3965.

\hypertarget{ref-athey2011comparing}{}
Athey, Susan, Jonathan Levin, and Enrique Seira. 2011. ``Comparing Open
and Sealed Bid Auctions: Evidence from Timber Auctions*.'' \emph{The
Quarterly Journal of Economics} 126 (1). Oxford University Press:
207--57.

\hypertarget{ref-baye2003strategic}{}
Baye, Michael R, and Heidrun C Hoppe. 2003. ``The Strategic Equivalence
of Rent-Seeking, Innovation, and Patent-Race Games.'' \emph{Games and
Economic Behavior} 44 (2). Elsevier: 217--26.

\hypertarget{ref-bimpikis2014designing}{}
Bimpikis, Kostas, Shayan Ehsani, and Mohamed Mostagir. 2014. ``Designing
Dynamic Contests.'' Working paper, Stanford University.

\hypertarget{ref-che2003optimal}{}
Che, Yeon-Koo, and Ian Gale. 2003. ``Optimal Design of Research
Contests.'' \emph{The American Economic Review} 93 (3). American
Economic Association: 646--71.

\hypertarget{ref-dechenaux2014survey}{}
Dechenaux, Emmanuel, Dan Kovenock, and Roman M Sheremeta. 2014. ``A
Survey of Experimental Research on Contests, All-Pay Auctions and
Tournaments.'' \emph{Experimental Economics}. Springer, 1--61.

\hypertarget{ref-dixit1987strategic}{}
Dixit, Avinash Kamalakar. 1987. ``Strategic Behavior in Contests.''
\emph{The American Economic Review} 77 (5). American Economic
Association: 891--98.

\hypertarget{ref-dohmen2011individual}{}
Dohmen, Thomas, Armin Falk, David Huffman, Uwe Sunde, Jrgen Schupp, and
Gert G Wagner. 2011. ``Individual Risk Attitudes: Measurement,
Determinants, and Behavioral Consequences.'' \emph{Journal of the
European Economic Association} 9 (3). Wiley Online Library: 522--50.

\hypertarget{ref-donald1996identification}{}
Donald, Stephen G, and Harry J Paarsch. 1996. ``Identification,
Estimation, and Testing in Parametric Empirical Models of Auctions
Within the Independent Private Values Paradigm.'' \emph{Econometric
Theory} 12 (03). Cambridge Univ Press: 517--67.

\hypertarget{ref-good2014microtask}{}
Good, Benjamin M, Max Nanis, CHUNLEI Wu, and ANDREW I Su. 2014.
``Microtask Crowdsourcing for Disease Mention Annotation in Pubmed
Abstracts.'' In \emph{Pacific Symposium on Biocomputing. Pacific
Symposium on Biocomputing}, 282--93. NIH Public Access.

\hypertarget{ref-green1983comparison}{}
Green, Jerry R, and Nancy L Stokey. 1983. ``A Comparison of Tournaments
and Contracts.'' \emph{The Journal of Political Economy} 91 (3):
349--64.

\hypertarget{ref-grossman1987dynamic}{}
Grossman, Gene M, and Carl Shapiro. 1987. ``Dynamic R\&D Competition.''
\emph{Economic Journal} 97 (386). Royal Economic Society: 372--87.

\hypertarget{ref-harris1987racing}{}
Harris, Christopher, and John Vickers. 1987. ``Racing with
Uncertainty.'' \emph{The Review of Economic Studies} 54 (1). Oxford
University Press: 1--21.

\hypertarget{ref-laffont1995econometrics}{}
Laffont, Jean-Jacques, Herve Ossard, and Quang Vuong. 1995.
``Econometrics of First-Price Auctions.'' \emph{Econometrica} 63 (4).
Econometric Society: 953--80.

\hypertarget{ref-lazear1981rank}{}
Lazear, Edward P, and Sherwin Rosen. 1981. ``Rank-Order Tournaments as
Optimum Labor Contracts.'' \emph{The Journal of Political Economy} 89
(5): 841--64.

\hypertarget{ref-leaman2008banner}{}
Leaman, Robert, Graciela Gonzalez, and others. 2008. ``BANNER: An
Executable Survey of Advances in Biomedical Named Entity Recognition.''
In \emph{Pacific Symposium on Biocomputing}, 13:652--63.

\hypertarget{ref-mary1984economic}{}
Mary, O'Keeffe, W Kip Viscusi, and Richard J Zeckhauser. 1984.
``Economic Contests: Comparative Reward Schemes.'' \emph{Journal of
Labor Economics} 2 (1). University of Chicago Press: 27--56.

\hypertarget{ref-moldovanu2001optimal}{}
Moldovanu, Benny, and Aner Sela. 2001. ``The Optimal Allocation of
Prizes in Contests.'' \emph{The American Economic Review}. JSTOR,
542--58.

\hypertarget{ref-moldovanu2006contest}{}
---------. 2006. ``Contest Architecture.'' \emph{Journal of Economic
Theory} 126 (1). Elsevier: 70--96.

\hypertarget{ref-paarsch1992deciding}{}
Paarsch, Harry J. 1992. ``Deciding Between the Common and Private Value
Paradigms in Empirical Models of Auctions.'' \emph{Journal of
Econometrics} 51 (1-2). Elsevier: 191--215.

\hypertarget{ref-parreiras2010contests}{}
Parreiras, Srgio O, and Anna Rubinchik. 2010. ``Contests with Three or
More Heterogeneous Agents.'' \emph{Games and Economic Behavior} 68 (2).
Elsevier: 703--15.

\hypertarget{ref-siegel2009all}{}
Siegel, Ron. 2009. ``All-Pay Contests.'' \emph{Econometrica} 77 (1).
Wiley Online Library: 71--92.

\hypertarget{ref-siegel2014contests}{}
---------. 2014. ``Contests with Productive Effort.''
\emph{International Journal of Game Theory} 43 (3). Springer: 515--23.

\hypertarget{ref-szymanski2003economic}{}
Szymanski, Stefan. 2003. ``The Economic Design of Sporting Contests.''
\emph{Journal of Economic Literature} 41 (4). American Economic
Association: 1137--87.

\hypertarget{ref-taylor1995digging}{}
Taylor, Curtis R. 1995. ``Digging for Golden Carrots: An Analysis of
Research Tournaments.'' \emph{The American Economic Review} 85 (4).
American Economic Association: 872--90.

\hypertarget{ref-zizzo2002racing}{}
Zizzo, Daniel John. 2002. ``Racing with Uncertainty: A Patent Race
Experiment.'' \emph{International Journal of Industrial Organization} 20
(6). Elsevier: 877--902.

\end{document}