This is just going to be a quick overview of the various things I’ve done during this rotation and the results that I’ve generated. I started off trying various straight voting methods for sorting through the AMT data. The goal was to hit 87% consistency with the gold standard, which is the best performance we get from a simple voting method. I tried 3 methods:1.	Experience voting: each turker vote is weighted by how many total annotations they’ve marked. (TestExperienceVoting.java)2.	Trust Voting: where each turker vote is weighted by a trust score, or their f-score against the gold standard for the training documents. (TestTrustVoting.java)3.	Crowd Agreement: Where each turker vote is weighted as a function of their agreement with the other turkers, e.g. how often do they agree with turkers that have marked the same documents that they have.  (TestCrowdAgreement.java)Unfortunately, all of these methods were able to match, but not exceed, the performance of the simple voting method as measured by f-score. After trying the various weighted voting methods, we tried exploring the “hotnet” method. The goal of the hotnet method was to create a net of tokens from the document that would “bleed” “heat” from one token to the next in an attempt to pull out annotations that were not marked by many users but were correct. The work on this method is all contained in the org.scripps.hotnet package. The hotnet method worked by tokenizing each document by using various tokenizing methods (I ended up tokenizing every word and punctuation mark for all of my real data). Each turker marked annotation was then also tokenized, and each turker that marked a specific token was counted as a single unit of ‘heat’. The hotnet was then itereated through time, where the ‘heat’ of each token would bleed into the tokens adjacent to it by simple diffusion. The initial method for creating annotations from tokens was to just concatenate all adjacent tokens above a certain heat threshold into annotations. The results for this method are in the output/HotNet folder. 	However, we decided that that method of creating annotations would cause many problems with syntax / creating annotations that no sane turker would make. Two other methods were tried. The first was to take each annotation my token combiner spat out and find the user-marked annotation that was most similar to it in terms of character positions. The results for this one are in the  output/user_corrected folder. The second method was to take each turker marked annotation and find the average token heat for that annotation. Each annotation that had an average token heat above a certain threshold was considered in the evaluation. The results for these are contained in output/avgAnnoHeatMethod folder. 	Unfortunately, the hotnet really did not work like we wanted. The problems were numerous, but chief among them were the following:*	There is no good initial heat distinction correctly and incorrectly marked tokens. (No discernable pattern, at least)*	Single turkers could greatly disrupt the heat diffusion process, e.g. creating 15-15-15-1-15-15 patterns that would cause the single ( and usually incorrectly marked) token to create annotations that otherwise wouldn’t be considered. *	“Island” tokens, ones that did not have any token neighbors on either side with heat, lost lots of heat from diffusing out into the 0-heat sinks on either side. I curbed this using 0-heat walls, which caused tokens to not lose heat to neighboring tokens that had 0 initial heat.  (This was used for everything but the HotNet folder)*	In terms of performance, iterating through the hotnet in time seemed to almost always decrease the performance of the created annotations against the gold standard data. In many cases a simple token voting method (time = 0) was the best performer. I tried one other “net” method, which was the Annotation Net idea (AnnoNet). In this method, each unique user-marked annotation was given an initial heat based off how many turkers marked that exact annotation. Then, the annotations exchanged their heat depending on how much overlap they exhibited. The overlap was done by character with two different methods, coverage and encapsulation. Coverage  gave annotation A a fraction of Annotation B’s heat depending on how much A covered B. Encapsulation gave Annotation A a fraction of B’s heat depending on how much B covered A, or how much A was “encapsulated” in B. Here’s a graphic:---------  A: length 9   --         B: length 2In encapsulation, A would receive 2/9 of B’s heat. In coverage, A would receive 2/2 of B’s heat. The opposite is true for B, in encapsulation B would receive 2/2 of A’s heat and 2/9 of A’s heat in coverage. 	Once again we did not get the results we wanted from this method, although there could be some fleshing out here. The results are in the output/annoNets folder. Finally, I did some cost analysis in order to see how performance of documents improved versus how many annotators marked. I did a randomized method, individual “annotator events” (single turker marking a single document) were randomly chosen and added to a document, each document was considered done when it satisfied two requirements:1.	Each user-marked annotation either had enough votes above a fractional threshold for acceptance or below a rejection threshold.2.	A minimum number of turkers had marked the document.The initial results for these data look promising; it appears that there is a point at which additional turkers make minimal improvements in performance.  Several plots, which are in the output/costAnalysisByDoc folder, show how for certain parameters performance does not necessarily improve with additional turkers; some documents continue to show poor performance even when they have the maximum number of turkers annotate them. In general, a minimum acceptance of around 0.4-0.6, a maximum rejection of 0.0-0.2, and a minimum number of turkers of 4-5 will give good results for a majority of the papers without incurring high costs. The plots are of two types. One shows the Average Cost of a document vs. the Average F score, with a red mean line. The title is the total average f score. (F score is evaluated for each individual document then total average is taken)The second type of plot shows the documents ordered by their average cost. The black dots and line show the average cost curve. The F-score for each document is simultaneously plotted as a red dot. For these plots the f-score is scaled by 15 to fit the axes. 