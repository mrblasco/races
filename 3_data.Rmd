The experimental design
=====================

The field experiment was conducted between March 2 and 16, 2016. The context of the experiment was an online programming contest. In an online programming contest,  participants compete to write source code that solves a designated problem. These contests are quite common and xxxx either as a tournament or a race competition. 

The contest was hosted on the online platform Topcoder.com. Since its launch in 2001, Topcoder.com administers on a weekly basis several competitive programming contests for thousands of competitors from all over the world. Typical assigned problems are data science problems (e.g., classification, prediction, natural language processing) that demand some background in machine learning and statistics. All Topcoder members (about 1M registered users in 2016) can compete and attain a "rating" that provides a metric of their ability as contestants. Other than attaining a rating, the competitors having made the top five submissions in a competition are typically awarded a monetary prize the extent of which depends on the nature and complexity of the problem but is generally between $5,000 and $20,000.

In this study, we worked together with researchers from the United States National Health Institute (NIH) and the Scripps Research Institute (SCRIPPS) to select a challenging problem for the experimental programming competition. The selected problem was based on an algorithm called BANNER built by NIH [@leaman2008banner] that uses expert labeling to annotate abstracts from a prominent life sciences and biomedical search engine, PubMed, so disease characteristics can be more easily identified. The goal of the programming competition was to improve upon the current NIH's system by using a combination of expert and non-expert labeling, as described by @good2014microtask.

The competition was announced on the platform and to all community members via email. A preliminary online registration was required to enroll in the competition, which resulted in 340 pre-registered participants. Among the pre-registered members, we selected the 299 who had registered to a programming contest at least once before the present contest. This choice was to ensure that participants were xxxx.

Participants were then randomly assigned to separate groups of 10 or 15 people. In each of these groups, contestants were given access to a "virtual room" that is a private web page listing handles of the other participants of the group, a leaderboard updated regularly during the competition, and a common chat that they can use to ask clarifying questions (visible to everyone in the group) with respect to the problem at hand. 

A problem statement containing a full description of the algorithmic challenge, the rules of the game, and payoffs was published at the beginning of the submission phase. The submission phase was of 8 days in which participants could submit their computer programs. Each submission was automatically scored and feedback in the form of preliminary scores was published regularly on the website via the leaderboard.

Groups were randomly assigned to one of three different competitive settings: a race, a tournament, and a tournament with a _reserve target_, which is the lowest acceptable score by the platform for a submission to be awarded a prize. 

The experimental design is summarized by the Table XXXX.

```{r experimental-design, results='asis'}
tab <- with(races, table(treatment, room_type))
render.table(tab, "Experimental design", "tab: experimental design")
```

In all groups, the first placed competitor was awarded a prize of $1,000, and an additional, consolatory prize of $100 was awarded to the second one. 

In a race competition, however, the first to achieve a score equal to xxxx was placed first. The level was chosen xxxx. 

In a tournament, xxxx. 

Finally, in a tournament with reserve,  xxxx. 

Additional grand prizes of xxxx were awarded to the top xxx in every treatment.

Data
---------

The bulk of our data comes from the online Topcoder's profile of each participant. This profile typically includes information of when the member registered to the platform,  the current rating in a variety of different competitions, the number of past competitions, and so on. Additional demographic information,  was collected via a  pre-registration survey where competitors were asked to state their gender, age, geographic origin, etc. Participants were als also asked a self-reported measure of risk aversion [xxx] and to forecast how many hours they expected to compete in the next few days of the challenge(the exact question was: "looking ahead xxxx").  

Finally, we also asked participants to respond to a survey at the end of the submission phase. In this final survey, they were asked to look back and tell us their best estimate of the time spent working on the problem. Also, we gathered comments on the xxx. And questions such as xxxx. 

Table XXX summarizes the data. 

```{r, results='asis'}
index <- c("Country"="country_name"
          , "Age"="age"
          , "Gender"="gender"
          , "Highest degree"="educ"
          , "Pref. programming lang."="plang"
)
foo <- sapply(races[, index], function(x) summary(x, maxsum=5)) ## get summary of each var
foo.test <- sapply(races[, index], function(x) chisq.test(table(x, races$treatment) + 1, simulate.p.value=TRUE)$p.val)
bar <- matrix(nrow=length(unlist(foo)), ncol=4) # Initialize

bar[, 1] <- gsub("\\..*","", names(unlist(foo))) ## 1st col has var name
duplicated.rows <- duplicated(bar[, 1])
bar[, 1][duplicated.rows] <- "" ## don't repeat variable names
bar[, 2] <- gsub(".*?\\.","", names(unlist(foo))) ## second column has factor levels
bar[, 3] <- as.numeric(unlist(foo))
bar[, 3][is.na(bar[, 3])] <- 0 ## in case anything has been coerced
bar[!duplicated.rows, 4] <- format(foo.test, nsmall=3)

colnames(bar) <- c("Variable","Response Category","Frequency", "P-value")
rownames(bar) <- rep("", nrow(bar))
bar <- xtable(bar)
align(bar) <- c("l","l","p{4cm}","r","r") ## make sure factor levels don't take up more than 4cm

print(bar, tabular.environment = "longtable", include.rownames=FALSE, size="footnotesize",     
  floating=FALSE, # 
  hline.after=NULL, #
  add.to.row=list(pos=list(-1,0, nrow(bar)), #
  command=c('\\toprule ', # use nice booktabs formatting
  '\\midrule \\endhead ', # with running headers from longtable
  '\\bottomrule ')))

select <- c("Algo rating"="algorating"
          , "Algo competitions"="algoevents"
          , "Algo registrations"="algoreg"
          , "MM rating"="mmrating"
          , "MM competitions"="mmevents"
          , "MM registrations"="mmreg"
          , "Time zone"="timezone"
          , "Risk aversion"="risk"
)
stargazer(races[, select], header=FALSE, title='Descriptive statistics', align=TRUE, covariate.labels=names(select), digits=1)
# HERE TABLE Summary statistics!
```

