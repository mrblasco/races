<p>&nbsp;</p>
<p><b>Prizes [TO BE DISPLAYED IN ROOMS WITH THE RACE COMPETITION]</b></p>

<p>
The <b>first 2 coders of your room</b> to achieve a score greater than [SCORE THRESHOLD] (according to system test results) will be awarded room prizes: </p>
<ol>
<li>place: $1,200</li>
<li>place: $200</li>
</ol>
<p>In addition to the room prizes, the <b>first 3 coders across all rooms with the same competition style</b> to achieve a score greater than [SCORE THRESHOLD]  will receive the following grand prizes:</p>
<ol>
<li>place: $3,000</li>
<li>place: $1,500</li>
<li>place: $500</li>
</ol>
<p>So you can win up to $4,200. Only submissions that score at least [SCORE THRESHOLD] are eligible for prizes.</p>
<p><b>Prizes [TO BE DISPLAYED IN ROOMS WITH TRADITIONAL MM COMPETITION]</b></p>
<p>
The <b>best 2 performers of your room</b> (according to system test results) will be awarded room prizes: </p>
<ol>
<li>place: $1,200</li>
<li>place: $200</li>
</ol>
<p>In addition to the room prizes, <b>the best 3 performers across all rooms with the same competition style</b> will receive the following grand prizes:</p>
<ol>
<li>place: $3,000</li>
<li>place: $1,500</li>
<li>place: $500</li>
</ol>
<p>So you can win up to $4,200. </p>

<p><b>Prizes [TO BE DISPLAYED IN ROOMS WITH THE MM COMPETITION W/RESERVE]</b></p>
<p>
The <b>best 2 performers of your room</b> that score at least [SCORE THRESHOLD] (according to system test results) will be awarded room prizes: </p>
<ol>
<li>place: $1,200</li>
<li>place: $200</li>
</ol>
<p>In addition to the room prizes, the <b>best 3 performers across all rooms with the same competition style</b> that score at least [SCORE THRESHOLD] will receive the following grand prizes:</p>
<ol>
<li>place: $3,000</li>
<li>place: $1,500</li>
<li>place: $500</li>
</ol>

<p>So you can win up to $4,200. Only submissions that score at least [SCORE THRESHOLD] are eligible for prizes. </p>


<p>&nbsp;</p>
<p><b>Overview</b></p>
<p>
The BioNLP (Biomedical Natural Language Processing) community, represented by the academic group from The Scripps Research Institute, is testing the limits of crowdsourcing for generating annotated corpora within the biomedical domain and for doing information extraction directly. To accomplish these tasks effectively, algorithms are needed that can learn to accurately merge data collected from multiple annotators of varying quality and integrate this data into predictive models.</p>
<p>There is an already developed open-source supervised learning system called BANNER that achieves a good level of prediction power after being trained on subset of abstracts, manually annotated by experts. However, the training capabilities of the current algorithm are restricted to a very small (expert) dataset, which is limited by expensive expert's time. There is an idea that this limitation can be overcome if we teach BANNER algorithm how to further improve its accuracy by training on <a href="http://www.mturk.com">MTurk</a>-annotated abstracts, potentially available in the data in much larger quantities.</p>
<p>In this contest, the goal is to improve BANNER accuracy by teaching it on MTurk-annotated abstracts. </p>

<p>&nbsp;</p>
<p><b>Available Software</b></p>
<p>All the software is available through the following <a href="https://bitbucket.org/NTL_CIL_Harvard/banner">git repository</a></p>
<p>The BANNER Java source code is available in the <a href="https://bitbucket.org/NTL_CIL_Harvard/banner/src/5fd83e65de5fc09b26d9576dd84e25469cbee2b5/banner_source/?at=master">banner_source</a> folder of the repository for usage and modifications. The file BANNER_BioC.sh  in the banner_source/scripts folder can be used to execute BANNER on BioC formatted training and testing files.</p>
<p>More detail on the BioC format can be found <a href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3889917/">here</a>.</p>
<p>A tutorial on getting started with BANNER can be found <a href="https://bitbucket.org/NTL_CIL_Harvard/banner/src/5fd83e65de5f/tutorial/?at=master">here</a>.</p>
<p>The software trains a model by taking a number of annotated abstracts as input. The trained model can then be used to annotate abstracts that have not been annotated before.</p>
<p>Example code can be download <a href="https://bitbucket.org/NTL_CIL_Harvard/banner/src/5fd83e65de5fc09b26d9576dd84e25469cbee2b5/crowd_words/?at=master">here</a> that performs the following:</p>
<ul>
<li>Reads in a MTurk annotation file</li>
<li>Applies a simple voting procedure to make reasonably good approximations of a non-redundant training corpus and stores them as new BioC files</li>
<li>The BioC files can be used directly in order to train BANNER.</li>
</ul>

<p>&nbsp;</p>
<p><b>Available Data</b></p>
<p>The training and the testing data are available in the same <a href="https://bitbucket.org/NTL_CIL_Harvard/banner">git repository</a> in their respective folders:
<a href="https://bitbucket.org/NTL_CIL_Harvard/banner/src/5fd83e65de5fc09b26d9576dd84e25469cbee2b5/training_data/?at=master">training_data</a> and  <a href="https://bitbucket.org/NTL_CIL_Harvard/banner/src/5fd83e65de5fc09b26d9576dd84e25469cbee2b5/testing_data/?at=master">testing_data</a>.</p>
<p>Some of the abstracts are labelled by experts, while some are labelled by MTurk workers (15 annotators per abstract). You will be given the following training data:</p>
<ul>
<li>493 abstracts annotated by both experts and MTurk.</li>
<li>693 abstracts annotated only by experts and 900 annotated only with MTurk.</li>
</ul>
<p>You will be given 2300 abstracts that needs to be annotated. Only a subset of 300 will be used for scoring. The remaining abstracts will be used to prevent contestants from manually annotating the abstracts. The example case will only score 5 abstract, for provisional scoring 195 abstracts will be used and the remaining 100 abstracts will be used for system tests.</p>
<p>In order to be eligible for a prize, you will be requested to submit all the source code and data that you used to generate your results. We must be able to re-create your submission results from the provided source.
</p>

<p>&nbsp;</p>
<p><b>Implementation</b></p>
<p>
In this contest, the training and testing data will remain the same for example, provisional and system tests. You have to generate your results offline and embed them in your submission.</p>
<p>Your annotate method should return the contents of the generated annotated abstracts. For each annotation, 3 integer values (ID, Offset, Length) need to be added to your return array. The ID value indicates the ID of the abstract while the offset relates to the starting index of the section in the abstract followed  by the length in characters for the annotation. The provided software will generate a submission file for you.</p>
<p>Steps that you can take in order to generate a submission file by training BANNER on some data:</p>
<ul>
<li>Get the banner code and data from BitBucket: <pre>git clone https://bitbucket.org/NTL_CIL_Harvard/banner</pre></li>
<li>Browse to the banner_source folder, compile the code: <pre>ant -buildfile build.xml</pre></li>
<li>Train a model: <pre>java -cp 'lib/*' banner.eval.BANNER train config/banner_bioc.xml</pre></li>
<li>Test the model: <pre>java -cp 'lib/*' BANNER_BioC config/banner_bioc.xml data/test_file.xml out.xml</pre></li>
</ul>
<p>A <i>BannerAnnotate.java</i> file will be created that can be submitted. 
The <i>BannerAnnotatorVis.java</i> can be compiled and executed together with the generated file in order to test your submission with the example test locally on your PC.
</p>

<p>&nbsp;</p>
<p><b>Scoring</b></p>
<p>
Your score will be calculated by matching your annotations with our expert annotations. <a href="http://en.wikipedia.org/wiki/F1_score">F-Score</a> over the set of abstracts will be used and multiplied by 1000000.</p>
<p>Code that calculates your score can be downloaded <a href="https://bitbucket.org/NTL_CIL_Harvard/banner/src/5fd83e65de5fc09b26d9576dd84e25469cbee2b5/mm_tester/?at=master">here</a>.</p>
<p>You can see these scores for example test cases when you make example test submissions. If your solution fails to produce a proper return value, your score for this test case will be 0.</p>

<p>&nbsp;</p>
<p><b>Use of Git</b></p>
<p>
As part of this contest, we would like for you to use Git <a href="http://git-scm.com/downloads">(http://git-scm.com/downloads)</a> and to make daily code commits as you develop your solution. When the competition is over, you can share with us your local repository and this will allow us to examine the history of your code commits.
</p>

<p>&nbsp;</p>
<p><b>Special Rules</b></p>
<p>
<ul>
<li>In order to receive the prize money, you will need to fully document your code and explain your algorithm. If any parameters were obtained from the training data set, you will also need to provide the program used to generate these parameters. There is no restriction on the programming language used to generate these training parameters. Note that all this documentation should not be submitted anywhere during the coding phase. Instead, if you win a prize, a TopCoder representative will contact you directly in order to collect this data.</li>
<li>You may not use any external (outside of this competition) source of data to train your solution.</li>
<li>You are not limited to using BANNER, you can use any other publicly available tool or write your own software to produce the output.</li>
<li>You are not allowed to manually annotate the testing data.</li>
</ul>
</p>

