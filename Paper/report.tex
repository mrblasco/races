\documentclass[10pt, titlepage]{article}
\usepackage{beamerarticle}
\usepackage[utf8]{inputenc}
\usepackage{amssymb,amsmath}

% Page settings
\usepackage[letterpaper, margin=1in]{geometry}
\usepackage{times} % palatino, lmodern
\usepackage{setspace}
\onehalfspacing
%\doublespacing  % \singlespacing 

% Appendix
\usepackage{appendix}

% Figure caption on top
% See: https://github.com/axelsommerfeldt/caption/blob/master/doc/caption-eng.pdf
\usepackage[bf,textfont=sc,position=above]{caption}

% Line numbers
%\usepackage{lineno}
%\linenumbers

% Links
\usepackage{hyperref}
\hypersetup{%
  colorlinks=false,% hyperlinks will be black
  linkbordercolor=red,% hyperlink borders will be red
  pdfborderstyle={/S/U/W 1}% border style will be underline of width 1pt
}

% Tables
\usepackage{array,booktabs,longtable,rotating}

% Position tables {here, top, bottom, page}
\makeatletter
\def\fps@table{htbp}
\makeatother

%% ... at the end of paper
\usepackage{float,endfloat}

% Graphics
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother



\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}


%\usepackage{xcolor}
%\usepackage{framed}
%\colorlet{shadecolor}{orange!15}
\usepackage{array}
\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}p{#1}}

%\usepackage{floatrow}
%\floatsetup[table]{capposition=top}
%\floatsetup[figure]{capposition=top}

% Math environments
\newtheorem{proposition}{Proposition}
\newtheorem{define}{Definition}


% Thresholds, limits, bounds, etc.
\newcommand\deadline{\bar{t}}
\newcommand\target{\underline{y}}

% Competitions
\newcommand\race{\text{race}}
\newcommand\tournament{\text{tour}}

% Cost functions
\newcommand\ctime{c_{\tau}}
\newcommand\cscore{c_{y}}
\newcommand\cability{c_{a}}
\newcommand\costs{\cability(a_i)\cscore(y_i)\ctime(t_i)}

% Distribution of types
%\newcommand\ability{a_i}
\newcommand\marginaltype{\hat{a}}
\newcommand\mtype{\hat{a}}
\newcommand\lotype{\underline{a}}
\newcommand\hitype{\bar{a}}



% Derivatives
\newcommand\dystar{\frac{\partial y^*(x,\target)}{\partial\target}dF_{N:N}(x)}

\title{Races or Tournaments? Theory and Evidence from the Field\thanks{Blasco: Harvard Institute for Quantitative Social Science, Harvard
University, 1737 Cambridge Street, Cambridge, MA 02138 (email:
\href{mailto:ablasco@fas.harvard.edu}{\nolinkurl{ablasco@fas.harvard.edu}}).}}
\providecommand{\subtitle}[1]{}
\subtitle{{[}PRELIMINARY AND INCOMPLETE{]}}
\author{Andrea Blasco \and Kevin J. Boudreau \and Karim R. Lakhani \and Michael Menietti}
\date{Last updated: 28 August, 2017}

\begin{document}
\maketitle
\begin{abstract}
We examine the performance of two different choices of contest design:
the race (where the winner is the first to achieve a minimum quality)
and the tournament (where the winner is the one with the highest quality
in a given period). After characterizing the optimal design, we report
results of a field experiment conducted to compare the performance of
three alternatives motivated by theory: the race, the tournament, and
the tournament with a minimum quality requirement. Outcomes in a race
are of comparable quality, supplied faster, and with lower participation
rates. Based on these findings, we show the optimal design under several
counterfactual situations.

\smallskip\noindent 
JEL Classification: M15; M52; O31.

\smallskip\noindent 
Keywords: races; tournaments; contest theory; crowdsourcing; innovation.
\end{abstract}


\clearpage
\tableofcontents
\setcounter{tocdepth}{2}
\clearpage

\section{Introduction}\label{introduction}

Many economic situations---in the public sector, business, sports, and
academia---are decided by either a tournament---a competition to be
best---or a race---a competition to be first. In the public sector,
government-sponsored races and tournaments have lead to major
improvements in numerous areas like agriculture (xxx), navigation (xxx),
and aviation (xxx), exerting a tremendous impact on economic growth. In
business, both types of competition are regularly used by managers to
raise workers' motivation and performance (xxx). In academia, they are
used by philanthropic organizations to increase the productivity of
scientists via competitive research grant awards (xxx) and individual
prizes for merit (xxx).

While sometimes the competition being a race or tournament is due to
external factors like regulation or tradition, in many other situations
contest designers can choose how they want the contestants to compete.
This is the case, for example, of governmental agencies procuring public
services; executives designing internal remuneration schemes for their
employees; organizations sponsoring external online competitions to
engage the members of large online communities like in crowdsourcing
contests (xxx).\footnote{online} Understanding the consequences of
adopting different competition types is thus crucial for contest
designers to make appropriate decisions.

In this paper, we investigate this question by focusing on the possible
trade-off between choosing a race and a tournament. The races vs
tournaments issue is examined from two different perspectives. First, we
consider the role of contestants' strategic interactions under these two
competitive regimes. Then, we characterize the optimal design and we
examine how it depends upon the contest designer's preferences towards
the time and quality of contestants' effort. In particular, contest
designer's revenues depend on how contestants perform a task (e.g.,
solving a problem) that requires time to perform and can be performed at
varying levels of quality.

To investigate these issues, we proceed in two ways. Firstly, we develop
a contest model that encompasses both the race and the tournament in a
single framework. In particular, our approach extends the contest model
introduced by Moldovanu and Sela (2001) to a situation in which contest
designers care about both the timing and quality of effort. Exploring
the duality of the model, we compare equilibrium behaviors under both
competitive formats and characterize the optimal choice for the contest
designer. Secondly, we design and execute an experiment to test key
implications of the theory in the field. More specifically, the
experiment compares and contrast behaviors under three different
competition regimes: races, tournaments, and tournaments with a minimum
quality reserve (called tournaments with reserve).

To fix ideas, consider the following real example. After the World
Health Organization (WHO) issued a report in 2014 calling antimicrobial
resistance\footnote{Antimicrobial resistance is ``the ability of a
  microbe to resist the effects of medication previously used to treat
  them'' (Wikipedia). It is often called ``antibiotic'' resistance
  because it is a phenomenon likely caused by a misuse of antibiotics.
  These resistant microbes are increasingly difficult to treat.} ``a
major threat to public health,'' the United Kingdom and the European
Commission decided independently to sponsor two different innovation
contests aimed at boosting the development of solutions to the problem.
The contest sponsored by the UK government was called the ``Longitude
Prize: Reduce the use of antibiotics.'' The one sponsored by the EU was
called ``Horizon prize for better use of antibiotics.'' Though both the
contest designers were facing a seemingly identical problem --- it is
striking how similar the contest names ended up being, the two contests
were very different from a contest design perspective. The Longitude
prize was a race competition: it awarded a prize to the first team to
submit a solution satisfying some pre-specified effectiveness criteria.
The Horizon prize was a tournament: teams were facing a fixed 12 month
deadline within which they could submit solutions and the best solution
relative to the other submissions was awarded a prize.

The reasons why contest designers --- facing identical problems ---
might end up with very different designs is unclear, raising questions
about the determinants of contest designers' choices. We argue that
there are two key factors that shape the decisions of contest designers.
First, contest designers act to maximize their expected revenues.
Expected revenues depend upon the intensity of their preferences for
time and quality of effort --- e.g., developing effective solutions to
antimicrobial resistance in the shortest time possible. According to
these preferences, contest designers have (a) to schedule deadlines to
create conditions of time pressure for contestants; and (b) select
minimum criteria to avoid rewarding solutions of unsatisfying quality
and (c) set up either a competition to be first or a competition to be
best. By setting deadlines and minimum-quality criteria, contest
designers affect the incentives for contestants to enter the
competition, as well as the competitive pressure on their rivals. By
choosing how contestants will compete, contest designers direct
competitive pressures towards improving only one dimension of the
problem, while keeping the efforts in other dimensions fixed. These
different aspects --- deadlines, quality requirements, and competition
styles --- interact with one another, greatly influencing the manner in
which contestants behave.

While races and tournaments are similar in various respects, it is
unclear which one will dominate the other. For example, tournaments can
incentivize fast solutions of high quality by fixing appropriate
deadlines. However, a tournament with a ``too short'' deadline may
implicitly raise entry costs, which in turn will reduce the competitive
pressure to increase quality. For similar reasons, a race with a ``too
high'' quality requirement might incentivize performances of higher
quality, but it might also limit participation, thus reducing the
competitive pressure on the time to perform the task.

Another important factor for contest designers is how the choice of
tournaments vs races will affect the efficiency of the contest. In
particular, one good reason to limit entry in a contest is to decrease
inefficiencies like duplication costs. As suggested by Fullerton and
McAfee (1999), it is sometimes more efficient to design contests that
limit competition to only the two top contestants, instead of having an
open competition. This is because having many low-skilled contestants
may be just a waste of resources, as they keep essentially unaltered the
competitive pressure on the top contestants while raising duplication
costs. Also tournaments and races, by imposing deadlines and minimum
quality requirements to win, limit competition in different ways and can
be designed to deliver ``efficient'' results. This issue seems most
relevant for firms and online platforms running multiple challenges at
once among a fixed set of potential contestants, whereby seeking to
implement an efficient allocation of workers/platform members to tasks.

Our theoretical approach follows traditional contest models, as in
Moldovanu and Sela (2001). In this setting, contests have an all-pay
structure by which participants pay an immediate cost for an uncertain
future reward. The decision of timing and quality is made under the
uncertainty of the costs of the rivals. The contest designer wants to
maximize revenues and has preferences for both time and quality.
Following the analysis of the model, we show that the optimal design
depends on the number of participants and the concavity of their cost
function. We also show conditions under which races dominate
tournaments--- even when the contest designer does not care about the
timing --- and vice versa. We also characterize outcomes when contest
designers can introduce a minimum quality reserve (i.e., tournaments
with reserve) showing that tournaments with (sufficiently high) reserve
always dominate races in terms of performance.

Our empirical approach involves the design and execution of a field
experiment. The context of our field experiment is an online programming
competition run on the online platform Topcoder at the end of 2016. In a
typical programming competition, participants compete writing source
code solving a given problem for winning cash prizes. We worked together
with researchers from the United States National Health Institute (NIH)
and the Scripps Research Institute (SCRIPPS) to select a challenging
problem for the contest. The selected problem was based on an NIH's
algorithm called BANNER (Leaman, Gonzalez, and others 2008) that uses
expert labeling to annotate abstracts of a repository of life sciences
articles (e.g., PubMed), so disease characteristics can be more easily
identified. The goal of the programming competition was to improve upon
the current NIH's system by using a combination of expert and non-expert
labeling, as described by Good et al. (2014). To encourage
participation, we offered \$40,000 in cash prizes to participants.

Our intervention consisted in sorting at random participants into
independent virtual rooms of 10 or 15 people. These virtual rooms were
then randomly assigned to one of three different competitive settings:
(a) tournaments, (b) races, and (c) tournaments with reserve.
Contestants were thus facing the same task to perform, same cash
incentives, etc. The only difference was how they were competing against
one another.

We find that tournaments yield about \(1.3\) times higher participation
rates than races and tournaments with reserve, as predicted by theory.
We find stark differences in terms of ability between entrants and
non-entrants. But only insignificant differences in entrants between
tournaments and races. That is, the conditional probability of entering
the competition for a competitor with a given ability was the same in
races and tournaments. This lack of skill-based sorting does not explain
the excess of entry in tournaments, suggesting other sorting mechanisms
than problem-solving ability are at play.

We also examine treatment differences in performance. While we find only
insignificant differences in the quality of outcomes across competition
styles, the variance was higher in tournaments compared to races.
Suggesting that, although tournaments may cost more than races on
average, they are more relatively more likely to generate outcomes of
very high value. At the same time, races offer other advantages, as
solutions are submitted quicker in a race relative to tournaments.

In comparing races and tournaments w/reserve, we detect a
``substitution'' effect as high ability competitors were more likely to
enter races compared to tournaments w/reserve --- while keeping entry
rates constant across the treatments. This substitution effect may
explain the relatively higher volatility of outcomes in the tournaments
w/reserve compared to races.

Finally, we use a structural approach to estimate our contest model.
This has two main purposes (a) provide a basis for the external validity
of our results to other settings and (b) enable the analysis of
efficiency problems using counterfactual simulations based on the
estimated model.

Implications of these findings for contest designers are discussed at
the end of this paper.

\section{Literature}\label{literature}

This paper is related to the contest theory literature in economics
(Dixit 1987 Baye and Hoppe (2003), Parreiras and Rubinchik (2010),
Moldovanu and Sela (2001), Moldovanu and Sela (2006), Siegel (2009),
Siegel (2014)). It also relates to the literature on innovation contests
(Taylor 1995, Che and Gale (2003)). And the personnel economics approach
to contests (Lazear and Rosen 1981, Green and Stokey (1983), Mary,
Viscusi, and Zeckhauser (1984)).

Empirically, Dechenaux, Kovenock, and Sheremeta (2014) provide a
comprehensive summary of the experimental literature on contests and
tourments. Large body of empirical works have focused on sports contests
Szymanski (2003). More recently, field experiments conducted inside
firms have studied workers response to internal contests in terms of
productivity --- individual (xxxx) and in teams (xxxxx) --- and ideation
(xxxx). A growing number of studies have also focused on similar
incentives but applied to ``virtual'' environments like online contests
(xxxx).

In our structural approach to estimate contest model, this paper follows
results from classical econometrics of auctions (Paarsch 1992, Laffont,
Ossard, and Vuong (1995), Donald and Paarsch (1996)) and more recent
ones (Athey, Levin, and Seira 2011, Athey and Haile (2002), and Athey
and Haile (2007)).

Several aspects of contest design have been investigated, including the
optimal prize structure {[}XXX, xxxx, xxxx{]}, number of competitors
{[}XXX, XXX{]}, and imposing restrictions to competition such as minimum
effort requirements {[}XXX, XXX{]}. Also, a great deal of theoretical
models of races and tournaments have been developed and applied to a
wide range of economic situations including patent races {[}xxx{]}, arms
races {[}xxx{]}, sports {[}xxx{]}, the mechanism of promotions inside
firms {[}xxxx{]}, sales tournaments {[}xxxx{]}, etc.

Harris and Vickers (1987), Grossman and Shapiro (1987) investigate the
dynamics issues patent races where the interest is how firms compete for
a patent. Bimpikis, Ehsani, and Mostagir (2014) looks at the problem of
how to design an information structure that is optimal when the contest
is a race and innovation is uncertain (encouragement and competition
effect). In the laboratory, Zizzo (2002) finds poor support to
predictions of dynamic xxxx. In general we do not know much about the
dynamic aspect of contests.

As pointed out by Baye and Hoppe (2003), many of these models of
tournament and race competitions are specific cases of a more general
``contest games.'' And sometimes it is possible to design one or the
other in a way to exploit a ``duality.'' This type of duality suggests
that races and tournaments are ``strategically equivalent'' from the
point of view of contestants. That is, ex-ante they might expect similar
payoffs. While this is a very important theoretical result, it does not
say how contest designers should or can exploit this duality, an issue
that remains largely unknown. Relative to Baye and Hoppe (2003), we
provide results on the problem of maximization of the contest designer,
showing that even when tournaments and races yield same ex-ante payoff
to contestants, they might not yield same payoffs (revenues) for contest
designers.

\newcommand\reserve{\text{res}}
\newcommand\competition{c}
\newcommand\ability{a}
\newcommand\performance{y}
\newcommand\timing{t}
\newcommand\Timing{T}
\newcommand\Performance{Y}
\newcommand\marginal{\underline{a}}


\section{The model}\label{the-model}

Consider a generalization of the contest game introduced by Moldovanu
and Sela (2001). In this game, \(n\) contestants (\(i=1,..., n\)) are
competing for \(p\) money prizes (\(k=1,...,p\)) of value
\(v_1\geq v_2\geq ...\geq v_p\geq0\) with total value normalized to one
\(\sum_{k=1}^p v_k =1\). They simultaneously decide \((a)\) how fast to
perform a given task and \((b)\) the quality of their performance (e.g.,
the time to solve a problem and effectiveness of the solution). The
timing and performance for each contestant \(i\) are denoted by
\(\timing_i\in(0,\deadline]\) and \(\performance_i\in[0,\bar y]\),
respectively. The upper limit of the time interval \(\deadline>0\) is
the deadline of the contest and the upper limit of the performance
interval \(\bar y>0\) is the highest technically feasible performance
level (e.g., in a prediction task, making 100 percent accurate
predictions).

Each contestant \(i\) incurs a cost of performing the task
\(c_i = C(\performance_i, \timing_i) / \ability_i\) given by the cost
function \(C(\performance_i, \timing_i)\) scaled by an individual
ability parameter \(a_i\) reflecting skills, time constraints, and other
elements affecting contestant \(i\)'s performance.

The cost function \(C(\cdot)\) is a decreasing function of time, an
increasing function of quality, is multiplicative (i.e., the higher the
quality the more time it takes to perform the task), and is zero when
performance is zero \(C(y_i=0, \cdot)=0\) (i.e., the contestants can
drop out of the competition at no cost).

To make things more concrete, we consider the following functional form:
\[
    C(\performance, \timing) = \performance^\alpha \timing^\beta,
\] where the elasticity of the cost function with respect to its inputs
is denoted by the parameters \(\alpha\geq1\) and \(\beta\leq0\).

The individual ability \(a_i\) of each contestant is drawn at random
from a common distribution function \(F_A(\cdot)\) with (absolutely
continuous) density \(f_A(\cdot)\) on the unit interval. Players observe
privately the realization of their own ability (their \emph{type})
before making their choices of how much cost to incur.

Each contestant \(i\) gets a payoff that is either \(v_k - c_i\), if
\(i\) wins the kth prize, or is \(-c_i\), if \(i\) does not win a prize.
Winners are determined according to a set of competition rules, which
are common knowledge. We consider three kinds of competition rules ---
called ``race,'' ``tournament,'' and ``tournament with reserve''
competitions --- defined below.

\begin{definition}[Races]
In a race competition, contestants are required to achieve a minimum target performance $\target\in(0, \bar y)$. The first to achieve a performance equal or higher than $\target$ wins the first prize; the second to achieve a performance equal or higher than $\target$ wins the second prize; and so on until either all the prizes are awarded or there are no more contestants who have achieved a performance equal or higher than $\target$.
\end{definition}

\begin{definition}[Tournaments]
In a tournament competition, contestants face no minimum performance requirements. So, everyone is eligible for prizes. The contestant that achieves the highest performance relative to others wins the first prize; the contestant that achieves the second highest performance wins the second prize, and so on until all prizes are awarded.
\end{definition}

\begin{definition}[Tournaments with reserve]
In a tournament with reserve competition, contestants are required to achieve a minimum target performance $\target_{\reserve}\in(0,\bar y)$ and, unlike races, prizes are awarded based on performance. The contestant that achieves the highest performance relative to others wins the first prize; the contestant that achieves the second highest performance wins the second prize, and so on until all prizes are awarded  or there are no more contestants who have achieved a performance equal or higher than $\target_{\reserve}$.
\end{definition}

\subsection{Equilibrium}\label{equilibrium}

We now solve the model for the unique symmetric Bayesian Nash
equilibrium (hereafter equilibrium) separately for each kind of
competition rules --- races, tournaments, and tournaments with reserve.

\subsubsection{Races}\label{races}

Consider first a race competition. A key observation in this case is
that any performance below the target \(\target\) gives a payoff of zero
and any performance equal or above \(\target\) gives a constant
probability of winning a prize. Since the cost function is increasing in
performance, it is a dominant strategy for the contestants to either
drop out the competition --- choosing a cost of zero (i.e., setting
\(y_i^*=0\)) --- or enter the competition --- choosing a strictly
positive cost (i.e., setting \(y_i^*=\target\)).

At equilibrium, for any contestant \(i=1, .., n\) in a race competition,
there is an ability threshold \(\marginal>0\) such that any contestant
with an ability below the threshold will drop out the competition; and
any contestant with an ability above the threshold will enter the
competition incurring a cost given by

\begin{equation}
    \label{cost races}
    c_i^* = \target^\alpha t^*(a_i)^\beta / a_i
\end{equation}

where \(t^*(\cdot)\) is a monotonic decreasing function of ability given
by

\begin{equation}
        t^*(a_i) = \left[
            \deadline^\beta - \frac{1}{\target^\alpha}\sum_{k=1}^{p} v_k \int_{\marginal}^{a_i} x \dot p_k(x) d x
        \right]^{1/\beta}
\end{equation}

and \(\dot p_k(\cdot)\) is the first derivative of \(p_k(\cdot)\) the
probability of winning the kth prize. This probability involves the
distribution of the \(k\) first order statistics of individual abilities
and is given by

\[
    p_k(x) = \frac{(n-1)!}{(k-1)!(n-k)!} F_A(x)^{k-1} (1-F_A(x))^{n-k}. 
\]

Note that \(t^*(\marginal)=\deadline\), as the marginal type
\(\marginal\) picks the deadline at equilibrium; and the marginal type
is defined implicitly by the zero profits condition

\begin{equation}
    \label{zero profits}
    \sum_{k=1}^p p_{k}(\marginal) v_k - \target^\alpha \deadline^\beta / \marginal = 0, 
\end{equation}

where, for the above expression to hold, \(\marginal\) must be positive,
not zero.

\subsubsection{Tournaments}\label{tournaments}

Unlike races, tournaments do not award prizes based on the contestants'
timing. In particular, any \(t_i\) equal or below the deadline
\(\deadline\) gives a constant probability of winning a prize. It is
thus a dominant strategy for the contestants to select a timing exactly
equal to \(\deadline\). This choice does not affect entry, as
contestants can still choose to incur arbitrarily small costs by
lowering their performance. So, everyone enter at equilibrium.\footnote{More
  specifically, even when the deadline is exceedingly short, then the
  expected reward of winning becomes zero for everyone, which makes the
  contestants indifferent between entering the contest with a
  performance of zero, or dropping out.}

At equilibrium, any contestant \(i=1, .., n\) with an ability \(a_i\)
will enter the competition incurring a cost given by

\begin{equation}
    \label{cost tournaments}
    c_i^* = y^*(a_i)^\alpha \deadline^\beta / a_i
\end{equation}

where \(y^*(\cdot)\) is a monotonic increasing function of ability given
by

\begin{equation}
        y^*(a_i) = 
        \left[\frac{1}{\deadline^\beta}\sum_{k=1}^{p} v_k \int_{0}^{a_i} x \dot p_k(x) d x \right]^{1/\alpha}. 
\end{equation}

Note that \(y^*(0)=0\) (so, the equilibrium cost
\(\lim_{a_i\to 0} c_i^* = 0\)).

\subsubsection{Tournaments with reserve}\label{tournaments-with-reserve}

Tournaments with reserve generalize tournaments by adding a minimum
performance requirement \(\target_{\reserve}\). As for races, any
performance below the target \(\target_{\reserve}\) gives a zero
probability of winning a prize. It is thus a dominant strategy for
players to either drop out the competition --- by choosing a cost of
zero --- or enter the competition with a strictly positive cost. In
addition, since awards are not based on the timing of performance, it is
a dominant strategy for the contestants to choose a timing equal to the
deadline \(\deadline\).

At equilibrium, for any contestant \(i=1, .., n\) in a tournament with
reserve competition, there is an ability threshold
\(\marginal_\reserve>0\) such that any contestant with an ability below
the threshold will drop out the competition; and any contestant with an
ability above the threshold will enter the competition incurring a cost
given by \[
    c_i^* = y_\reserve^*(a_i)^\alpha \deadline^\beta / a_i
\] where \(y_\reserve^*(\cdot)\) is a monotonic increasing function of
ability given by

\begin{equation}
        y_\reserve^*(a_i) = \left[
            \target_{\reserve}^\alpha + \frac{1}{\deadline^\beta}\sum_{k=1}^{p} v_k \int_{\marginal_\reserve}^{a_i} x \dot p_k(x) d x
        \right]^{1/\alpha}.
\end{equation}

Since the equilibrium performance for the contestants entering the
competition is a monotonic transformation of the individual ability, the
marginal ability threshold \(\marginal_\reserve\) is defined implicitly
by a similar zero profits condition as equation \eqref{zero profits} in
the races. Therefore, the marginal type \(\marginal_{\reserve}\) is
strictly positive. Moreover, the marginal type in tournaments with
reserve and races is the same when both competitions have an equal
minimum required performance target, all else being equal.

\subsection{Races vs Tournaments}\label{races-vs-tournaments}

From the contestants' point of view, the main difference between races
and tournaments lies in the incurred costs at equilibrium. In
particular, as summarized by the next proposition, there are two main
differences in terms of equilibrium costs between races and tournaments.

\begin{proposition}
(a) the low-ability contestants --- those with an ability below the marginal type ($a_i<\marginal$) --- will drop out the race competition thereby incurring no cost, whereas they will enter a tournament competition incurring positive costs; and (b) the high-ability contestants --- those with an ability above the marginal type ($a_i\geq\marginal$)--- will enter both types of competition incurring positive costs that are higher in races relative to tournaments with a difference that is independent on individual ability. 
\end{proposition}

\begin{proof}
Omitted.
\end{proof}

In particular, for any high-ability contestant (\(a_i\geq\marginal\))
the difference in cost at equilibrium \(\Delta_i\) between race and
tournament competitions is given by the difference between equation
\eqref{cost races} and \eqref{cost tournaments}, which can be written as

\begin{equation}
    \Delta_i = c_{i,\tournament}^* - c_{i,\race}^* 
    = \sum_{k=1}^{p} v_k \int_0^{\marginal} x \dot p_k(x) d x - \target^\alpha \deadline^\beta \leq 0.
\end{equation}

It can be hence shown (using the zero profit condition
\eqref{zero profits}) that the equilibrium cost difference \(\Delta_i\)
is less or equal than zero for any \(a_i\geq \marginal\). It is also
interesting to note that the individual ability \(a_i\) does not appear
in the equation. The cost difference between races and tournaments is
thus the same for everyone, independently of their ability (provided it
is higher than the marginal type).

The comparison between races and tournaments with reserve leads to
similar results. As soon as, the tournament has a lower performance
target than the race (\(\target_{\reserve} \leq \target\)) there are
higher entry rates in the tournament with reserve and there is a
negative cost difference for the entrants. These differential effects,
however, are decreasing in the difference between the targets and will
eventually vanish entry in competitions with equal targets
(\(\Delta_{i}=0\) for any \(a_i\geq\marginaltype\) when
\(\target_{\reserve}=\target\)).

In what concerns differences in performance, the model shows that the
performance of contestants in tournaments do not always dominate that of
those in races, and vice versa.

\begin{proposition}
There always exist an interval of ability realizations $\hat A$ such that the equilibrium performance of a contestant with an ability $a_i\in \hat A$ in a race is greater than the performance of the same in  individual in the tournament.
\end{proposition}

\begin{proof}
Consider the individual ability to be the marginal type $\marginal$. The individual with an ability equal to the marginal type gets at equilibrium an expected payoff of zero in a race, whereas it gets a strictly positive expected payoff in tournament. Since the probability of winning the kth prize is the same between races and tournaments --- it is the probability of the ability being higher than $k-1$ order statistics and lower than $n-k$ order statistics of the realized abilities (the bid is a monotonic transformation of the individual ability or, in other words, rankings are virtually the same). The expected payoffs in equilibrium can only differ in the incurred cost. Hence, to be an equilibrium, the player in the tournament should bid less than the player in the race to earn a strictly positive expected payoff. 
\end{proof}

To illustrate this point, we use a numeric example. Consider \(n=3\)
contestants competing for \(p=2\) prizes of value \(v_1=2/3\) and
\(v_2=1/3\); the cost elasticities are \(\alpha=1\) and \(\beta=-1\);
and abilities are distributed from a Beta distribution (with shape
parameters \(s_1=4\) and \(s_2=2\)). As shown in Figure
\ref{example contest}, there is an interval of abilities between about
0.82 and 0.9 where the equilibrium performance of a contestant \(i\)
with an ability \(a_i\in (0.82,0.9)\) in a race competition is greater
than that of the same contestant in a tournament.

\begin{figure}
\centering
\caption{Example of equilibrium behavior in a race (dotted) and tournament competition (solid)}
\label{example contest}
\includegraphics{Figures/contestExample-1.pdf}
\begin{minipage}{\textwidth}
\footnotesize\emph{Notes:} xxxx
\end{minipage}
\end{figure}

\subsection{The designer's problem}\label{the-designers-problem}

We consider now the contest designer's problem of selecting the
competition style that maximizes its expected revenues. The problem
consists of choosing a set of rules --- race, tournament, and tournament
with reserve --- and, for the race and tournament with reserve, an
optimal target of performance.

We assume that the contest designer's revenues are an increasing
function of the performance achieved by the winner, a decreasing
function of the time to perform the task of the winner; and are not
affected by the outcomes of the non-winning contestants. Let the
winner's equilibrium actions be denoted by the pair \((y^w, t^w)\) and
let \(p^*\) denote the lower value between \(p\) --- the number of
available prizes --- and the number of contestants eligible for prizes.
For a given competition style with target \(\target\) and deadline
\(\deadline\), the contest designer's revenues are given by \[
    y_w + \tau (\deadline - t_w) - \sum_{k=1}^{p^*} v_k 
\] when \(p^*>0\); and are zero when there is no winner \(p^*=0\). The
parameter \(\tau\) allows differences in the impact on revenues between
the time and performance of the winner (e.g., when \(\tau=0\) revenues
are independent of the time it took the winner to perform the task).

At the beginning of the game, the contest designer makes its choice
without knowning the realized abilities of the contestants, and assuming
players behave according to the equilibrium described above.

Let us first consider the comparison between races and tournaments,
while holding the target in the race exogenously fixed. As summarized in
the next proposition, the choice of the competition depends on the
(relative) {[}XXX{]}

\begin{proposition}
For a contest with xxxx, there is a threshold $\hat\tau$ given by 
$$
 \hat\tau = 
$$
above which the contest designer will prefer the race to the tournament.
\end{proposition}

\begin{proof}
See the Appendix. 
\end{proof}

Proof. In a tournament, the objective function is

\begin{align}
R_\tournament & = \Pr(t_{(1:n)}\leq \deadline) \left\{\int y^*(x \mid t_{(1:n)}\leq \deadline) dF_{n:n}(x) - \tau \deadline - 1 \right\}  \nonumber\\
  & = \int_{\mtype}^{\hitype} y^*(x) dF_{n:n}(x) - \tau \deadline - 1. 
\end{align}

That is, the contest designer's objective function is the sum of the
expected output quality for a given deadline, minus the cost \(\tau\) of
having the winner working on the task until completion (i.e., until the
deadline), and the cost of the prize pool (recall the prize pool is
normalized to one).

{[}Implicitly, you're assuming that the prize is always large enough to
ensure positive effort.{]} {[}Second prize too is stochastic!!!!{]}

In a race, the objective function is

\begin{align}
R_\race & =  
  \Pr(a_{(N)}\geq \mtype) \left\{\target - \alpha -
  \Pr(a_{(N-1)}\geq \mtype) (1-\alpha) \right\}
  - \tau \int_{\mtype}^{\infty} t^*(x) dF_{N:N}(x) \nonumber\\
  & = [1-F_{N:N}(\mtype)] \left\{\target - \alpha -
  [1-F_{N-1:N}(\mtype)] (1 - \alpha) \right\}
  - \tau \int_{\mtype}^{\infty} t^*(x) dF_{N:N}(x).
\end{align}

Note. \(t^*(x) \leq \deadline\) for all \(x\)'s. Thus, a lower bound for
the above objective function can be computed:

\begin{align}
\underline {R_\race} & = 
  [1-F_{N:N}(\mtype)] \left\{\target - \alpha -
  [1-F_{N-1:N}(\mtype)] (1 - \alpha) - \tau \deadline\right\}
\end{align}

An even simpler lower bound is rewriting the above expression as if
\(\alpha=1\) (note if the real alpha was set 1 then also mtype would
change and therefore setting alpha hits a lower bound only when mtype
does xxxx when alpha is 1).

Note. \(y^*(x)\) is lower than \(\target\) for all \(a < \mtype\). Thus,
a lower bound of the tournament's expression is

\begin{align}
\overline {R_\tournament} & = 
  [1-F_{N:N}(\mtype)] \target + \int_{\mtype}^\infty y^*(x) dF_{N:N}(x) 
  - \tau \deadline - 1. 
\end{align}

\begin{align}
  \underline {R_\race} \geq & \overline {R_\tournament} \nonumber\\
  [1-F_{N:N}(\mtype)] (\target - 1 - \tau \deadline) \geq &
  [1-F_{N:N}(\mtype)] \target + \int_{\mtype}^\infty y^*(x) dF_{N:N}(x) 
  - \tau \deadline - 1 \nonumber\\
  - [1-F_{N:N}(\mtype)] (\tau\deadline + 1) \geq &
  \int_{\mtype}^\infty y^*(x) dF_{N:N}(x) 
  - (\tau \deadline + 1) \nonumber\\
  F_{N:N}(\mtype) (\tau \deadline + 1) \geq &
  \int_{\mtype}^\infty y^*(x) dF_{N:N}(x) \nonumber\\
  \tau \geq & 
    \left[
      \frac{\int_{\mtype}^\infty y^*(x) dF_{N:N}(x)}{F_{N:N}(\mtype)} -1 
    \right] \frac{1}{\deadline}
\end{align}

End proof.

When the cost of time \(\tau\) is sufficiently high, the race is
preferred. Interestingly, the threshold is a function of the deadline to
complete the job, as xxx. It also depends on the shape of xxxx.

\subsubsection{Optimal minimum-entry}\label{optimal-minimum-entry}

Now we turn to discuss the contest designer's choice of an optimal
minimum requirement \(\target\). So far, we have assumed that
\(\target_\race>\target_\tournament\). Now, we show that the assumption
that xxxx is indeed an optimal choice of the contest designer. This is
summarized in the next proposition.

\begin{proposition}
Suppose the contest designer can choose the target that max profits in both the race and the tournament. Then, the optimal $\target$ in tournament is generally lower than that in a race.
\end{proposition}

To prove that it is indeed the case. We proceed in two steps. First, we
assume that the contest designer does not care about minimizing the
timing of the innovation by imposing \(\tau = 0\). For simplicity,
assume that \(\alpha=1\) (winner-takes-all). In a race, this means that
the optimal target will be a value that makes equal the costs in terms
of less participation versus the gains in terms of higher values of the
winning solutions. Formally, the contest designer's problem in a race is

\begin{align}
  \text{maximize } & R^\race = [1-F_{N:N}(\mtype)] (\target_\race - 1).
\end{align}

Note that \(\mtype\) depends on the target. This is clearly concave in
\(\target_\race\). Thus, the first order condition is also sufficient.

\begin{align}\label{foc race}
  \text{FOC } & \Rightarrow -F^\prime_{N:N}(\mtype) \mtype^\prime (\target_\race - 1) + [1-F_{N:N}(\mtype)] = 0.
\end{align}

In a tournament, \ldots{}

\begin{align}
  \text{maximize } & R^\race = \int_{\mtype}^\infty y^*(x, \target) d F_{N:N}(x) - [1-F_{N:N}(\mtype)]. 
\end{align}

Convexity is not sure. If not, then the optimal target is zero. Which is
lower than the optimal target in a race.

Instead. If the objective function is (strictly) concave then there's an
internal solution.

\begin{align} \label{foc tournament}
  \text{FOC } \Rightarrow & 
    \frac{d\int_{\mtype}^\infty y^*(x, \target) d F_{N:N}(x)) }{d \target}
      + F^\prime_{N:N}(\mtype) \mtype^\prime =0 \nonumber\\ 
    & \text{(by using Leibniz rule)}\nonumber\\
  \Rightarrow & - y^*(\mtype, \target) \mtype^\prime F^\prime_{N:N}(\mtype) 
      + \int_{\mtype}^\infty \dystar - F^\prime_{N:N}(\mtype) \mtype^\prime = 0\nonumber\\
  \Rightarrow & -\target \mtype^\prime F^\prime_{N:N}(\mtype) 
      + \int_{\mtype}^\infty \dystar - F^\prime_{N:N}(\mtype) \mtype^\prime = 0.
\end{align}

Using \eqref{foc race} with \eqref{foc tournament}, the optimal target
is the same in the race and the tournament only if

\begin{align} 
  \int_{\mtype}^\infty \dystar = [1- F_{N:N}(\mtype)].
\end{align}

\[
  \frac{\partial y^*(x, \target)}{\partial \target} = 
    \frac{c_y^\prime(\target)}{c_y^\prime(y^*(x, \target))}. 
\]

Then.

\begin{itemize}
\item
  If \(c_y(\cdot)\) is linear, we have that the ratio is one for all
  \(x\).
\item
  If \(c_y(\cdot)\) is convex, then we have that it is less than one. If
\item
  If \(c_y(\cdot)\) is concave, then we have that it is higher than one.
\end{itemize}

As a result, if linear or convex the first order condition is lower than
that in the race. Since the obj. function is concave (second order is
decreasing), the target should be lower in a tournament than in a race
to satisfy the first order condition. (a lower target increases the
focs.).

Conjecture. If \(\tau>0\), the \(\target\) in the race is higher.

\section{The experiment}\label{the-experiment}

\subsection{The context}\label{the-context}

While many observational studies have focused on various aspects of
races and tournaments, the design of observational studies for comparing
races with tournaments can be problematic. First, contests are run under
widely differing circumstances like the objective, duration, and prize
structure of the competition. But even lacking any of these differing
aspects, a proper comparison might be impossible. As our model
illustrates, contestants are generally confronted with sensible payoff
differences between a race and a tournament competition, all else being
equal. These differences and other elements will create selection
problems that are hard to control in purely observational studies.

Instead of using naturally occurring data, we test our theory by
designing and executing a field experiment. In doing so, one need an
environment where the same type of contest can be replicated under
different competition styles, and while keeping fixed all other relevant
characteristics. In particular, it is important to fix the same
objective and prize structure while changing only the competition style;
and it is crucial to avoid selection problems by assigning at random
contestants to each competition style.

Such an environment was provided by \emph{Topcoder}, a company based in
the United States that administers a popular online platform for
computer programming competitions and that agreed to provide (a) access
to its large member base of competitors (over 1 million registered users
in 2016) and (b) access to its platform's tools for managing online
contests like web-based leaderboards, scoring methods to determine the
winners, and a payment system to reward competitors residing all over
the world. These two aspects --- large sample and tools for managing
online contests --- considerably simplified the execution of our
experiment.

Beyond simplifying the execution of our experimental design, three key
factors made this platform ideal for our experiment. First, the platform
offers periodic programming competitions in which members usually
participate. It hosts biweekly race-style programming competitions based
on programming speed (known as ``Single Round Matches'' or SRMs) and
once a month tournament-like programming competitions based on
performance in data science problems (known as ``Marathon Matches'' or
MMs).\footnote{The SRMs involve a race-like competition whereby
  contestants are presented with several algorithmic problems and get
  points based on the time elapsed between when a problem was open until
  a working solution was submitted. The other type (MMs) is a
  two-week-long tournament where contestants are presented with one
  difficult algorithmic problem and the top five solutions are awarded a
  cash prize.} Therefore, one can expect platform members to be familiar
with the kind of strategic interactions of a race or a tournament
competition, as implicitly assumed by our game-theoretic contest model.

Second, the platform computes and keeps track of different measures of
competitors' individual ability based on their past performance. These
include multiple ``skill ratings'' that are computed and provide a
metric of their ability as contestants in the race-like competitions
(SRMs) and tournament-like competitions (MMs). As a result, one could
expect to (a) estimate treatment effects controlling for very accurate
measures of individual ability,\footnote{Topcoder's skill ratings are
  often used by software companies for hiring.} and (b) examine the
association between individual ability and the observed sorting patterns
in races and tournaments. In a contest, showing the distribution of
skill ratings to the contestants was also an easy way to help them
forming common beliefs about the distribution of abilities, as assumed
by the model.

A third key factors making this platform ideal for our experiment was
that it collects rich data analytics on contest participation like the
timing, extent, and score of solutions submitted during the contest.
This is because all solutions must be submitted online to be considered
for the contest. A typical submission consists of a single source code
file.\footnote{The source code being one single file does not end up
  limiting the complexity of the solutions, as coders are allowed to
  call other functions from a large database of libraries.} The
submitted file is then executed on the server and returns a score.
Scores are an objective metric of quality that usually combine different
criteria like how fast solutions run or the prediction error in a
classification task into one single numeric value. As the platform
provided us with all these measures, we were able to measure the timing,
extent, and quality of participation for each competitor with high
accuracy.

\subsection{Experimental design}\label{experimental-design}

The goal of the experiment was to compare and contrast treatment effects
of three different competition styles: (a) races, (b) tournaments, and
(c) tournaments with reserve, as defined in our theoretical model
section. To further test treatment differences under a varying degree of
competition intensity, each of the treatments had two levels of
intensity determined by the number of competitors. One intensity level
had 10 contestants (small size) and the other 15 contestants (large
size).

Forming the 3x2 factorial design shown in Table
\ref{experimental design}, the treatments were applied to 24 virtual
competition rooms, which are our main experimental units. Each room
consisted of a list of randomly assigned competitors and a customized
webpage with a problem description, a provisional leaderboard (updated
about every 48 hours), and a submission system through which contestants
could submit their codes.

\begin{table}
\centering
\caption{Experimental Design}
\label{experimental design}
\begin{tabular}{@{}lrrr}
  \\[-1.8ex]\hline\hline\\[-1.8ex]
 & Large & Small & Sum \\ 
  \hline\\[-1.86ex]
Race & 60 & 40 & 100 \\ 
  Tournament & 60 & 40 & 100 \\ 
  Tournament w/reserve & 60 & 40 & 100 \\ 
  Sum & 180 & 120 & 300 \\ 
   \hline\\[-1.8ex]
\end{tabular}
\end{table}

A total prize pool of more than \$40,000 was offered as an incentive for
participation. The prize structure across the rooms was the same. In
each room, we offered cash prizes of \$1,000 and \$100 to the first and
second placed competitor. An additional ``grand'' prize of \$6,000 was
offered to the top competitor (i.e., the first among all top room
competitors) in each competition treatment (a total of 3 grand prizes).

Registration to the contest was conducted online. The announcement of a
a four-day registration period was sent via email to all newsletter
subscribers and was publicized in a post on the platform's blog.
Registration was free and open to every ``experienced'' platform member.
In particular, we excluded newly signed-up members and all those who had
never registered for a MMs competition before the experiment.
Registration involved an informed consent and a short registration
survey asking for demographics data.

All registered members were then sorted at random into lists of 10 and
15 competitors. Instead of complete at random, randomization of
competitors to lists was done trying to balance the skill rating
distribution across rooms. In particular, registered participants were
sorted by their skill rating and then sequentially assigned to different
lists {[}AB: \textcolor{red}{double-check exact procedure}{]}. These
lists were then randomly assigned to one of our treatments.\footnote{Each
  room's webpage required a login and was inaccessible to anyone else.}

Participants knew in advance the nature of the problem, the timing of
the competition, and the room size. They were, however, not aware of the
competition style and the other contestants in the room. This
information was communicated to them via an email with a link to their
competition room and announcing the start of an eight day submission
period.

At the end of the submission phase, the platform administered the
scoring of submissions and payments to the winners. All contestants were
invited to take a final survey to collect self-reported measures of
effort --- limited-edition T-shirts were offered as incentive. This
final survey took place a few days after the end of the submission
period (but before the final ranking was shown and winners fully
identified).

\subsubsection{The algorithmic problem}\label{the-algorithmic-problem}

To select a challenging algorithmic problem for the experiment, we
worked together with researchers from the United States National Health
Institute (NIH) and the Scripps Research Institute (SCRIPPS). The
selected problem was based on an algorithm called BANNER (Leaman,
Gonzalez, and others 2008) that was built by researchers at the NIH. The
algorithm uses domain-expert manual labeling to train a Natural Language
Entity Recognition (NLER)'s model that performs automatic annotation of
abstracts from a large corpus of biomedical research papers (e.g.,
PubMed). As automatic annotations help disease characteristics to be
more easily identified, improving the existing methods to annotate
abstracts was very important for the searchability and identifiability
of most relevant papers among millions of records.

The specific goal of the programming competition was to improve upon the
current NIH's BANNER by using a combination of domain-expert and
non-expert (e.g., Amazon Mechanical Turk's workers) manual labeling
(e.g., Good et al. 2014). As a measure of the ability to make correct
predictions of domain expert annotations we used the F-score defined as
the harmonic mean of precision and recall:
\(F = 2 * (precision * recall) / (precision + recall)\)). This score was
computed on 300 abstracts (100 of which were not disculosed to avoid
overfitting) with about a thousand entities to correctly identify from a
dictionary with 15 hundred labels.

One crucial issue in the execution of our experiment was the choice of a
quality target for the race and tournament with reserve competitions. To
this end, we followed the following two main criteria. First, we wanted
a target representing an improvement of current methods but that was
achievable in a 8 days. To do that, we run a pre-trail experiment that
involved 4 highly-skilled coders. They were asked to solve the same
problem in isolation for 5 days. Results of this pre-trail experiment
helped us forming basic predictions about technical feasibility of
different targets. To pick the final value, however, we relied on the
expectations of NIH researchers who developed the current systems. That
is, we surveyed the NIH researchers who developed Banner asking for
three percentage improvements they considered ``useful,'' ``desirable,''
and ``very unlikely.'' The final target used in the experiment was close
to the ``very unlikely'' threshold. The baseline F-score achieved by NIH
researchers was 0.793. We set up a hard-to-reach F-score target for the
race competition which was 0.818 (about a 3 percent increase of the
baseline).

\subsection{Data}\label{data}

We collected extensive platform data for each registered participant.
These include the full history of their registrations in past contests,
as well as the outcomes of these contests like the number of solutions
submitted, placements in the final rankings, and total cash prizes
earned. As reported in Table \ref{summary}, our data show that those who
registered in the experiment were very experienced competitors. They had
been platform members for an average of 5 years, had registered in an
average of 18 multi-week long contests (MMs), in which they had been
earning a median of 6 hundred dollars per year in cash prizes. As our
data shows, on average, they had submitted solutions in only 7 of these
contests (29 percent of their registrations), suggesting that even
highly experienced platform members frequently drop out of competitions.
Our sample had also registered in an average of 46 speed-based contests
(SRMs) with a relatively higher submission rate (84 percent), presumably
because these contests take only a few hours to complete and have
relatively simpler problems that require less effort to solve.

\begin{table}
\centering
\caption{Descriptive statistics}
\label{summary}
\begin{tabular}{@{}lrrrrrrr}
  \\[-1.8ex]\hline\hline\\[-1.8ex]
 & Mean & Median & St.Dev. & Min & Max & Obs. & F-statistic \\ 
  \hline\\[-1.86ex]
 \multicolumn{1}{@{}l}{\emph{Platform data:}}\\
 \\[-1.86ex]~rating & 13.2 & 13 & 4 & 6 & 30 & 210 & 0.003 \\ 
   \\[-1.86ex]~ratingsrm & 1320.1 & 1238 & 560 & 312 & 2958 & 238 & 0.114 \\ 
   \\[-1.86ex]~nreg & 17.6 & 9 & 23 & 1 & 161 & 299 & 0.631 \\ 
   \\[-1.86ex]~nsub & 7.2 & 2 & 12 & 0 & 91 & 299 & 0.410 \\ 
   \\[-1.86ex]~nregsrm & 45.7 & 20 & 64 & 0 & 365 & 299 & 0.149 \\ 
   \\[-1.86ex]~nsubsrm & 40.6 & 19 & 58 & 0 & 338 & 299 & 0.187 \\ 
   \\[-1.86ex]~year & 5.1 & 5 & 4 & 0 & 14 & 299 & 0.617 \\ 
   \\[-1.86ex]~paidyr & 64.9 & 6 & 150 & 0 & 1069 & 139 & 0.104 \\ 
   \\[-1.86ex]~nwins & 3.0 & 1 & 5 & 1 & 27 & 28 & 1.908 \\ 
   \\[-1.86ex]\hline\multicolumn{1}{@{}l}{\emph{Survey data:}}\\
 \\[-1.86ex]~ntop5 & 4.0 & 2 & 8 & 1 & 56 & 70 & 0.565 \\ 
   \\[-1.86ex]~ntop10 & 5.0 & 2 & 9 & 1 & 64 & 94 & 1.464 \\ 
   \\[-1.86ex]~male & 1.0 & 1 & 0 & 0 & 1 & 276 & 0.981 \\ 
   \\[-1.86ex]~timezone & 2.1 & 2 & 5 & -8 & 10 & 277 & 1.340 \\ 
   \\[-1.86ex]~postgrad & 0.4 & 0 & 0 & 0 & 1 & 278 & 1.289 \\ 
   \\[-1.86ex]~below30 & 0.7 & 1 & 0 & 0 & 1 & 278 & 0.680 \\ 
   \\[-1.86ex]~risk & 6.4 & 7 & 2 & 1 & 10 & 279 & 0.141 \\ 
   \\[-1.86ex]~hours & 31.3 & 24 & 25 & 0 & 192 & 277 & 0.388 \\ 
   \\[-1.86ex]~hours12 & 7.7 & 6 & 7 & 0 & 48 & 277 & 0.664 \\ 
   \\[-1.86ex]~hours34 & 7.0 & 5 & 6 & 0 & 48 & 277 & 0.592 \\ 
   \\[-1.86ex]~hours56 & 7.5 & 5 & 7 & 0 & 48 & 278 & 0.329 \\ 
   \\[-1.86ex]~hours78 & 9.1 & 8 & 8 & 0 & 48 & 277 & 0.538 \\ 
   \hline\\[-1.8ex]
\end{tabular}
\begin{minipage}{\textwidth}
\footnotesize\emph{Notes:}{ Platform data: `year` denotes the years as platform member; `nreg` and `nregsrm` are the counts of registrations to past MMs and SRMs competitions, respectively; `nsub` and `nsubsrm` are the counts of submissions to past MMs and SRMs competitions, respectively; `paidyr` is prize money per year (in thousand of dollars) won in past competitions; `nwins`, `ntop5`, `ntop10` denote placements in past MMs competitions; Registration survey: `risk` is a measure of risk aversion; `hours` anticipated hours of work on solving the problem of the contest; `male` indicates the gender; `timezone` refers to competitor's residence during the contest; `postgrad` is an indicator for post-graduate educational degree (MAs or PhDs); and `below30` indicates age below 30 years old.
}\end{minipage}
\end{table}

A key variable to measure was competitors' ability in solving
algorithmic problems (like the named entity recognition task used for
the experiment) and their speed in programming working solutions. A
sensible measure of problem-solving ability was the individual skill
rating that is computed on multi-week long contests (MMs). Similarly, a
sensible measure for programming speed was the skill rating computed on
speed based competitions (SRMs).\footnote{The skill rating is an
  elo-type measure of a competitor's relative ability compared to
  others. This measure is represented by a number that increases or
  decreases at the end of a competition depending on the difference
  between a hypothetical expected rank (based on the pre-contest values
  of the skill rating of the opponents) and the actual rank achieved by
  a competitor at the end of a competition. When the actual rank is
  higher than the expected rank, the skill rating increases whereas it
  decreases otherwise. Skill ratings are computed independently for each
  type of competition run on the platform (MMs and SRMS).}

\begin{figure}
\centering
\caption{Distribution of Topcoder's skill ratings}
\label{skill ratings}
\includegraphics{Figures/ratingPlot-1.pdf}
\end{figure}

As shown in Figure \ref{skill ratings}, both skill ratings (those
measuring problem solving ability and speed) have slightly asymmetric
probability distributions due to the presence of a few competitors with
very high skill ratings relative to the rest. The right-hand panel of
Figure \ref{skill ratings} shows that, while the skill ratings computed
on MMs is positively correlated with the skill rating computed on
speed-based competitions SRMs, there is considerable variation in
skills.\footnote{The fraction of explained variance or R squared by
  simple linear regression was 0.212.} So, a relatively good competitor
in speed-based contests (SRMs), could turn out to be a relatively bad
competitor when faced with hard-to-solve algorithmic problems in
multi-week long contests (MMs) --- and vice versa. However, variation
seems to reduce for the very top competitors, (those who have high
scores in at least one of the two types of competitions), suggesting
that being fast and being able to solve hard problems are highly
correlated for top competitors.

The online registration survey provided additional data on basic
demographics like gender, age, geographic origins, education, and most
preferred programming language. It also provided measures of attitudes
towards risk as our initial survey asked registrants to indicate their
``willingness to take risks in general'' on a 11-point scale (from 0
``Unwilling'' to 10 ``Completely willing'').\footnote{The validity and
  economic relevance of this way to measure risk preferences has been
  shown by Dohmen et al. (2011).} Our survey also collected measures of
time availability. Registrants were asked to make a forecast of how many
hours they anticipated to be able to work on the problem during the
submission phase of the challenge.\footnote{The exact question was:
  ``Looking ahead a week, how many hours do you forecast to be able to
  work on the solution of the problem?''. Participants had to pick an
  integer between 0 and 48 hours for every 2 days of the submission
  phase (a total of 4 choices).}

Demographics reflected the overall distribution of characteristics of
active platform members. As shown in Table \ref{summary}, the gender
composition was highly unbalanced towards male (95 percent) and young
(below 30 years old) (66 percent) competitors. In terms of risk
aversion, the median response was higher than in previous studies
(xxxx), indicating competitors had perhaps higher propensity to take
risk than the norm. Our data also show that participants anticipated a
median of 24 hours of work to solve the algorithmic problem during the
contest (anticipating slightly more hours of work in the first and last
2 days of the contest).

To test whether randomization into treatment groups was successful, we
conducted a series of F-tests to check the statistical significance of
mean differences in competitors' characteristics between treatments. As
shown in Table \ref{summary}, these tests returned very small F
statistics for each variable indicating that our randomization was
successful.

\section{Results}\label{results}

At the end of the eight day submission period of the contest, we
collected individual data like the number, timing, and score of each
code submission made by competitors. As competitors behaviors are
generally correlated within each room, we then aggregated these
individual data into room responses to examine differences between
competition styles at the room level.\footnote{While such a dependence
  does not generally affect estimatation of mean differences, it might
  bias inference accuracy. As actions across rooms can be treated as
  independent, the inferential problem is thus less severe when the
  analysis is at the room level.}

\subsection{Entry}\label{entry}

Our analysis of the effects of different competition styles begins by
looking at competitors' entry. The entry variable is the fraction of
competitors in a room who made at least one submission during the eight
day submission period of the contest. Our theoretical model predicts
that, all else being equal, competitors' entry will be higher in a
tournament than in a race competition, because of the lack of minimum
performance requirements. For the same reason, it also predicts higher
entry rates in regular tournaments compared to tournaments with reserve.
Consistent with these predictions, Figure \ref{room entrants} shows a
higher median percentage of entrants in tournaments compared to both
races and tournaments with reserve. It also shows that the difference in
entry between tournaments and races, though it is consistently positive
across room sizes, seemed more marked in small rooms compared to large
rooms, suggesting sobering effects associated with size.

\begin{figure}
\caption{Percentage of room entrants by competition and room size}
\label{room entrants}
\includegraphics{Figures/entryPlot-1.pdf}
\end{figure}

On average, 4.1 competitors (33.8 percent) assigned to tournaments were
submitting solutions compared to 3.2 competitors (26.1 percent) assigned
to races and 3.4 (26.2 percent) assigned to tournaments with reserve. To
test if the observed positive difference in entry between tournaments
and races was statistically greater than zero, we used a multiple linear
regression model. We regressed the fraction of entrants in each room
against treatment dummies and room size dummies. To control for random
differences in competitors' baseline characteristics across rooms, we
added controls for differences in room abilities and
demographics.\footnote{We consider two sets of room controls: a
  ``partial set'' and a ``full set.'' The partial set includes control
  variables with a relatively higher chance of being different across
  treatment groups. Those are variables with a relatively high (i.e.,
  equal or larger than one) F statistic as reported on Table
  \ref{summary}. The full set includes all the available controls with
  the exception of a few variables. To avoid multicollinearity issues,
  the full set does not include: the anticipated hours of work in each
  day (\texttt{hours12-78}) that are replaced by the sum
  (\texttt{hours}); and the number of top five placements that was
  highly correlated with the number of wins. Other minor issues. Since
  our data has only a few winners with many wins, instead of using the
  mean number of wins of the room, we took an indicator for whether a
  winner was in the room or not.}

\begin{table}
\centering
\caption{Estimates of the effects of competition and room size on entry}\label{ols entry}
\begin{tabular}{@{}lcccccc}
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
 & \multicolumn{6}{c}{\textit{Dependent variable:}} \\ 
\cline{2-7} 
\\[-1.8ex] & \multicolumn{3}{c}{Entry/n} & \multicolumn{3}{c}{log(Entry/n)} \\ 
\\[-1.8ex] & (1) & (2) & (3) & (4) & (5) & (6)\\ 
\hline \\[-1.8ex] 
 Tournament & 0.076 & 0.108$^{*}$ & 0.181$^{***}$ & 0.303 & 0.396$^{*}$ & 0.678$^{***}$ \\ 
  & (0.055) & (0.061) & (0.038) & (0.207) & (0.225) & (0.112) \\ 
  & & & & & & \\ 
 Tournament w/reserve & 0.001 & 0.080 & 0.058 & 0.024 & 0.352 & 0.422 \\ 
  & (0.055) & (0.090) & (0.086) & (0.207) & (0.331) & (0.252) \\ 
  & & & & & & \\ 
 Room size (small) & $-$0.004 & $-$0.013 & $-$0.012 & $-$0.124 & $-$0.167 & $-$0.156 \\ 
  & (0.045) & (0.053) & (0.041) & (0.169) & (0.197) & (0.119) \\ 
  & & & & & & \\ 
 Constant & 0.263$^{***}$ & 0.413 & $-$0.187 & $-$1.370$^{***}$ & $-$0.066 & $-$3.410 \\ 
  & (0.045) & (0.518) & (0.845) & (0.169) & (1.910) & (2.480) \\ 
  & & & & & & \\ 
\hline \\[-1.8ex] 
Room controls & no controls & partial & full & no controls & partial & full \\ 
Observations & 24 & 24 & 24 & 24 & 24 & 24 \\ 
R$^{2}$ & 0.113 & 0.272 & 0.952 & 0.137 & 0.323 & 0.972 \\ 
\hline 
\hline \\[-1.8ex] 
\end{tabular} 
\begin{minipage}{1.000000\textwidth}
\footnotesize\emph{Note:} The table reports regression estimates of the effects of different competition styles on entry computed using three sets of room controls: ``no controls", ``partial", ``full." Standard errors are reported in parenthesis. ***,**, * indicate statistical significance at 1, 5, and 10 percent level.
\end{minipage}
\end{table}

As shown in Table \ref{ols entry}, the estimated difference in entry
between tournaments and races is of 0.08. This corresponds to a
difference of 1 more entrant every 13 = 1/0.076 competitors or, using
the specification with a log-transformed proportion (column 4), an
increase by a factor of 1.35 = exp(0.303) in entry relative to races.
Without any adjustment for random differences in the baseline
characteristics of the rooms, the effect on entry is significantly
greater than zero with a one-sided test at 10 percent level (t=1.391;
one-sided p=0.09). Statistical significance increases when the effect is
computed using the ``partial'' and ``full'' set of controls, thus
adjusting for random differences at the room level like skills, past
experience, and other demographics. Taken together, these results show
evidence supporting a positive effect of tournaments on entry relative
to races. In addition, our estimates show no systematic difference in
entry rates between races and tournaments with reserve (as predicted by
theory); and fail to detect any statistically significant difference
between large and small rooms.

Because tournaments yielded higher entry rates relative to races, it is
now useful to discuss possible drivers of entry by looking at the
sorting patterns across competition styles. Our theory suggests that,
all else being equal, the positive gap in entry of tournaments compared
to races should be driven by low-ability competitors sorting into
tournaments at higher rates. Though theory prescinds from practical
definitions of ability, two main types of abilities seemd the most
relevent in our context. One involves proficiency in solving algorithmic
problems like having adequate problem-solving skills and a good
knowledge of programming. The second pertains to having as much time
availability as required to solve the given problem. Our analysis thus
focused on two types of data: platform data on skills and experience;
and the hours of work as anticipated by competitors at registration.

To examine the sorting patterns in our data, we first estimated the
conditional probability of entry as a function of our favourite ability
measures --- the skill ratings and anticipated hours of work. As shown
in the upper panels of Figure \ref{sorting plot}, the linear-regression
estimates of the conditional probability of entry computed for the
median competitor (i.e., 24 anticipated hours) was increasing in both
our ability measures --- the skill rating (left panel) and anticipated
hours (right panel) --- with large differences in probability between
individuals at the top and the bottom of the covariate distribution. So,
entrants were more skilled than non-entrants. To see if these sorting
patterns were different across tournaments and races, we then estimated
the same relationships conditional on competitors being randomly
assigned to races or tournaments (i.e., interacting the covariates with
treatment dummies). As shown in the lower panels of Figure
\ref{sorting plot}, the linear-regression estimates with interactions
overlap quite well one another. This means that the propensity to enter
of an individual with a given programming skills --- measured by the
skill rating --- holding constant time availability --- measured by the
anticipated hours of work --- was the same across treatments, and vice
versa. So, based on these measures, our data provide no evidence of a
differential in ability-based sorting between tournaments and races.

\begin{figure}
\caption{Sorting by skills and time availability}
\label{sorting plot}
\includegraphics{Figures/sortingplots-1.pdf}
\end{figure}

Differences in means for the all other covariates (not shown) supported
the same conclusion --- positive and significant differences in ability
between entrants and non-entrants but no differing patterns between
tournaments and races. Somewhat surprisingly, the analysis of
differences in means showed an unexpected difference in ability
characteristics between entrants in races and those in tournaments
w/reserve. As shown in Table \ref{sorting table}, those in the races had
significantly more time availability and had registered to less past
competitions than those in the tournaments w/reserve. Since entry was
the same in both treatments, this evidence suggests the presence of a
``substitution'' effect. Such as substitution effect is not predicted by
our theory, as it predicts differences to be driven by higher/lower
entry rates, which are not expected to arise when races and tournament
w/reserve have the same target quality. And we will return on this point
in the discussion of the results.

\begin{table}
\centering
\caption{Sorting patterns}
\label{sorting table}
\begin{tabular}{@{}lcccc}
  \\[-1.8ex]\hline\hline\\[-1.8ex]
Variable & \multicolumn{1}{L{2cm}}{Difference entrants vs non-entrants} & \multicolumn{1}{L{2cm}}{Difference entrants in races vs tournaments} & \multicolumn{1}{L{2cm}}{Difference entrants in races vs tournaments w/reserve} & Obs. \\ 
  \hline\\[-1.86ex]
log(rating) &  0.14*** &  0.07 &  $-$0.13 & 210 \\ 
   & (0.04) & (0.11) & (0.11) &  \\ 
  log(ratingsrm) &  0.07 &  0.08 &  $-$0.18 & 238 \\ 
   & (0.07) & (0.16) & (0.16) &  \\ 
  log(nreg) &  0.77*** &  0.50 &   0.86** & 299 \\ 
   & (0.16) & (0.4) & (0.41) &  \\ 
  log(nregsrm+1) &  0.36 &  0.40 &  $-$0.29 & 299 \\ 
   & (0.23) & (0.55) & (0.57) &  \\ 
  hours &  7.31** & $-$2.22 & $-$17.18** & 277 \\ 
   & (3.35) & (8.15) & (8.31) &  \\ 
  hours12 &  1.97** & $-$1.64 &  $-$4.91** & 277 \\ 
   & (0.9) & (2.2) & (2.24) &  \\ 
  hours34 &  1.77** & $-$0.42 &  $-$4.06* & 277 \\ 
   & (0.84) & (2.05) & (2.09) &  \\ 
  hours56 &  1.68* & $-$0.04 &  $-$3.33 & 278 \\ 
   & (0.9) & (2.19) & (2.23) &  \\ 
  hours78 &  1.81* &  0.14 &  $-$4.63* & 277 \\ 
   & (0.99) & (2.41) & (2.46) &  \\ 
   \hline\\[-1.8ex]
\end{tabular}
\begin{minipage}{\textwidth}
\footnotesize\emph{Notes:}{ The table reports conditional mean differences of various competitors' characteristics conditional on entry and the randomly assigned competition style. Standard errors are reported in parenthesis. ***,**, * indicate statistical significance for t-test at 1, 5, and 10 percent level.
}\end{minipage}
\end{table}

Let us now summarize the evidence obtained so far. Consistent with
theoretical predictions, we found evidence that competitors in
tournaments had higher propensity to enter the competition relative to
those randomly assigned to the other treatments. While our data show
stark differences between entrants and non-entrants, we did not detect
significant differences in entrants' characteristics between races and
tournaments. Both conditional and unconditional differences in means
between races and tournaments are insignificant. In particular, we found
no evidence of skill-based sorting in terms of two well-suited measures
of ability --- skill ratings and anticipated hours of work. By contrast,
we found significant differences in entrants' characteristics between
races and tournaments with reserve. Since entry rates are not different
between these two treatment groups, a substitution effect must have
occurred, which was unexpected as not predicted by theory.

\subsection{Scores}\label{scores}

After the analysis on entry, we focused on comparing treatment effects
on performance. Our model predicts that, all else being equal,
tournaments with reserve will yield higher performance levels compared
to races and ``regular'' tournaments. The model also predicts lower
volatility in performance of races relative to tournaments. To
investigate these issues, we collected data on scores for each solution
submitted during the contest. We then focused the analysis on the
highest score attained by a competitor's last submission in each room,
and we divided all room values by the target quality to express the
percentage relative to the target quality.\footnote{During each contest,
  the platform keeps ``provisional'' and ``system'' scores for each
  solution submitted. In data science problems, provisional and system
  scores are computed in the same way but on different datasets to
  prevent the so-called ``overfitting'' problem (i.e., a form of model
  misspecification). Provisional scores are computed on a ``testing''
  dataset and then published on the leaderboard. System scores are
  computed on a smaller ``system testing'' dataset and kept secret until
  the end of the contest. As these dataset are representative of the
  same data generating process, both scores are basically measuring the
  same kind of performance and indeed are highly correlated (the
  correlation coefficient was 0.816 in our data). To simplify
  exposition, we report the analysis based on the system scores but
  results are the same with provisional scores as well.}

\begin{figure}
\caption{Highest room scores by competition style and room size}
\label{scores plot}
\includegraphics{Figures/scoresplot-1.pdf}
\end{figure}

\begin{table}
\centering
\caption{Estimates of the Effect of Competition Style on Performance}\label{scores table}
\begin{tabular}{@{}lcccccc}
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
 & \multicolumn{6}{c}{\textit{Dependent variable:}} \\ 
\cline{2-7} 
\\[-1.8ex] & \multicolumn{3}{c}{Highest score} & \multicolumn{3}{c}{log(Highest score)} \\ 
\\[-1.8ex] & (1) & (2) & (3) & (4) & (5) & (6)\\ 
\hline \\[-1.8ex] 
 Tournament & 0.14 & 0.36 & 0.46 & 0.001 & 0.004 & 0.005 \\ 
  & (0.74) & (0.80) & (0.69) & (0.01) & (0.01) & (0.01) \\ 
  & & & & & & \\ 
 Tournament w/reserve & $-$0.01 & 0.05 & 0.37 & $-$0.0003 & 0.0004 & 0.004 \\ 
  & (0.74) & (1.18) & (1.56) & (0.01) & (0.01) & (0.02) \\ 
  & & & & & & \\ 
 Room size (small) & $-$0.57 & $-$0.69 & $-$0.41 & $-$0.01 & $-$0.01 & $-$0.004 \\ 
  & (0.60) & (0.70) & (0.74) & (0.01) & (0.01) & (0.01) \\ 
  & & & & & & \\ 
 Constant & 100.00$^{***}$ & 100.00$^{***}$ & 103.00$^{***}$ & 4.61$^{***}$ & 4.60$^{***}$ & 4.64$^{***}$ \\ 
  & (0.60) & (6.79) & (15.30) & (0.01) & (0.07) & (0.15) \\ 
  & & & & & & \\ 
\hline \\[-1.8ex] 
Room controls & no controls & partial & full & no controls & partial & full \\ 
Observations & 24 & 24 & 24 & 24 & 24 & 24 \\ 
R$^{2}$ & 0.05 & 0.25 & 0.91 & 0.05 & 0.25 & 0.91 \\ 
\hline 
\hline \\[-1.8ex] 
\end{tabular} 
\begin{minipage}{1.000000\textwidth}
\footnotesize\emph{Note:} The table reports regression estimates of the effects of different competition and room size on the highest score in a room computed using three sets of room controls: ``no controls", ``partial", ``fulla." Standard errors are reported in parenthesis. ***,**, * indicate statistical significance at 1, 5, and 10 percent level.
\end{minipage}
\end{table}

As shown in Figure \ref{scores plot}, the distribution of the top final
scores was centered around the target quality in each treatment, with
higher variance in tournaments and tournaments w/reserve compared to
races. The top final scores were thus below the target quality in about
half of the rooms in each treatment; with larger differences between
high and low scores in tournaments relative to races.

Regression estimates of the treatment effects are shown in Table
\ref{scores table}. In all our specifications --- with or without
adjustment for random differences in the baseline characteristics of the
rooms --- estimates of treatment effects were small (\(<1\) percentage
point) and insignificant, suggesting performance differences were too
small to be detected. By contrast, using an F-test to compare the
variances of two samples, we found the difference in variance between
races and the other two treatments combined was significant (p=0.058).

Taken together, these results show that top final scores were below the
target quality in about half of the rooms in each treatment. The total
payout in tournaments was hence about twice as high as in the races and
tournaments w/reserve, where prizes are distributed only when top
solutions achieve a score equal or higher than the target. And, at the
same time, the mean top scores in tournaments was significantly
different from those in the other treatments. This seems to suggest that
races and tournaments w/reserve were more ``efficient'' than
tournaments.

\footnote{FOOTNOTE --- While final scores give a very powerful
  indication of performance, one problem with measuring individual
  performance using final scores is that any failure in the submitted
  code might return very low values, as shown in Figure
  \ref{scores over time}, that are not wholly indicative of performance.
  As outliers will in general bias the room means, we explored two
  corrective methods (1) we trimmed last scores before computing the
  room means; and (2) we replaced all final scores that were below the
  baseline score with the baseline. The first method gives a winsorized
  mean, which is an unbiased estimate of the average. The second
  approach equals censoring by assuming that all entrants had a
  performance at least equal to the final score that one would obtain by
  submitting the BANNER's algorithm without making any useful change.}

\subsection{Speed}\label{speed}

We now turn to examine differences in the timing of submissions. Our
theory predicts that, all else being equal, solutions will be developed
faster in races compared to tournaments. The prediction can be seen
either with respect to the average ``time'' to develop a submission or
the speed to develop a submission of a given quality. To test this
prediction we examined he mean time of the first submission scaled by
the score.

As shown in Figure \ref{room time}, races yielded solutions, in median,
about 2 days faster than tournaments; and this difference seems larger
in small rooms where contestant in tournaments might have less
incentives to act in advance to prevent others from entering the
contest.

To test to see if the difference between races and tournaments was
greater than zero, we used a multiple regression approach. Estimates and
standard errors are shown in Table \ref{ols speed}. Speed was
significantly lower in races than in the other treatments. When we add
controls for random differences in baseline characteristics like average
experience of room competitors, the result becomes slightly
insignificant, though it remains well below the significant threshold
for a one-sided test. Statistical significance is higher in the
specification with the log-transformed variable, which is to be
preferred as it gives a better fit in terms of \(R^2\).

\begin{figure}
\caption{Time of first submission by competition and room size}
\label{room time}
\includegraphics{Figures/firstsubbox-1.pdf}
\end{figure}

\begin{table}
\centering
\caption{Estimates of the Effect of Competition Style on Production Speed}\label{ols speed}
\begin{tabular}{@{}lcccccc}
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
 & \multicolumn{6}{c}{\textit{Dependent variable:}} \\ 
\cline{2-7} 
\\[-1.8ex] & \multicolumn{3}{c}{Speed} & \multicolumn{3}{c}{log(Speed)} \\ 
\\[-1.8ex] & (1) & (2) & (3) & (4) & (5) & (6)\\ 
\hline \\[-1.8ex] 
 Tournament & $-$1.10$^{*}$ & $-$1.09 & $-$1.57 & $-$0.80$^{*}$ & $-$0.87$^{*}$ & $-$1.14 \\ 
  & (0.61) & (0.65) & (0.74) & (0.43) & (0.46) & (0.57) \\ 
  & & & & & & \\ 
 Tournament w/reserve & $-$1.22$^{*}$ & $-$0.76 & $-$1.99 & $-$1.11$^{**}$ & $-$0.45 & $-$0.92 \\ 
  & (0.61) & (0.96) & (1.66) & (0.43) & (0.68) & (1.28) \\ 
  & & & & & & \\ 
 Room size (small) & 0.61 & 0.50 & $-$0.38 & 0.12 & 0.19 & $-$0.07 \\ 
  & (0.50) & (0.57) & (0.79) & (0.35) & (0.40) & (0.61) \\ 
  & & & & & & \\ 
 Constant & 1.18$^{**}$ & 4.55 & 17.70 & $-$0.37 & 2.79 & 9.91 \\ 
  & (0.50) & (5.50) & (16.40) & (0.35) & (3.89) & (12.70) \\ 
  & & & & & & \\ 
\hline \\[-1.8ex] 
Room controls & no controls & partial & full & no controls & partial & full \\ 
Observations & 24 & 24 & 24 & 24 & 24 & 24 \\ 
R$^{2}$ & 0.24 & 0.43 & 0.88 & 0.27 & 0.44 & 0.85 \\ 
\hline 
\hline \\[-1.8ex] 
\end{tabular} 
\begin{minipage}{1.000000\textwidth}
\footnotesize\emph{Note:} The table reports regression estimates of the effects of different competition and room size on the highest score in a room computed using three sets of room controls: ``no controls", ``partial", ``fulla." Standard errors are reported in parenthesis. ***,**, * indicate statistical significance at 1, 5, and 10 percent level.
\end{minipage}
\end{table}

To summarize the results obtained so far, we have found evidence
supporting a higher participation in the Tournament. This higher
participation, however, does not seem to be driven by low-skilled
competitors. We have also found that competitors in the race made
submissions faster without sacrificing performance, which suggests that
they have paid higher costs from effort associated with the accelerated
speed compared to other competitors. Finally, we reject the hypothesis
that the Tournament with Reserve yields higher performance levels.

\subsection{Structural estimation}\label{structural-estimation}

{[}TBA{]}

\section{Discussion}\label{discussion}

{[}TBA{]}

\section*{References}\label{references}
\addcontentsline{toc}{section}{References}

\hypertarget{refs}{}
\hypertarget{ref-athey2002identification}{}
Athey, Susan, and Philip A Haile. 2002. ``Identification of Standard
Auction Models.'' \emph{Econometrica} 70 (6). Wiley Online Library:
2107--40.

\hypertarget{ref-athey2007nonparametric}{}
---------. 2007. ``Nonparametric Approaches to Auctions.''
\emph{Handbook of Econometrics} 6. Elsevier: 3847--3965.

\hypertarget{ref-athey2011comparing}{}
Athey, Susan, Jonathan Levin, and Enrique Seira. 2011. ``Comparing Open
and Sealed Bid Auctions: Evidence from Timber Auctions*.'' \emph{The
Quarterly Journal of Economics} 126 (1). Oxford University Press:
207--57.

\hypertarget{ref-baye2003strategic}{}
Baye, Michael R, and Heidrun C Hoppe. 2003. ``The Strategic Equivalence
of Rent-Seeking, Innovation, and Patent-Race Games.'' \emph{Games and
Economic Behavior} 44 (2). Elsevier: 217--26.

\hypertarget{ref-bimpikis2014designing}{}
Bimpikis, Kostas, Shayan Ehsani, and Mohamed Mostagir. 2014. ``Designing
Dynamic Contests.'' Working paper, Stanford University.

\hypertarget{ref-che2003optimal}{}
Che, Yeon-Koo, and Ian Gale. 2003. ``Optimal Design of Research
Contests.'' \emph{The American Economic Review} 93 (3). American
Economic Association: 646--71.

\hypertarget{ref-dechenaux2014survey}{}
Dechenaux, Emmanuel, Dan Kovenock, and Roman M Sheremeta. 2014. ``A
Survey of Experimental Research on Contests, All-Pay Auctions and
Tournaments.'' \emph{Experimental Economics}. Springer, 1--61.

\hypertarget{ref-dixit1987strategic}{}
Dixit, Avinash Kamalakar. 1987. ``Strategic Behavior in Contests.''
\emph{The American Economic Review} 77 (5). American Economic
Association: 891--98.

\hypertarget{ref-dohmen2011individual}{}
Dohmen, Thomas, Armin Falk, David Huffman, Uwe Sunde, Jrgen Schupp, and
Gert G Wagner. 2011. ``Individual Risk Attitudes: Measurement,
Determinants, and Behavioral Consequences.'' \emph{Journal of the
European Economic Association} 9 (3). Wiley Online Library: 522--50.

\hypertarget{ref-donald1996identification}{}
Donald, Stephen G, and Harry J Paarsch. 1996. ``Identification,
Estimation, and Testing in Parametric Empirical Models of Auctions
Within the Independent Private Values Paradigm.'' \emph{Econometric
Theory} 12 (03). Cambridge Univ Press: 517--67.

\hypertarget{ref-fullerton1999auctionin}{}
Fullerton, Richard L, and R Preston McAfee. 1999. ``Auctionin Entry into
Tournaments.'' \emph{Journal of Political Economy} 107 (3). JSTOR:
573--605.

\hypertarget{ref-good2014microtask}{}
Good, Benjamin M, Max Nanis, CHUNLEI Wu, and ANDREW I Su. 2014.
``Microtask Crowdsourcing for Disease Mention Annotation in Pubmed
Abstracts.'' In \emph{Pacific Symposium on Biocomputing. Pacific
Symposium on Biocomputing}, 282--93. NIH Public Access.

\hypertarget{ref-green1983comparison}{}
Green, Jerry R, and Nancy L Stokey. 1983. ``A Comparison of Tournaments
and Contracts.'' \emph{The Journal of Political Economy} 91 (3):
349--64.

\hypertarget{ref-grossman1987dynamic}{}
Grossman, Gene M, and Carl Shapiro. 1987. ``Dynamic R\&D Competition.''
\emph{Economic Journal} 97 (386). Royal Economic Society: 372--87.

\hypertarget{ref-harris1987racing}{}
Harris, Christopher, and John Vickers. 1987. ``Racing with
Uncertainty.'' \emph{The Review of Economic Studies} 54 (1). Oxford
University Press: 1--21.

\hypertarget{ref-laffont1995econometrics}{}
Laffont, Jean-Jacques, Herve Ossard, and Quang Vuong. 1995.
``Econometrics of First-Price Auctions.'' \emph{Econometrica} 63 (4).
Econometric Society: 953--80.

\hypertarget{ref-lazear1981rank}{}
Lazear, Edward P, and Sherwin Rosen. 1981. ``Rank-Order Tournaments as
Optimum Labor Contracts.'' \emph{The Journal of Political Economy} 89
(5): 841--64.

\hypertarget{ref-leaman2008banner}{}
Leaman, Robert, Graciela Gonzalez, and others. 2008. ``BANNER: An
Executable Survey of Advances in Biomedical Named Entity Recognition.''
In \emph{Pacific Symposium on Biocomputing}, 13:652--63.

\hypertarget{ref-mary1984economic}{}
Mary, O'Keeffe, W Kip Viscusi, and Richard J Zeckhauser. 1984.
``Economic Contests: Comparative Reward Schemes.'' \emph{Journal of
Labor Economics} 2 (1). University of Chicago Press: 27--56.

\hypertarget{ref-moldovanu2001optimal}{}
Moldovanu, Benny, and Aner Sela. 2001. ``The Optimal Allocation of
Prizes in Contests.'' \emph{The American Economic Review}. JSTOR,
542--58.

\hypertarget{ref-moldovanu2006contest}{}
---------. 2006. ``Contest Architecture.'' \emph{Journal of Economic
Theory} 126 (1). Elsevier: 70--96.

\hypertarget{ref-paarsch1992deciding}{}
Paarsch, Harry J. 1992. ``Deciding Between the Common and Private Value
Paradigms in Empirical Models of Auctions.'' \emph{Journal of
Econometrics} 51 (1-2). Elsevier: 191--215.

\hypertarget{ref-parreiras2010contests}{}
Parreiras, Srgio O, and Anna Rubinchik. 2010. ``Contests with Three or
More Heterogeneous Agents.'' \emph{Games and Economic Behavior} 68 (2).
Elsevier: 703--15.

\hypertarget{ref-siegel2009all}{}
Siegel, Ron. 2009. ``All-Pay Contests.'' \emph{Econometrica} 77 (1).
Wiley Online Library: 71--92.

\hypertarget{ref-siegel2014contests}{}
---------. 2014. ``Contests with Productive Effort.''
\emph{International Journal of Game Theory} 43 (3). Springer: 515--23.

\hypertarget{ref-szymanski2003economic}{}
Szymanski, Stefan. 2003. ``The Economic Design of Sporting Contests.''
\emph{Journal of Economic Literature} 41 (4). American Economic
Association: 1137--87.

\hypertarget{ref-taylor1995digging}{}
Taylor, Curtis R. 1995. ``Digging for Golden Carrots: An Analysis of
Research Tournaments.'' \emph{The American Economic Review} 85 (4).
American Economic Association: 872--90.

\hypertarget{ref-zizzo2002racing}{}
Zizzo, Daniel John. 2002. ``Racing with Uncertainty: A Patent Race
Experiment.'' \emph{International Journal of Industrial Organization} 20
(6). Elsevier: 877--902.

\end{document}