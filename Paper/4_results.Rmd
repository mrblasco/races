```{r}
read_chunk("R/results.R")
read_chunk("R/results_sorting.R")
read_chunk("R/sorting.R")
read_chunk("R/scores.R")
```

# Results

At the end of the eight day submission period of the contest, we collected individual data like the extent, timing, and score of each code submission made by competitors. As competitors behaviors are generally correlated within each room, we then aggregated these individual data into room responses to examine differences between competition styles at the room level.[^dependence]

[^dependence]: While such a dependence does not generally affect estimatation of mean differences, it might bias inference accuracy. As actions across rooms can be treated as independent, the inferential problem is thus less severe when the analysis is at the room level.

## Entry

```{r}
entry_perc <- round(100 * with(entry, tapply(submit/n, treatment, median)))
```

Our analysis of the effects of different competition styles begins by looking at competitors' entry. The entry variable is the fraction of competitors in a room who made at least one submission during the eight-day submission period of the contest.  All else being equal, one should expect ---from our theoretical model--- a higher entry in tournaments relative to races due to the difference in minimum performance requirements. Consistent with this prediction, the median percentage of entrants was greater in rooms with tournaments (`r entry_perc['tournament']` percent) than in those with races (`r entry_perc['race']`  percent) or tournaments with reserve (`r entry_perc['reserve']` percent). This difference in room entry rates was consistently positive across room sizes (Figure \ref{room entrants}) although more marked when the room size was small, suggesting sobering effects associated with room size.

```{r entry_rates_figure, fig.width=9, fig.height=4, include=FALSE}
```

\begin{figure}
\caption{Percentage of room entrants by competition and room size}
\label{room entrants}
\includegraphics{Figures/entry_rates_figure-1.pdf}
\end{figure}

To test if the observed positive difference in entry rates between tournaments and races was statistically greater than zero, we used a multiple linear regression model regressing the fraction of entrants in each room against treatment dummies and room size dummies. To control for random differences in competitors' baseline characteristics across rooms, we added two sets of control variables: a "selected" and a "full" set. In particular, the selected set included controls with relatively higher chances of being unbalanced across rooms (i.e., those with an F statistic equal or larger than one, as reported on Table \ref{summary}).[^selection] The full set included instead all the available controls with few exceptions to avoid technical issues like multicollinearity.[^chickensink] 

[^selection]: We could have used more sophisticated variable selection techniques like lasso or stepwise variable selection methods. These data-driven approaches, however, will generally bias the inference after selection (i.e., conventional inferential statistics are only valid if the model is chosen independent of the data being used to calculate them) demanding further adjustments for hypothesis testing that we wanted to avoid.
[^chickensink]: The full set does include the measures of anticipated hours of work in each day (`hours12-78`) that were replaced by the total number of hours (`hours`); the number of top five placements that was highly correlated with the number of wins; and, since our data has only a few winners with many wins, the mean number of wins of the room was replaced by an indicator for whether a winner was in the room or not.


```{r entrylm, results='asis'}
```

```{r}
# Extract estimates and test statistics for 3 models
mat <- matrix(ncol=3, nrow=3)
rownames(mat) <- c('est', 'tstat', 'pval')
colnames(mat) <- c('no', 'selected', 'full')
for (i in 1:3) {
	model <- m.log[[i]]
	mat[1, i] <- coef(model)['treatmenttournament']
	mat[2, i] <- summary(model)$coef["treatmenttournament", 3]
	mat[3, i] <- summary(model)$coef["treatmenttournament", 4] / 2
}
```

All our regression results (Table \ref{ols entry}) show consistently that tournaments increased average entry rates relative to races. In the regression with no controls, we estimated an average increase by a factor of  `r round(exp(mat['est', 'no']), 2)` = exp(`r round(mat['est', 'no'], 2)`), which was significantly greater than zero (one-sided test) at 10 percent level (t=`r mat['tstat', 'no']`; one-sided p=`r mat['pval', 'no']`). After adjusting for key random differences at the room level, the estimated effect was even bigger (`r round(exp(mat['est', 'selected']), 2)` and `r round(exp(mat['est', 'full']), 2)` for the regression with selected and full set of controls, respectively) and statistically significant at 5 percent level. Our regression results show hence evidence supporting a positive effect of tournaments on entry relative to races, as predicted by our theory. In addition, regression results indicate that participation rates between races and tournaments with reserve are the same, which is also consistent with theory, and suggest that the difference in size between rooms was inconsequential for participation rates.

A possible explanation for the gap in entry between tournaments and races is that "low-ability" competitors entered more in tournaments than in races as a consequence of the lack of minimum performance requirements --- as in the equilibrium of our model. This prediction is hard to test because theory prescinds from defining ability in practice. A few key variables, however, seemed intuitively associated with ability in our context.  In particular, we focused on problem solving skills, programming speed, and time availability that were measured by the skill ratings (MMs and SRMs) and the anticipated hours of work collected through the registration survey, as discussed in Section \ref{data}.

To test the extent of ability-based sorting and differences across treatments, we first examined the conditional probability of entry as a function of our ability measures.^[Here, since not everyone has a skill rating, we imputed any missing values at random.] Results (not reported) show a significant and strong association between entry and our measures of problem solving skills and time availability, but an insignificant association between entry and programming speed.^[These results are broadly consistent between models with and without the imputed values.] So, as one would expect, a competitor's ability in terms of problem solving and time constraints is an important determinant of entry. To estimate differences across treatments, we used then non-parametric regression, which generally provides a better fit and, at least in our setting, was less sensitive to extreme values of covariates than conventional multiple regression.

So, for each treatment $l$, we estimated the conditional probability of entry, which is given by
$$
	\Pr(\text{entry}|\text{treatment, skill rating, hours}) = \hat g_l(\text{skill rating}, \text{hours}).
$$ 
Results (Figure \ref{sorting plot}) show that the function $\hat g_{l}(\cdot, \cdot)$ is increasing in our set of ability measures, the skill rating and anticipated hours of work. Moreover, the conditional probabilities in tournaments and races overlap quite well (Figure \ref{sorting plot}, left panel) for low- and middle-ability competitors (the bulk of our sample),  indicating thus that the difference in entry between races and tournaments was not due to those with low problem solving abilities. If anything, high-ability competitors were a little more likely to enter tournaments than races (though this difference is based only on a handful of observations, as shown by the "rug-plot" at the bottom of the graphs). At the same time, we found that the probability of entry in tournaments and races diverge (Figure \ref{sorting plot}, right panel) by about one percentage point difference for low- and middle-values of reported time availability, thus suggesting that the gap in entry was driven by differences in time availability between competitors. That is, competitors who had less time available for the contest (holding constant their problem solving ability) ended up entering tournaments more often than races, arguably because of the differential relevance of time in the two competition formats. 

To summarize, we found a gap in entry between tournaments and races, whereby competitors exhibit a significantly higher probability of entering tournaments compared to races. This gap seems to be driven by a negative effect of races on the participation of competitors with less time available to work on the contest. This finding also indicates that individual time constraints, in addition to problem solving skills, fit with our theoretical definition of "ability" and should be taken into account in the contest design.

<!-- 
Discussion: [Given our set of competitor characteristics is orthogonal to treatment assignment, the (non-parametric) estimated conditional probability of entry given ability and treatment assignment is correctly identified. Conventional statistical inference, however, might be biased because of correlations within rooms.]
 -->

```{r sorting, cache=TRUE, fig.width=9, fig.height=4, include=FALSE}
```

\begin{figure}
\caption{Sorting differences across treatments}
\label{sorting plot}
\includegraphics{Figures/sorting-1.pdf}
\end{figure}


## Productivity

We now turn to evaluate differences in productivity between tournaments and races. Our empirical context provides an accurate way to measure productivity because all submissions got scored based on their performance relative to the given problem, as described in Section \ref{the-context}. Thus our measure of room productivity was the maximum final (i.e., on the last submission) score achieved by competitors in each room.[^scores]

[^scores]: During each contest, the platform keeps "provisional" and "system" scores for each solution submitted. In data science problems, provisional and system scores are computed in the same way but on different datasets to prevent the so-called "overfitting" problem (i.e., a form of model misspecification). Provisional scores are computed on a "testing" dataset and then published on the leaderboard. System scores are computed on a smaller "system testing" dataset and kept secret until the end of the contest. As these dataset are representative of the same data generating process, both scores are basically measuring the same kind of performance and indeed are highly correlated (the correlation coefficient was `r with(scores, cor(provisional, final,use='complete.obs'))` in our data). To simplify exposition, we report the analysis based on the system scores but results are the same with provisional scores as well. 

Our model predicts that the productivity in tournaments with reserve should be higher than that in races. Contrary to this prediction, the median room productivity in tournaments with reserve appears equal to that in the other rooms (Figure XXX), with rooms in tournaments with reserve achieving both the highest and lowest productivity overall. In addition, rooms highest scored submissions were below the target in about half of the rooms in each treatment (with xxxx), resulting in a total payout that was about twice as high as in  tournaments than in races or tournaments w/reserve (e.g., where solutions below the target are ineligible for prizes). 

```{r scoresplot, fig.width=8, fig.height=4, include=FALSE}
```

\begin{figure}
\caption{Room productivity by competition style and room size}
\label{scores plot}
\includegraphics{Figures/scoresplot-1.pdf}
\end{figure}

```{r scorestable, results='asis'}
```

```{r}
vt <- var.test(final.cens ~ treatment=='race', data=final)
```

Using the multiple linear regression approach described before, regression results (not shown) indicate small and insignificant treatment differences in all our specifications (i.e., with or without controls), suggesting performance differences were too small to be detected. By contrast, using an F-test to compare the variances of two samples, we found the difference in variance between races and the other two treatments combined was significant (p=`r vt$p.value`). Room size was not different xxxx. 

Taken together, these results show that room productivity was comparable across rooms.  At the same time, the total payout was larger in tournaments, indicating that races and tournaments w/reserve were more "efficient" than tournaments. 


<!-- 
percentage of entrants in tournaments compared to both races and tournaments with reserve. It also shows that the difference in entry between tournaments and races, though it is consistently positive across room sizes, seemed more marked in small rooms compared to large rooms, suggesting sobering effects associated with size.
 -->


<!-- 
^[FOOTNOTE --- While final scores give a very powerful indication of performance, one problem with measuring individual performance using final scores is that any failure in the submitted code might return very low values, as shown in Figure \ref{scores over time}, that are not wholly indicative of performance. As outliers will in general bias the room means, we explored two corrective methods (1) we trimmed last scores before computing the room means; and (2) we replaced all final scores that were below the baseline score with the baseline. The first method gives a winsorized mean, which is an unbiased estimate of the average. The second approach equals censoring by  assuming that all entrants had a performance at least equal to the final score that one would obtain by submitting the BANNER's algorithm without making any useful change.]
 -->

## Speed

We have already examined differences in entry and productivity, where productivity was measured in a given time unit (i.e., the duration of the contest). We now turn to examine differences in productivity intended as the timing it took competitors to achieve a solution of a given quality (i.e., their speed). 

Our measure of speed was the time difference between the start of the competition and the first submission scaled by the score achieved by the first submission of the room. 


<!-- 
 Our theory predicts that, all else being equal, solutions will be developed faster in races compared to tournaments. The prediction can be seen either with respect to the average "time" to develop a submission or the speed to develop a submission of a given quality. To test this prediction we examined he mean time of the first submission scaled by the score. 
 -->

As shown in Figure \ref{room time}, races yielded solutions, in median, about 2 days faster than tournaments; and this difference seems larger in small rooms where contestant in tournaments might have less incentives to act in advance to prevent others from entering the contest. 

To test to see if the difference between races and tournaments was greater than zero, we used a multiple regression approach. Estimates and standard errors are shown in Table \ref{ols speed}. Speed was significantly lower in races than in the other treatments. When we add controls for random differences in baseline characteristics like average experience of room competitors, the result becomes slightly insignificant, though it remains well below the significant threshold for a one-sided test. Statistical significance is higher in the specification with the log-transformed variable, which is to be preferred as it gives a better fit in terms of $R^2$. 


```{r firstsubbox, fig.width=9, fig.height=4, include=FALSE}
par(mfrow=c(1, 3))
form <- formula(days ~ treatment); ylim=c(0,8)
boxplot.custom(form, data=speed, main="All rooms", ylim=ylim)
boxplot.custom(form, data=subset(speed, room_size=='Small'), main="Small rooms\n(10 competitors)", ylim=ylim)
boxplot.custom(form, data=subset(speed, room_size=='Large'), main="Large rooms\n(15 competitors)", ylim=ylim)
```

\begin{figure}
\caption{Time of first submission by competition and room size}
\label{room time}
\includegraphics{Figures/firstsubbox-1.pdf}
\end{figure}


```{r}
speed0 <- speed
speed1 <- aggregate(provisional_first ~ room_id + treatment + room_size, data=races, mean)
speed <- merge(speed0, speed1)
speed$speed <- ((speed$provisional_first/1e6)/target) / speed$day
```


```{r speedbox, fig.width=9, fig.height=4, include=FALSE}
par(mfrow=c(1, 3))
form <- formula(((provisional_first/1e6)/target) ~ treatment); ylim=c(.5,1.1)
boxplot.custom(form, data=speed, main="All rooms", ylim=ylim)
boxplot.custom(form, data=subset(speed, room_size=='Small'), main="Small rooms\n(10 competitors)", ylim=ylim)
boxplot.custom(form, data=subset(speed, room_size=='Large'), main="Large rooms\n(15 competitors)", ylim=ylim)
```

<!-- 
\begin{figure}
\caption{Average Speed by competition and room size}
\label{room speed}
\includegraphics{Figures/speedbox-1.pdf}
\end{figure}
 -->


```{r speedlm, results='asis'}
speed$days <- NULL
speed$provisional_first <- NULL
m <- rep()
m$speed.lm <- lm(speed ~ treatment + room_size, data=speed)
m$speed.lm.partial <- update(m$speed.lm, ~ . + nwins + ntop10 + timezone + postgrad + male)
m$speed.lm.full <- lm(speed ~ . -hours12-hours34-hours56-hours78, data=speed[,-1])
m.log <- lapply(m, function(x) update(x, "log(speed) ~ .")) 

regtab(c(m, m.log), digits=2, notes.width=1, keep.stat=c('n','rsq')
	, omit="nwins|hours|rating|nreg|nsub|ntop|risk|male|time|postgrad|below|paid|year"
	, add.lines=list(c("Room controls", rep(c("no controls", "partial", "full"), 2)))
	, dep.var.labels=c("Speed","log(Speed)")
	, covariate.labels=c("Tournament","Tournament w/reserve", "Room size (small)")
	, notes="The table reports regression estimates of the effects of different competition and room size on the highest score in a room computed using three sets of room controls: ``no controls\", ``partial\", ``fulla.\" Standard errors are reported in parenthesis. ***,**, * indicate statistical significance at 1, 5, and 10 percent level."
	, caption="Estimates of the Effect of Competition Style on Production Speed"
	, label='ols speed')
```


To summarize the results obtained so far, we have found evidence supporting a higher participation in the Tournament. This higher participation, however, does not seem to be driven by low-skilled competitors. We have also found that competitors in the race made submissions faster without sacrificing performance, which suggests that they have paid higher costs from effort associated with the accelerated speed compared to other competitors. Finally, we reject the hypothesis that the Tournament with Reserve yields higher performance levels.

## Structural estimation

[TBA]
