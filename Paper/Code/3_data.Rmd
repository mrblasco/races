```{r}
races2 <- races
```

The experimental design
=====================

The field experiment was conducted between March 2 and 16, 2016. The context of the experiment was an online programming contest. In an online programming contest,  participants compete to write source code that solves a designated problem. These contests are quite common and xxxx either as a tournament or a race competition. 

The contest was hosted on the online platform Topcoder.com. Since its launch in 2001, Topcoder.com administers on a weekly basis several competitive programming contests for thousands of competitors from all over the world. Typical assigned problems are data science problems (e.g., classification, prediction, natural language processing) that demand some background in machine learning and statistics. All Topcoder members (about 1M registered users in 2016) can compete and attain a "rating" that provides a metric of their ability as contestants. Other than attaining a rating, the competitors having made the top five submissions in a competition are typically awarded a monetary prize the extent of which depends on the nature and complexity of the problem but is generally between $5,000 and $20,000.

In this study, we worked together with researchers from the United States National Health Institute (NIH) and the Scripps Research Institute (SCRIPPS) to select a challenging problem for the experimental programming competition. The selected problem was based on an algorithm called BANNER built by NIH [@leaman2008banner] that uses expert labeling to annotate abstracts from a prominent life sciences and biomedical search engine, PubMed, so disease characteristics can be more easily identified. The goal of the programming competition was to improve upon the current NIH's system by using a combination of expert and non-expert labeling, as described by @good2014microtask.

The competition was announced on the platform and to all community members via email. A preliminary online registration was required to enroll in the competition, which resulted in 340 pre-registered participants. Among the pre-registered members, we selected the 299 who had registered to a programming contest at least once before the present contest. This choice was to ensure that participants were xxxx.

Participants were then randomly assigned to separate groups of 10 or 15 people. In each of these groups, contestants were given access to a "virtual room" that is a private web page listing handles of the other participants of the group, a leaderboard updated regularly during the competition, and a common chat that they can use to ask clarifying questions (visible to everyone in the group) with respect to the problem at hand. 

A problem statement containing a full description of the algorithmic challenge, the rules of the game, and payoffs was published at the beginning of the submission phase. The submission phase was of 8 days in which participants could submit their computer programs. Each submission was automatically scored and feedback in the form of preliminary scores was published regularly on the website via the leaderboard.

Groups were randomly assigned to one of three different competitive settings: a race, a tournament, and a tournament with a _reserve target_, which is the lowest acceptable score by the platform for a submission to be awarded a prize. 

The experimental design is summarized by the Table XXXX.

```{r design, results='asis'}
tab <- with(races, table(treatment, room))
rownames(tab) <- capitalize(rownames(tab))
xtab <- xtable(tab, caption="Experimental design", label='experimental design')
render.xtable2(xtab)
```

In all groups, the first placed competitor was awarded a prize of $1,000, and an additional, consolatory prize of $100 was awarded to the second one. 

In a race competition, however, the first to achieve a score equal to xxxx was placed first. The level was chosen xxxx. 

In a tournament, xxxx. 

Finally, in a tournament with reserve,  xxxx. 

Additional grand prizes of xxxx were awarded to the top xxx in every treatment.

Data
-----

The bulk of our data comes from the online Topcoder's profile of each participant. This profile includes the date when the member registered to the platform, an array of ratings measuring coder's success in past competitions, the number of past competitions, and so on. These data are summarized in FIGURE XXXX. 

FIGURE XXXX

Additional demographic information was collected via a pre-registration survey where competitors were asked their gender, age, geographic origins, education, and the most preferred programming language. Participants were als also asked their willingness to take risks "in general" [as in @dohmen2011individual]. We also collected their forecast on the hours they expected to work on the problem in the next few days of the challenge^[The exact question was: "The submission phase begins March 08. Looking ahead a week, how many hours do you forecast to be able to work on the solution of the problem?"]. At the end of the submission phase, competitors were also asked to fill a final suvery and looking back and tell us their best estimate of the time spent working on the problem. Also, we gathered reactions to the different competition modes with questions such as xxxx. 

Table XXX summarizes the data.

```{r descriptive, results='asis'}
vars <- c("country", "age", "gender", "educ", "plang")
foo <- sapply(races[, vars], function(x) summary(x, maxsum=6))
balance.test <- function(x) chisq.test(table(x, races$treatment) + 1, simul=TRUE)$p.val 
foo.pval <- sapply(races[, vars], balance.test)

xtab <- xtable.factors(foo, foo.pval)
caption(xtab) <- "Descriptive statistics"
label(xtab) <- "descriptives"
align(xtab) <- c("{}l","l","p{5cm}","r","r","r")

attributes(xtab)$notes <- "This table shows the frequency of each response category for five categorical variables: country of origin; age; gender; highest academic degree achieved; and most preferred programming language. A Pearson's Chi-squared test finds no association between each categorical variable and the treatments (p-values are reported in the last column)."

render.xtable2(xtab, include.rownames=FALSE)
```

```{r risk, fig.height=6, fig.cap="Responses to the question about willingness to take risks \"in general,\" measured on an eleven-point scale."}
tab <- table(factor(races$risk, levels=0:10))
barplot(tab, xlab="Responses to general risk question\n(0=not at all willing, 10=very willing)", ylab="Respondents")
```

```{r looking-ahead, fig.cap="Responses to the question about expected hours of work for every 2 days of the competition."}
with(races, boxplot(week1, week2, week3, week4, notch=TRUE, xlab="Days of the competition\n(1=first 2 days, 4=last 2 days)", ylab="Expected hours of work"))
axis(1, 1:4)
```

```{r, fig.cap='xxxx'}
# Ratings per room?
with(races, hist(mm_events/mm_reg, main=NA, xlab="With-submission over registered-only competitions")) # With submission over registered
```

```{r}
diff.days <- function(time1, time2, ...) {
  as.numeric(difftime(time1, time2, units='days', ...))
}
Total <- function(x) sum(x)

# Weeks at the start of the competition
start_date <- as.Date("2016-03-02")
week_diff <- NA

quantile(races$mm_events, p=c(0.1, 0.9), na.rm=TRUE) -> rounds_qtl
quantile(races$mm_rating, p=c(0.1, 0.9), na.rm=TRUE) -> rating_qtl
```

A total of `r nrow(races)` competitors signed-up to take part in the challenge.  They were all xxxx members of the platform with between `r min(week_diff)` and `r max(week_diff)` weeks as registered members. In terms of skill ratings, the distribution was highly skewed with competitors in the highest 90th percentile having participated in `r diff(rounds_qtl)` more competitions than those in the 10th percentile. Likewise skills as measured by the individual ratings, if there was one, had a skewed distribution with `r round(diff(rating_qtl))` higher points than those in the 10th percentile; see Figure \ref{eq: distribution experience}. 

```{r ratings, fig.width=9, fig.height=9}
require(car)
spm(~ mm_rating+mm_events+algo_rating+algo_events, data=races, pch=16)
```