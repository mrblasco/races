\documentclass[12pt,]{article}
\usepackage{beamerarticle}
\usepackage[utf8]{inputenc}
\usepackage{hyperref}

% Math
\usepackage{amssymb,amsmath,amsthm} 
\newtheorem{proposition}{Proposition}

% Fontfamily
\usepackage{palatino}%\usepackage{lmodern, times}

% Page settings
\usepackage[letterpaper, margin=1in]{geometry}
\usepackage{setspace}                           
\onehalfspacing %\doublespacing  % \singlespacing 

% Appendix
\usepackage{appendix}

% Line numbers
%\usepackage{lineno}
%\linenumbers


% Tables
\usepackage{dcolumn}
\usepackage{array,booktabs,longtable,rotating}
\newenvironment{tablenotes}[1][]{
  \begin{minipage}{\textwidth}\emph{Notes:}{\footnotesize #1}
}{\end{minipage}}
\makeatletter
\def\fps@table{htbp}
\makeatother

% Graphics
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\usepackage{natbib}% plainnat
\bibliographystyle{aer}


\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}



\title{Races or Tournaments? {[}PRELIMINARY AND INCOMPLETE{]}\thanks{Blasco: Harvard Institute for Quantitative Social Science, Harvard
University, 1737 Cambridge Street, Cambridge, MA 02138 (email:
\href{mailto:ablasco@fas.harvard.edu}{\nolinkurl{ablasco@fas.harvard.edu}}).}}
\author{Andrea Blasco \and Kevin J. Boudreau \and Karim R. Lakhani \and Michael Menietti}
\date{Last updated: 24 January, 2017}

\begin{document}
\maketitle
\begin{abstract}
A wide range of economic and social situations are decided by either a
race or a tournament. In either situations, agents choose whether and
how much to exert some costly effort to increase the probability of
being awarded a prize under the uncertainty about the actions of other
agents. While a tournament yield outcomes greater than those of a race,
the latter prevents unnecessary costs due to an excess of participation.
We examine this trade-off empirically. We report the results of a field
experiment conducted online where we compare the outcomes --- efforts,
quality, and diversity of outputs --- of three alternative competitive
situations motivated by theory: the race, the tournament, and the
tournament with a quality requirement.

\smallskip\noindent 
JEL Classification: xxx; xxx; xxx.

\smallskip\noindent 
Keywords: xxxx; xxxx xxxx.
\end{abstract}


% Todo notes
%\usepackage[textsize=tiny]{todonotes}
%\newcommand\redmarginpar[1]{\marginpar{\footnotesize{\textcolor{red}{#1}}}}
%\listoftodos[Notes]
\clearpage
\tableofcontents
\setcounter{tocdepth}{2}
\clearpage

\section{Introduction}\label{introduction}

Organizations use often prizes as incentive schemes for innovation. In
the United States, the government routinely sponsors open calls for
innovation tackling important issues of public policy (public health,
education, environment protection) where participants compete to finish
a project, and the winners, if any, are awarded a prize. The same
happens inside firms where internal contests are used to expand the
innovative capacity of the firm by stimulating workers ideas. Contests
are also sponsored by various institutions to source ideas from people
outside the boundaries of the organization such as the members of large
online communities of practitioners (Innocentive, Topcoder, XPRIZE).

Typically, the objective of the contest designer is to maximize the
quality and probability of a succesfull innovation while minimizing the
time it takes to complete the project. Striking a balance between these
two desirable but incompatible features is often difficult. Although
there are many facets of contest design that one may consider, one key
factor is the choice of how participants will compete to win the prize.
In general, there are two extreme forms of competitions. One is the
\emph{race} where the first to finish the innovation project wins. So,
the timing of innovation matters for payoffs. The second is the
\emph{tournament} where the best to finish the project within a given
unit of time wins, and the timing of innovation is irrelevant.

To fix ideas, imagine the government wants to find a solution to a
threat for public health. To minimize the risks that the threat will
materialize before a solution is actually found, one option is to design
a tournament with a tight deadline. Assuming competitors will reach a
successful innovation by that time, then the government will pick the
best solution awarding a prize to the winners. One problem with this
approach is that, for a given prize, the expected quality of the
innovation may be low given the short duration of the contest. The
alternative is to design a race where the quality of the solution is
fixed ex-ante by the contest designer and participants compete against
one another to be the first to deliver it. Understanding which xxxx
remains largely unexplored issue? Which approach is better? what is the
optimal behavior of firms? xxx?

In this study, we design and execute a field experiment to compare
behavior in a race and a tournament setting. We proceed in two ways.
First, we generalize an incomplete information contest model
\citep{moldovanu2001optimal} to enable a direct comparison of
equilibrium behaviors under both the race and the tournament within a
single theoretical framework. Then, we collect data from a field
experiment to test some of the implications of the theory, and provide
policy recommendations.

To be sure, there exist many models of races and tournaments that have
been applied to a wide range of economic situations: patent races, arms
races, sports, promotions inside firms, sales tournaments. 1.
Inefficiency arises because of misallocation or over-expenditure.
Leaving aside misallocation, the problem of over-expenditure xxxx. As
pointed out by \citet{baye2003strategic}, many of these models of
tournament and race competitions are specific cases of a more general
``contest games.'' And sometimes it is possible to design one or the
other in a way to exploit a ``duality.'' In other words, in theory, a
competition can be designed as a tournament to do xxx or as a race to do
xxx. While theoretically very useful, how to exploit this duality in
practice remains largely unknown. Lack of data. As before, xxxx. The
main challenge is self-selection. The answer to this optimal design
question relates to the cost function of agents with respect to ``time''
and to ``effort.'' It is hard to say which solution is better. However,
it is easier to tell whether you should have one prize or multiple
prizes.

3.1 Problems of inefficiency (discriminating)

3.1.1 Excess of effort 3.1.2 Excess of participation

3.2 Duality

3.3 Maximizing revenues (what about timing)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Our experiment
\end{enumerate}

The field experiment was conducted xxxx. We worked together with
researchers from the United States National Health Institute (NIH) and
the Scripps Research Institute (SCRIPPS) to select a challenging problem
for the contest. The selected problem was based on an algorithm called
BANNER built by NIH \citep{leaman2008banner} that uses expert labeling
to annotate abstracts from a prominent life sciences and biomedical
search engine, PubMed, so disease characteristics can be more easily
identified. The goal of the programming competition was to improve upon
the current NIH's system by using a combination of expert and non-expert
labeling, as described by \citet{good2014microtask}. The competition was
hosted online on the platform Topcoder.com (about 1M registered users in
2016). Submissions were made online. The top submissions were awarded a
monetary prize ranging between \$5000 to \$100 for a total prize pool of
\$40,000.

{[}This context is relevant in itself.{]} Programming competitions are a
very important source of incentives in the economy. In the United
States, the government routinely sponsors open contests to tackle a
variety of issues of public health, education, energy, environment
protection, and so on. A large part of which are conducted online
through the web portal Challenge.gov. Contests are also used extensively
by non-profits (xxxx) and in the private sector by sourcing ideas from
outside the boundaries of the organization.

4.2 Treatments

Competitors were randomly assigned to virtual rooms of 10 or 15 people.
These virtual rooms were then randomly assigned to one of three
different competitive settings: a race, a tournament, and a tournament
with a ``reserve score,'' which is the lowest acceptable by the platform
for a submission to be awarded a prize.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  Results
\end{enumerate}

By this perspective, we are able to show that races cannot be justified
simply by the goal of maximizing average effort. And the reason is
intuitive. A race awards a prize to first to hit a particular target.
Those who will judge the target to hard to achieve will not join the
competition and will drop out. On the contrary, those who are able to
achieve the target at low costs will not try to exceed the target. As a
result, the race is comparable to a competition with fixed ``entry
costs'' or a fixed entry requirement, where agents will decide to either
enter and pay a fixed prize, or stay out of the competition. Then, the
possible gains in terms of expected revenues from a race are limited to
those who would enter the competition and would exert less effort that
that required to hit the target. These potential benefits can be
obtained under a tournament as well by imposing a a fixed requirement to
be eligible for prizes. So, races are not chosen to maximize expected
effort of competitors, at least, in the traditional
``auction-theoretical'' sense.

In a tournament, this type of preferences can be satisfied by fixing a
deadline. Say time within which competitors are asked to provide their
efforts. However, assuming competitors have costs from making less time
in performing a task and there complementarities in costs, increasing
the deadline in a tournament is similar to raising the marginal cost for
everyone, which might not be an optimal solution. In a race, by
contrast, increasing the deadline will affect entry but, conditional on
entry, the time to complete the task will always be less than the
deadline. Which means that those with low costs will be mostly affected
by the deadline, whereas xxxx. Which may be a superior choice than the
tournament.

We find that, as our theory suggest, participation is higher in the
tournament and lower in the race and in the tournament with entry costs.
We further find that submission are quicker in a race, whereas are
equally distributed at the end of the competition in the the tournament
and in the tournament with quality requirement. With respect to final
scores, theory predicts as trade-off between a race and a tournament in
terms of higher scores vs faster submissions. We do find that scores are
higher in the tournament but we do not find a strong trade-off in the
sense that race had comparable good quality solutions than the
tournament.

\section{Literature}\label{literature}

This paper is related to the contest theory literature
\citet{dixit1987strategic} \citet{baye2003strategic},
\citet{parreiras2010contests}, \citet{moldovanu2001optimal},
\citet{moldovanu2006contest}, \citet{siegel2009all},
\citet{siegel2014contests}. It also relates to the literature on
innovation contests \citet{taylor1995digging}, \citet{che2003optimal}.
And the personnel economics approach to contests \citet{lazear1981rank},
\citet{green1983comparison}, \citet{mary1984economic}.

Empirically, \citet{dechenaux2014survey} provide a comprehensive summary
of the experimental literature on contests and tourments. Large body of
empirical works have focused on sports contests
\citet{szymanski2003economic}. More recently, inside firms (xxx) and
online contest (xxxx).

This paper is also related to the econometrics of auctions
\citet{paarsch1992deciding}, \citet{laffont1995econometrics},
\citet{donald1996identification} and more recently
\citet{athey2011comparing}, \citet{athey2002identification}, and
\citet{athey2007nonparametric}.

\section{The model}\label{the-model}

We generalize the contest game described by \citet{moldovanu2001optimal}
to a situation where \(N\) players decide how much effort to provide
along two dimensions: performance and time. The contest game is an N
player game with asymmetric information. Players move simultaneously to
maximize the expected utility. Each player \(i=1, 2, ..., N\) selects a
performance variable \(y_i\) and a timing \(t_i\), both being
nonnegative numbers. These variables can be thought of as the accuracy
of a solution to a given problem and the time to write the code
implementing such solution. Players incur a cost from effort given by
the function

\begin{equation}
  C_i(y_i, t_i) = \frac{1}{a_i} c(y_i, t_i)
\end{equation}

where the function \(c(\cdot)\) is xxxx. The cost parameter \(a_i\)
denotes the player's ability, which is privately observed at the
beginning of the game. It is common knowledge that abilities are drawn
from a common distribution \(F\) that is continuous on the semi-infinite
interval \([0,\infty)\) (e.g., exponential).

Let \(r_i\) denote the rank position of a player \(i\) relative to the
\(N-1\) others. The top \(K\) players (e.g., \(r_i\leq K\)) are awarded
a prize of value \(V_1 > V_2 > ... > V_K\). A player's probability of
winning a prize is given by the function
\(p_i(y_1,...y_N, t_1, ..., t_N)\).

The goal for each player is If the timing is above a given deadline
\(d>0\) or the performance is below a certain level \(q>0\), the player
gets zero utility. Otherwise, the player is given a rank based on his
performance and timing. A contest is xxxx. Ranked .

def: A tournament is a xxxx where players are ranked by their
performance level provided that the timing is below a deadline \(d\).

def: A race is a xxxx where players are ranked by their timing provided
that the performance is above a threshold level \(q\).

In a tournament, the agent having achieved the highest output quality
within the deadline gets the first prize, the agent having achieved the
second highest output quality gets the second prize, and so on. In a
race, by contrast, the first agent to achieve an output quality of at
least \({\bar y}\) within the deadline wins the first prize, the second
to achieve the same target gets the second prize, and so on.

Since agents move simultaneously, they do not know the performance of
others when deciding their efforts. On the other hand, it is assumed
that they know the number of competitors as well as their cost functions
to complete the task up to a factor \(a_i\) being the agent's private
ability in performing the task. Each agent knows his ability but does
not know the ability of the others. However, it is common knowledge that
abilities are drawn at random from a common distribution \(F_A\) that is
assumed everywhere differentiable on the support
\(V\subseteq [0, \infty)\).

It is further assumed that costs are multiplicative

\[C(y_i, t_i, a_i) = {c_{y}}(y) \cdot {c_{t}}(t)  \cdot a_i^{-1}\]

with \({c_{y}}(0)\geq 0\), \({c_{y}}^\prime>0\), \({c_{t}}(d)\geq 0\),
and \({c_{t}}^\prime<0\).

Each agent is risk neutral and faces the following decision problem

\[\begin{array}{ll}
    \mbox{maximize} & \sum_{j=1}^k \Pr(\text{ranked $j$'th}) V_j  - C(y_i, t_i, a_i).
  \end{array}\]

\subsubsection{Equilibrium}\label{equilibrium}

We provide here the symmetric equilibrium with one prize and \(n>2\)
agents. In appendix XXX, we provide a general formula for \(k>2\)
prizes.

Let \(y_{1:n} < y_{2:n} < ... < y_{n:n}\) denote the order statistics of
the \(y_j\)'s for every \(j\neq i\) and let \({F_{Y_{r:n}}}(\cdot)\) and
\({f_{Y_{r:n}}}(\cdot)\) denote the corresponding distribution and
density for the \(r\)'th order statistic.

\begin{proposition}

In a tournament, the unique symmetric equilibrium of the model gives,
for every \(i=1, ..., n\), the optimal time to completion \(t^*(a_i)\)
equal to the deadline \(d\) and the optimal output quality \(y^*(a_i)\)
as

\[\label{eq: optimal bid tournament}
  y^*(a_i) =  V_1 \int_{a_i}^\infty {f_{Y_{n:n}}} (z) dz\]

if \({a_i}\geq {\underline a}\) \citep[see][]{moldovanu2001optimal}, and
equal to zero otherwise.

\end{proposition}

An important property of is that \(y^*(a_i)\) has its upper bound in and
lower bound in . Also, equilibrium output quality is monotonic
increasing in the agent's ability \citep[see][]{moldovanu2001optimal}.
Thus, for every \(i=1, ..., n+1\), the equilibrium expected reward
depends only on the rank of his ability relative to the others. Using
\({F_{A_{r:n}}}\) to denote the distribution of the \(r\)'th order
statistic of abilities gives

\[\label{eq: expected payoffs tournament}
  {F_{A_{n:n}}}(a_i) V_1  - C(y_i^*, d, a_i).\]

Hence, by setting to zero and solving for the ability, gives the
marginal ability \({\underline a}\) as

\[{\underline a}= h(n, V, F_A, C, d).\]

\begin{corollary}

Equilibrium behavior in a race

\end{corollary}

\subsubsection{Contest designer's
problem}\label{contest-designers-problem}

The sponsor of the contest chooses the rules of the competition
including prize structure \(\{V_j\}_{j=1}^k\), deadline \(d\), target
quality \(q\), and competition format (race or tournament). The sponsor
maximizes an objective function that is the sum of total quality
\(Y=\sum_{i=1}^{n+1} Y_i\), time spent \(T=\sum_{i=1}^{n+1} T_i\) and
prizes paid \(V=\sum_{j=1}^k p_{j} V_j\) (with \(p_j=1\) if the prize is
awarded and \(p_j=0\) otherwise). Hence, the problem faced by the
sponsor is

\[\begin{array}{ll}
    \mbox{maximize} & \int {Y}   -  {\tau}{{\bf E}}{T} - {{\bf E}}{V}
  \end{array}\]

with the intensity of preferences towards time weighted by
\(c_t\geq 0\).

\subsection{Structural econometric
model}\label{structural-econometric-model}

\section{The experimental design}\label{the-experimental-design}

The field experiment was conducted between March 2 and 16, 2016. The
context of the experiment was an online programming contest where
participants compete writing a programming code that solves a designated
problem. Similar programming contests are quite common and either as a
tournament or a race competition.

The contest was hosted on the online platform Topcoder.com. Since its
launch in 2001, Topcoder.com administers on a weekly basis several
competitive programming contests for thousands of competitors from all
over the world. Typical assigned problems are data science problems
(e.g., classification, prediction, natural language processing) that
demand some background in machine learning and statistics. All Topcoder
members (about 1M registered users in 2016) can compete and attain a
``rating'' that provides a metric of their ability as contestants. Other
than attaining a rating, the competitors having made the top five
submissions in a competition are typically awarded a monetary prize the
extent of which depends on the nature and complexity of the problem but
is generally between \$5,000 and \$20,000.

In this study, we worked together with researchers from the United
States National Health Institute (NIH) and the Scripps Research
Institute (SCRIPPS) to select a challenging problem for the experimental
programming competition. The selected problem was based on an algorithm
called BANNER built by NIH \citep{leaman2008banner} that uses expert
labeling to annotate abstracts from a prominent life sciences and
biomedical search engine, PubMed, so disease characteristics can be more
easily identified. The goal of the programming competition was to
improve upon the current NIH's system by using a combination of expert
and non-expert labeling, as described by \citet{good2014microtask}.

The competition was announced on the platform and to all community
members via email. A preliminary online registration was required to
enroll in the competition, which resulted in 340 pre-registered
participants. Among the pre-registered members, we selected the 299 who
had registered to a programming contest at least once before the present
contest. This choice was to ensure that participants were xxxx.

Participants were then randomly assigned to separate groups of 10 or 15
people. In each of these groups, contestants were given access to a
``virtual room'' that is a private web page listing handles of the other
participants of the group, a leaderboard updated regularly during the
competition, and a common chat that they can use to ask clarifying
questions (visible to everyone in the group) with respect to the problem
at hand.

A problem statement containing a full description of the algorithmic
challenge, the rules of the game, and payoffs was published at the
beginning of the submission phase. The submission phase was of 8 days in
which participants could submit their computer programs. Each submission
was automatically scored and feedback in the form of preliminary scores
was published regularly on the website via the leaderboard.

Groups were randomly assigned to one of three different competitive
settings: a race, a tournament, and a tournament with a \emph{reserve
target}, which is the lowest acceptable score by the platform for a
submission to be awarded a prize.

The experimental design is summarized by the Table XXXX.

\begin{table}
\centering
\caption{Experimental design}
\label{tab: experimental design}
\begin{tabular}{@{}lcc}
  \\[-1.8ex]\hline \hline \\[-1.8ex]
 & Large & Small \\ 
  \hline \\[-1.86ex]
Race & 60 & 39 \\ 
  Tournament & 60 & 40 \\ 
  Reserve & 60 & 40 \\ 
   \\[-1.8ex]\hline \hline \\[-1.8ex]
\end{tabular}
\end{table}

In all groups, the first placed competitor was awarded a prize of
\$1,000, and an additional, consolatory prize of \$100 was awarded to
the second one.

In a race competition, however, the first to achieve a score equal to
xxxx was placed first. The level was chosen xxxx.

In a tournament, xxxx.

Finally, in a tournament with reserve, xxxx.

Additional grand prizes of xxxx were awarded to the top xxx in every
treatment.

\subsection{Data}\label{data}

The bulk of our data comes from the online Topcoder's profile of each
participant. This profile typically includes information of when the
member registered to the platform, the current rating in a variety of
different competitions, the number of past competitions, and so on.
Additional demographic information, was collected via a pre-registration
survey where competitors were asked to state their gender, age,
geographic origin, etc. Participants were als also asked a self-reported
measure of risk aversion {[}xxx{]} and to forecast how many hours they
expected to compete in the next few days of the challenge(the exact
question was: ``looking ahead xxxx'').

Finally, we also asked participants to respond to a survey at the end of
the submission phase. In this final survey, they were asked to look back
and tell us their best estimate of the time spent working on the
problem. Also, we gathered comments on the xxx. And questions such as
xxxx.

Table XXX summarizes the data.

\begin{table}[!htbp] \centering 
  \caption{Descriptive statistics} 
  \label{} 
\begin{tabular}{@{\extracolsep{5pt}}lD{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} } 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
Statistic & \multicolumn{1}{c}{N} & \multicolumn{1}{c}{Mean} & \multicolumn{1}{c}{St. Dev.} & \multicolumn{1}{c}{Min} & \multicolumn{1}{c}{Max} \\ 
\hline \\[-1.8ex] 
Algo rating & 299 & 1,051.000 & 730.000 & 0 & 2,958 \\ 
Algo competitions & 299 & 40.600 & 57.700 & 0 & 338 \\ 
Algo registrations & 299 & 45.700 & 64.300 & 0 & 365 \\ 
MM rating & 205 & 1,322.000 & 425.000 & 593 & 3,071 \\ 
MM competitions & 299 & 7.160 & 11.800 & 0 & 91 \\ 
MM registrations & 299 & 17.600 & 23.000 & 1 & 161 \\ 
Time zone & 279 & 2.130 & 5.080 & -8.000 & 10.000 \\ 
Latitude & 279 & 36.700 & 19.000 & -42.800 & 59.900 \\ 
Longitude & 279 & 25.200 & 77.700 & -122.000 & 149.000 \\ 
Risk aversion & 284 & 6.390 & 2.190 & 1 & 10 \\ 
\hline \\[-1.8ex] 
\end{tabular} 
\end{table}

\section{Results}\label{results}

A total of xxxx registered but only 299 competitors were selected for
the challenge; we excluded those with no past experience on the platform
and those with incomplete data on the survey. Signed up competitors were
experienced members of the platform: the overall time as registered
platform member at the start of the competition ranged between 52.542
and 770.548 weeks. Yet, the direct experience in competing was highly
skewed with competitors in the highest 90th percentile having
participated in 24 more competitions than those in the 10th percentile.
Likewise skills as measured by the individual ratings, if there was one,
had a skewed distribution with 1034 higher points than those in the 10th
percentile; see Figure \ref{eq: distribution experience}.

After the two-week submission period, 86 competitors made 1759
submissions overall. The distribution of submissions was rather skewed,
with participants in the 90th percentile making 50 more submissions than
those in the 10th percentile.

Assuming the decision was independent, to explore the determinants of
participation:

\begin{equation}
  Pr(y=1) = G(\text{Rating}_{i} + \text{Experience}_{i} + \text{T}_{i})
\end{equation}

where \(G()\) is logistic.

\begin{verbatim}
## 
## ======================================================
##                               Dependent variable:     
##                          -----------------------------
##                                     submit            
##                             (1)       (2)       (3)   
## ------------------------------------------------------
## poly(mmevents, deg = 3)1 12.200***                    
##                           (4.120)                     
##                                                       
## poly(mmevents, deg = 3)2   1.210                      
##                           (6.130)                     
##                                                       
## poly(mmevents, deg = 3)3  8.110*                      
##                           (4.740)                     
##                                                       
## poly(mmevents, deg = 2)1            4.690*    4.980*  
##                                     (2.540)   (2.590) 
##                                                       
## poly(mmevents, deg = 2)2            -0.212    -0.077  
##                                     (2.470)   (2.530) 
##                                                       
## poly(mmrating, deg = 2)1           10.100*** 8.530*** 
##                                     (3.120)   (3.260) 
##                                                       
## poly(mmrating, deg = 2)2            -0.942    -1.500  
##                                     (2.380)   (2.430) 
##                                                       
## lat                                           0.021** 
##                                               (0.009) 
##                                                       
## long                                           0.003  
##                                               (0.002) 
##                                                       
## Constant                 -0.961*** -1.020*** -1.900***
##                           (0.138)   (0.143)   (0.405) 
##                                                       
## ------------------------------------------------------
## Observations                299       299       279   
## Log Likelihood           -166.000  -164.000  -150.000 
## Akaike Inf. Crit.         341.000   337.000   315.000 
## ======================================================
## Note:                      *p<0.1; **p<0.05; ***p<0.01
\end{verbatim}

\begin{verbatim}
## Analysis of Deviance Table
## 
## Model: binomial, link: logit
## 
## Response: submit
## 
## Terms added sequentially (first to last)
## 
## 
##                         Df Deviance Resid. Df Resid. Dev Pr(>Chi)
## NULL                                      278        334         
## poly(mmevents, deg = 2)  2    17.87       276        316  0.00013
## poly(mmrating, deg = 2)  2     8.89       274        308  0.01172
## lat                      1     4.96       273        303  0.02589
## long                     1     1.85       272        301  0.17347
\end{verbatim}

This result does not seem to correlate well with the competitor's
experience or skills, as the Pearsons's correlation coefficient between
the count of past competitions or the rating and the count of
submissions is positive but generally low; see Table XXX. Thus,
differences in submissions appear idiosyncratic and perhaps related to
the way to organize the work rather than systematically associated with
underlying differences in experience or skills.

The timing of submissions was rather uniform during the submission
period with a peak of submissions made in the last of the competition.
(explain more)

\begin{verbatim}
#scores$submax <- ave(subs$sub_id, subs$handle, FUN=max)
#par(mfrow=c(2, 1), mar=c(4,4,2,2))
#plot(subid==1 ~ as.POSIXct(subts), data=scores, type='h', yaxt='n'
#    , xlab='', ylab='', main='Dispersion time first submission')
#plot(subid==submax ~ as.POSIXct(subts), data=scores, type='h'
#    , yaxt='n', xlab='', ylab='', main='Dispersion time last submission')
\end{verbatim}

Scores: xxxx

\subsection{Treatment differences}\label{treatment-differences}

Difference in participation by treatments are show in Table XX.

\begin{verbatim}
Fisher's Exact Test for Count Data
\end{verbatim}

data: tab p-value = 0.5 alternative hypothesis: two.sided

We find no differences in the room size.

\begin{verbatim}
Fisher's Exact Test for Count Data
\end{verbatim}

data: tab p-value = 1 alternative hypothesis: true odds ratio is not
equal to 1 95 percent confidence interval: 0.569 1.691 sample estimates:
odds ratio 0.985

Ex-post

Timing: early vs late

Using a Chi-square test of independence, we find no significant
differences in participation rates associated with the assigned
treatments (p-value: 1); see Table XX.

Further, we model participation rates as a logistic regression. We use a
polynomial of third degree for the count of past competitions to account
for non-linear effects of experience; and we use an indicator for
whether the competitor had a win or not. Also, taking into account
differences in ability, participation rates are not significantly
different.

\subsection{Estimation results}\label{estimation-results}

Participation to the competition by treatment is shown in Figure
\ref{fig:entry}. Participation here is measured by the proportion of
registered participants per treatment who made any submission during the
eight-day submission period. Recall that competitors may decide to enter
into the competition and work on the problem without necessarily
submitting. In a tournament, for example, competitors are awarded a
prize based on their last submission and may decide to drop out without
submitting anything. However, this scenario seems unlikely. In fact,
competitors often end up making multiple submissions because by doing so
they obtain intermediate feedback via preliminary scoring (see Section
XXX for details). In a race, competitors have even stronger incentives
to make early submissions as any submission that hits the target first
wins.

\begin{verbatim}
Table xxx
\end{verbatim}

We find that the propensity to make a submission is higher in the
Tournament than in the Race and in the Tournament with reserve, but the
difference is not statistically significant (a Fisher's exact test gives
a p-value of xxxxx). As discussed in Section XXX, we may not have enough
power to detect differences below 5 percentage points. However, we find
the same not-significant result in a parametric regression analysis of
treatment differences with controls for the demographics and past
experience on the platform; see Table \ref{entry}. Adding individual
covariates reduces variability of outcomes, potentially increasing the
power of our test. In particular, Table \ref{entry} reports the results
from a logistic regression on the probability of making a submissions.
Column 1 reports the results from a baseline model with only treatment
dummies. Column 2 adds demographics controls, such as the age,
education, and gender. Column 3 adds controls for the past experience on
the platform. Across all these specifications, the impact of the
treatment dummies (including room size) on entry is not statistically
significant.

\subsection{Simulation results}\label{simulation-results}

\section{Empirical analysis}\label{empirical-analysis}

\subsection{Estimation results}\label{estimation-results-1}

Participation to the competition by treatment is shown in Figure
\[fig:entry\]. Participation here is measured by the proportion of
registered participants per treatment who made any submission during the
eight-day submission period. Recall that competitors may decide to enter
into the competition and work on the problem without necessarily
submitting. In a tournament, for example, competitors are awarded a
prize based on their last submission and may decide to drop out without
submitting anything. However, this scenario seems unlikely. In fact,
competitors often end up making multiple submissions because by doing so
they obtain intermediate feedback via preliminary scoring (see Section
XXX for details). In a race, competitors have even stronger incentives
to make early submissions as any submission that hits the target first
wins.

\begin{verbatim}
Table xxx
\end{verbatim}

We find that the propensity to make a submission is higher in the
Tournament than in the Race and in the Tournament with reserve, but the
difference is not statistically significant (a Fisher's exact test gives
a p-value of~xxxx). As discussed in Section XXX, we may not have enough
power to detect differences below 5 percentage points. However, we find
the same not-significant result in a parametric regression analysis of
treatment differences with controls for the demographics and past
experience on the platform; see Table \[entry\]. Adding individual
covariates reduces variability of outcomes, potentially increasing the
power of our test. In particular, Table \[entry\] reports the results
from a logistic regression on the probability of making a submissions.
Column 1 reports the results from a baseline model with only treatment
dummies. Column 2 adds demographics controls, such as the age,
education, and gender. Column 3 adds controls for the past experience on
the platform. Across all these specifications, the impact of the
treatment dummies (including room size) on entry is not statistically
significant.

\subsection{Simulation results}\label{simulation-results-1}

\renewcommand\refname{References}
\bibliography{/Users/andrea/Papers/library.bib}

\end{document}