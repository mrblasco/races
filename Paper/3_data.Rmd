```{r}
read_chunk("R/data.R")
```

# The experiment

## The context

While many observational studies have focused on various aspects of races and tournaments separately, the design of observational studies for comparing races with tournaments is generally problematic. First, the variation in contest designs in terms of duration, solutions sought, and prize structures can be large, forcing researchers to collect observations from multiple designs with overlapping combinations in order to fully account for these differences. Second, as our model suggests, contestants often face sensible payoff differences between a race and a tournament competition, an issue that favors the emergence of selection problems that are hard to control in observational studies.

Instead of using naturally occurring data, we test our theory by designing and executing a field experiment. In doing so, one needs an environment where the same type of contest can be replicated under different competition styles while keeping fixed all other relevant characteristics. In particular, it is important to have competitors facing the same problem,  contest duration,  and prize structure while changing only the competition style. 

Such an environment was provided by  _Topcoder_, a company based in the United States that administers a popular online platform for computer programming competitions and that agreed to provide (a) access to its large member base of competitors (over 1 million registered users in 2016) and (b) access to its platform's tools for managing online contests (e.g., web-based leaderboards, scoring methods to determine the winners, a payment system to reward competitors residing all over the world). These two aspects --- large sample and tools for managing online contests --- made possible by considerably simplifying the execution of our experiment. 

Beyond giving the means for the execution of our experimental design, three key factors made this platform ideal. First, the platform offers periodic programming competitions free for all its members to participate. One can hence expect platform members to be familiar with the kind of strategic interactions that arise in a race or a tournament competition, as implicitly assumed by our game-theoretic contest model. In particular, every two weeks, the platform hosts one-day-long race-style programming competitions, known as "Single Round Matches" or SRMs. These SRM competitions are "for fun" (e.g., no monetary rewards) and winners are selected on the basis of their coding speed (e.g., the time it takes to write a software program to solve a given problem). In addition to SRMs, about once a month, the platform hosts two-week-long tournament-like programming competitions, known as "Marathon Matches" or MMs. These contests ask contestants to deal with more difficult data science problem and, importantly, assign a score to each submitted solution (e.g., in a prediction task, the score can be a measure of prediction accuracy like the mean squared error) awarding the top five solutions with cash prizes.^[Cash prizes usually range between $10-20,000 for the very top solution; $10-5,000 for the second highest; and less than $3000 for the rest.] As platform members tend to participate in both types of competitions, it is likely that those recruited for our experiment would have some direct experience with both race and tournament competitions.


Second, the platform computes and keeps track of different measures of competitors' individual ability based on their past performance. These include multiple "skill ratings" that are computed and provide a metric of their ability as contestants in each type of competition (race-like SRMs and tournament-like MMs) As a result, one could expect to (a) estimate treatment effects controlling for very accurate measures of individual ability,[^signal] and (b) examine the association between individual ability and the observed sorting patterns in races and tournaments.  In a contest, showing the distribution of skill ratings to the contestants was also an easy way to help them forming common beliefs about the distribution of abilities, as assumed by the model. 

[^signal]: Topcoder's skill ratings are often used by software companies for hiring. 


A third key factors making this platform ideal for our experiment was that it collects rich data analytics on contest participation like the timing, extent, and score of solutions submitted during the contest. This is because all solutions must be submitted online to be considered for the contest. A typical submission consists of a single source code file.[^libraries] The submitted file is then executed on the server and returns a score. Scores are an objective metric of quality that usually combine different criteria like how fast solutions run or the prediction error in a classification task into one single numeric value. As the platform provided us with all these measures, we were able to measure the timing, extent, and quality of participation for each competitor with high accuracy.

[^libraries]: The source code being one single file does not end up limiting the complexity of the solutions, as coders are allowed to call other functions from a large database of libraries.


## Experimental design

The goal of the experiment was to compare and contrast treatment effects of three different competition styles: (a) races, (b) tournaments, and (c) tournaments with reserve, as defined in our theoretical model section. To further test treatment differences under a varying degree of competition intensity, each of the treatments had two levels of intensity determined by the number of competitors. One intensity level had 10 contestants (small size) and the other 15 contestants (large size).

Forming the 3x2 factorial design shown in Table \ref{experiment table}, the treatments were applied to 24 virtual competition rooms, which are our main experimental units. Each room consisted of a list of randomly assigned competitors and a customized webpage with a problem description, a provisional leaderboard (updated about every 48 hours), and a submission system through which contestants could submit their codes. 

```{r experimental_design_table, results='asis'}
```
 
A total prize pool of more than \$40,000 was offered as an incentive for participation. The prize structure across the rooms was the same. In each room, we offered cash prizes of $1,000 and $100 to the first and second placed competitor. An additional "grand" prize of \$6,000 was offered to the top competitor (i.e., the first among all top room competitors) in each competition treatment (a total of 3 grand prizes).

<!-- Discussion: why ? -->

Registration to the contest was conducted online. The announcement of a a four-day registration period was sent via email to all newsletter subscribers and was publicized in a post on the platform's blog. Registration was free and open to every "experienced" platform member. In particular, we excluded newly signed-up members and all those who had never registered for a MMs competition before the experiment. Registration involved an informed consent and a short registration survey asking for demographics data.

All registered members were then sorted at random into lists of 10 and 15 competitors. Instead of complete at random, randomization of competitors to lists was done trying to balance the skill rating distribution across rooms. In particular, registered participants were sorted by their skill rating and then sequentially assigned to different lists [AB: \textcolor{red}{double-check exact procedure}]. These lists were then randomly assigned to one of our treatments.^[Each room's webpage required a login and was inaccessible to anyone else. ] 

Participants knew in advance the nature of the problem, the timing of the competition, and the room size. They were, however, not aware of the competition style and the other contestants in the room. This information was communicated to them via an email with a link to their competition room and announcing the start of an eight day submission period.

At the end of the submission phase, the platform administered the scoring of submissions and payments to the winners. All contestants were invited to take a final survey to collect self-reported measures of effort --- limited-edition T-shirts were offered as incentive. This final survey took place a few days after the end of the submission period (but before the final ranking was shown and winners fully identified). 
 
### The algorithmic problem

To select a challenging algorithmic problem for the experiment, we worked together with researchers from the United States National Health Institute (NIH) and the Scripps Research Institute (SCRIPPS). The selected problem was based on an algorithm called BANNER [@leaman2008banner] that was built by researchers at the NIH. The algorithm uses domain-expert manual labeling to train a Natural Language Entity Recognition (NLER)'s model that performs automatic annotation of abstracts from a large corpus of biomedical research papers (e.g., PubMed). As automatic annotations help disease characteristics to be more easily identified, improving the existing methods to annotate abstracts was very important for the searchability and identifiability of most relevant papers among millions of records.

The specific goal of the programming competition was to improve upon the current NIH's BANNER by using a combination of domain-expert and non-expert (e.g., Amazon Mechanical Turk's workers) manual labeling [e.g., @good2014microtask]. As a measure of the ability to make correct predictions of domain expert annotations we used the F-score defined as the harmonic mean of precision and recall: $F = 2 * (precision * recall) / (precision + recall)$). This score was computed on 300 abstracts (100 of which were not disculosed to avoid overfitting) with about a thousand entities to correctly identify from a dictionary with 15 hundred labels. 


One crucial issue in the execution of our experiment was the choice of a quality target for the race and tournament with reserve competitions. To this end, we followed the following two main criteria. First, we wanted a target representing an improvement of current methods but that was achievable in a 8 days. To do that, we run a pre-trail experiment that involved 4 highly-skilled coders. They were asked to solve the same problem in isolation for 5 days. Results of this pre-trail experiment helped us forming basic predictions about technical feasibility of different targets.  To pick the final value, however, we relied on the expectations of NIH researchers who developed the current systems. That is, we surveyed the NIH researchers who developed Banner asking for three percentage improvements they considered "useful," "desirable," and "very unlikely." The final target used in the experiment was close to the "very unlikely" threshold. The baseline F-score achieved by NIH researchers was `r baseline`. We set up a hard-to-reach F-score target for the race competition which was `r target` (about a `r round(100*target/baseline - 100)` percent increase of the baseline). 

<!-- 
The winner achieved a score of `r winner`. This represents  a `r 100*(winner - baseline)` percentage points increase compared to the baseline which can be regarded as a very remarkable improvement (more than `r round(100*winner/baseline - 100)` percent).
 -->


## Data

Using platform data (top panel of Table \ref{summary}), we find that the majority of competitors taking part in this study were mature users of the platform (`r round(mean(covars$year))` years of membership on average), with much experience in race-like (`r round(mean(covars$nregsrm))` SRM registrations on average) and tournament-like (`r round(mean(covars$nreg))` MM registrations on average) competitions by participating in which they earned a median of $`r round(100*median(covars$paidyr, na.rm=TRUE))` per year in cash prizes.

Data from a mandatory registration survey (bottom panel of Table \ref{summary}) further reveal that study participants were residing in `r length(p <- prop.table(table(races$country)))` countries (`r round(100*p[names(p)=='United States'])` percent in the United States), were below 30 years old (`r percent(mean(covars$below30, na.rm=TRUE))` percent), and were mostly male (`r percent(mean(covars$male, na.rm=T))` percent), reflecting an overall unbalanced gender participation on Topcoder which is common to many other online platforms like Wikipedia or Stack Overflow.^[See, for example, xxxx.] Our sample is thus hardly representative of online communities in general or even online communities of programmers, but it comprises a set of experienced and relatively homogeneous competitors. 

```{r descriptive_table, results='asis'}
```

A key variable to measure was competitors' ability in terms of (1) problem-solving skills targeted for data science problems (like the named entity recognition task used in the contest) and (2) programming speed. As a proxy for these two aspects of a competitor's ability, we collected individual skill ratings for each participant in the study. These ratings are publicly available on the platform and represent elo-type measures of a competitor's relative ability compared to others.[^skillrating]  In particular, we rely on the skill rating computed for MMs as a proxy of problem solving ability and on that computed for SRMs as a measure of programming speed.

[^skillrating]: This measure is represented by a number that increases or decreases at the end of a competition depending on the difference between a hypothetical expected rank (based on the pre-contest values of the skill rating of the opponents) and the actual rank achieved by a competitor at the end of a competition. When the actual rank is higher than the expected rank, the skill rating increases whereas it decreases otherwise. The skill ratings in our sample are computed independently for each type of competition run on the platform (MMs and SRMS) before this study.

```{r rating_density_comparison_figure, fig.width=9, fig.height=4, include=FALSE}
```

\begin{figure}
\centering
\caption{Distribution of Topcoder's skill ratings}
\label{skill ratings}
\includegraphics{Figures/rating_density_comparison_figure-1.pdf}
\end{figure}

The empirical density functions for these two skill ratings are shown in Figure \ref{skill ratings}. Both skill ratings have slightly asymmetric empirical densities due to the presence of a few competitors with very high skill ratings relative to the rest. A simple linear regression model regressing problem-solving (MM skill ratings) on programming speed (SRM skill ratings) shows that, though highly positively correlated, there is considerable variation between these two variables (the $R^2$ is equal to `r round(summary(rating.lm)$r.squared,2)`); suggesting the same competitor may perform very differently in races and tournaments.

Beyond ability, other factors may determine competitors' actions in a contest. One important factor could be the degree of risk aversion. The registration survey thus collected participants' attitudes towards risk, asking registrants to indicate their "willingness to take risks in general" on a 11-point scale  (from 0 "Unwilling" to 10 "Completely willing").[^risk] The median response was higher than in previous studies [@dohmen2011individual], indicating competitors had perhaps higher propensity to take risk than the norm. 

Registrants were also asked to make a forecast of how many hours they anticipated to work to solve the algorithmic problem during the submission phase of the challenge.[^forecasting] Our data show that participants anticipated a median of `r median(covars$hours, na.rm=T)` hours of total work during the contest (anticipating slightly more hours of work in the first and last days of the contest).

[^risk]: The validity and economic relevance of this way to measure risk preferences has been shown by @dohmen2011individual. 

[^forecasting]: The exact question was: "Looking ahead a week, how many hours do you forecast to be able to work on the solution of the problem?". Participants had to pick an integer between 0 and 48 hours for every 2 days of the submission phase (a total of 4 choices).


<!-- Interestingly, the ratio of submitted solutions over registered competitions in MMs is  low (`r round(100*mean(covars$nsub/covars$nreg))` percent) relative to SRMs (84%), suggesting that even highly experienced platform members frequently drop out of the competition. An issue on which we will return in the analysis of the results.  -->

<!-- 
We collected extensive platform data for each registered participant. These include the full history of their registrations in past contests, as well as the outcomes of these contests like the number of solutions submitted, placements in the final rankings, and total cash prizes earned. As reported in Table \ref{summary}, our data show that those who registered in the experiment were very experienced competitors. They had been platform members for an average of `r round(mean(covars$year))` years, had registered in an average of `r round(mean(covars$nreg))` multi-week long contests (MMs), in which they had been earning a median of `r round(median(covars$paidyr, na.rm=TRUE))` hundred dollars per year in cash prizes. As our data shows, on average, they had submitted solutions in only `r round(mean(covars$nsub))` of these contests (`r round(100*mean(covars$nsub/covars$nreg))` percent of their registrations), suggesting that even highly experienced platform members frequently drop out of competitions. Our sample had also registered in an average of `r round(mean(covars$nregsrm))` speed-based contests (SRMs) with a relatively higher submission rate (`r round(100*mean(with(covars, ifelse(nregsrm>0, nsubsrm/nregsrm, NA)), na.rm=TRUE))` percent), presumably because these contests take only a few hours to complete and have relatively simpler problems that require less effort to solve.
 -->


Finally, to test whether randomization into treatment groups was successful, we conducted a series of F-tests to check the statistical significance of mean differences in competitors' characteristics between treatments. As shown in Table \ref{summary}, these tests returned very small F statistics for each variable indicating that our randomization was successful. 

