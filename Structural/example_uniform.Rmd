% Example uniform distribution
%
%

Suppose abilities are drawn from the uniform distribution with parameters $a>0$ and $b>a$:

\begin{equation}
	a_i \sim \text{Uniform}(a, b).
\end{equation}

## Identification

The $G^{-1}(.)$ function for $n=2$ and $\alpha=1$ is then:
\begin{equation}
	G(\mtype) \equiv \mtype F(\mtype) = \mtype  \frac{(\mtype - a)}{b-a}
\end{equation}

Hence, the inverse $G^{-1}(\cdot)$ is
\begin{equation}
	G^{-1}(u) = \frac{1}{2} (b-a) \left(-\frac{\sqrt{a^2-4 a u+4 b u}}{a-b}-\frac{a}{a-b}\right)
\end{equation}

Important properties are:

1. $G(\cdot)$ is monotonic increasing in the parameter $b$ (the upper limit of the uniform distribution).

2. $G(\cdot)$ is monotonic increasing in the parameter $a$ (the lower limit of the distribution).

3. By the chain of derivatives, also the inverse $G^{-1}(\cdot)$ is monotonic increasing in the parameters $a$ and $b$. 

Our proof shows that when $G()$ is increasing monotonic in the distribution, then the model is _identifiable._ Let's check this result here.

By using $\theta=(a,b)$, the binomial probability $p(x, \theta)$ is then:
\begin{equation}
	p(x, \theta) =   \frac{b - G^{-1}(x, a, b)}{b-a}
\end{equation}

I need to verify that there are no $a,b$ combinations that give the same probability for any $x,x^\prime$ in $X$. 

## Estimation

The likelihood for $l=1,..., L$ iid rooms characterized by the vector $x_l$ with $Y_l$ participants is
\begin{equation}
	 \text{Likelihood} = \prod_{l=1}^L  p(x_l, \theta)^{Y_l}[1-p(x_l, \theta)]^{N_l-Y_l} 
\end{equation}
the binomial term can be ignored since it does not contain the parameters. The log-likelihood is then
\begin{equation}
	 \text{log-likelihood} = \sum_{l=1}^L  Y_l \log p(x_l, \theta) + (N_l - Y_l)\log (1-p(x_l, \theta)).
\end{equation}

First order conditions:
\begin{equation}
	 \sum_{l=1}^L   \left[\frac{Y_l}{p(x_l, \theta)} - \frac{(N_l - Y_l)} {(1-p(x_l, \theta)}\right] \frac{\partial p(x_l, \theta)}{\partial \theta} = 0.
\end{equation}
That is:
\begin{equation}
	 \sum_{l=1}^L
	 \left[\frac{Y_l - N_l p(x_l, \theta)}{p(x_l, \theta)(1-p(x_l, \theta)}\right]
	 \frac{\partial p(x_l, \theta)}{\partial \theta} = 0.
\end{equation}

Second order conditions: 
\begin{align}
	 \sum_{l=1}^L
	 \left[\frac{Y_l - N_l p(x_l, \theta)}{p(x_l, \theta)(1-p(x_l, \theta)}\right]
		\frac{\partial^2 p(x_l, \theta)}{\partial \theta^2} + \nonumber\\
	+\left[-\frac{Y_l}{p(x_l, \theta)^2} - \frac{(N_l - Y_l)} {(1-p(x_l, \theta)^2}\right]
		\left(\frac{\partial p(x_l, \theta)}{\partial \theta}\right)^2 \leq 0.
\end{align}

At the optimum, the objective function is concave. We call the local maximum the mle parameter estimate.
 